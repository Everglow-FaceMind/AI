{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ubz79S5XXE5",
        "outputId": "90c38e08-cd49-4b07-876e-deae7054ae53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3uhoN7htumo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from natsort import natsorted\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.signal\n",
        "from scipy.signal import find_peaks\n",
        "import sys\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras.backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L_7IQg7dWR_b",
        "outputId": "60eaebc7-d9a4-4080-dfb7-fd677d86be27"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 42,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 1,\n        \"max\": 49,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          33,\n          17,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 42,\n        \"samples\": [\n          \"/content/drive/MyDrive/everglow/ubfc-rppg/HR-npz/subject33.npz\",\n          \"/content/drive/MyDrive/everglow/ubfc-rppg/HR-npz/subject17.npz\",\n          \"/content/drive/MyDrive/everglow/ubfc-rppg/HR-npz/subject12.npz\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6848b163-ae3c-471e-960f-cd1f26b8b069\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>9</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>13</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>14</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>15</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>16</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>17</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>18</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>22</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>23</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>24</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>25</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>26</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>27</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>30</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>31</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>32</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>33</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>34</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>35</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>36</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>37</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>38</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>39</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>40</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>41</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>42</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>43</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>44</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>45</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>46</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>47</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>48</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>49</td>\n",
              "      <td>/content/drive/MyDrive/everglow/ubfc-rppg/HR-n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6848b163-ae3c-471e-960f-cd1f26b8b069')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6848b163-ae3c-471e-960f-cd1f26b8b069 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6848b163-ae3c-471e-960f-cd1f26b8b069');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dab03c8d-9214-41fb-92c1-75abc4ebd9d0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dab03c8d-9214-41fb-92c1-75abc4ebd9d0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dab03c8d-9214-41fb-92c1-75abc4ebd9d0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6e0522c3-4e3c-497c-af93-aedad0b44339\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6e0522c3-4e3c-497c-af93-aedad0b44339 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    id                                               path\n",
              "0    1  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "1    3  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "2    4  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "3    5  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "4    8  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "5    9  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "6   10  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "7   11  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "8   12  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "9   13  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "10  14  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "11  15  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "12  16  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "13  17  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "14  18  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "15  20  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "16  22  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "17  23  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "18  24  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "19  25  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "20  26  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "21  27  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "22  30  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "23  31  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "24  32  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "25  33  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "26  34  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "27  35  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "28  36  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "29  37  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "30  38  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "31  39  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "32  40  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "33  41  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "34  42  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "35  43  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "36  44  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "37  45  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "38  46  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "39  47  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "40  48  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n...\n",
              "41  49  /content/drive/MyDrive/everglow/ubfc-rppg/HR-n..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/everglow/ubfc-rppg/metadata_hr_csv_path.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSE3aJoS3_8R",
        "outputId": "589c70c7-bdb9-472a-b59c-fdc975318dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimum number of frames in the dataset: 1368\n"
          ]
        }
      ],
      "source": [
        "# frame_counts = []\n",
        "# for _, row in df.iterrows():\n",
        "#     with np.load(row['path'], allow_pickle=True) as data:\n",
        "#         video = data['video']\n",
        "#         frame_counts.append(video.shape[0])  # 비디오 시퀀스의 길이 (프레임 수) 추가\n",
        "\n",
        "# 최소 프레임 수 찾기\n",
        "# min_frames = min(frame_counts)\n",
        "min_frames = 1368\n",
        "print(f\"Minimum number of frames in the dataset: {min_frames}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yutGzoKH17ch"
      },
      "outputs": [],
      "source": [
        "def load_dataset(df):\n",
        "    video_data = []\n",
        "    ppg_data = []\n",
        "    hr_data = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        with np.load(row['path']) as data:\n",
        "            video = data['video']\n",
        "            wave = data['wave']\n",
        "            hr = data['hr']\n",
        "            if video.shape[0] > min_frames:\n",
        "                video = video[:min_frames]\n",
        "                wave = wave[:min_frames]\n",
        "                hr = hr[:min_frames]\n",
        "\n",
        "            video_data.append(video)\n",
        "            ppg_data.append(wave)\n",
        "            hr_data.append(hr)\n",
        "            ## 모델 입력을 HR로 할 결루\n",
        "            # heart_rate = calculate_hr(wave)\n",
        "            # ppg_data.append(heart_rate)\n",
        "\n",
        "    return np.array(video_data), np.array(hr_data)\n",
        "\n",
        "video_data, ppg_data = load_dataset(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g_buEW7PcnA"
      },
      "outputs": [],
      "source": [
        "# class ConvBNReLU3D(nn.Module):\n",
        "#     def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, padding=1):\n",
        "#         super(ConvBNReLU3D, self).__init__()\n",
        "#         self.conv = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "#         self.bn = nn.BatchNorm3d(out_planes)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "# class MobileNet3D(nn.Module):\n",
        "#     def __init__(self, input_shape=(1368, 64, 64, 3), num_classes=min_frames):\n",
        "#         super(MobileNet3D, self).__init__()\n",
        "#         self.layer1 = ConvBNReLU3D(input_shape[3], 32, stride=2, padding=1)\n",
        "#         self.layer2 = ConvBNReLU3D(32, 64, padding=1)\n",
        "#         self.layer3 = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "#         # Add more layers as needed...\n",
        "\n",
        "#         self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "#         self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.layer1(x)\n",
        "#         x = self.layer2(x)\n",
        "#         x = self.layer3(x)\n",
        "\n",
        "#         # Forward pass through additional layers...\n",
        "\n",
        "#         x = self.avgpool(x)\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# # Create the model\n",
        "# model = MobileNet3D(input_shape=(1368, 64, 64, 3), num_classes=1)\n",
        "# print(model)\n",
        "\n",
        "# # For regression, you might use MSE Loss\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "# # Example input tensor\n",
        "# input_tensor = torch.randn((1, 3, 1368, 64, 64))  # Batch size of 1\n",
        "# output = model(input_tensor)\n",
        "# print(output.shape)  # Expected shape: [1, 1] since num_classes=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykgXX8Pb76qi"
      },
      "outputs": [],
      "source": [
        "class HardSwish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * F.relu6(x + 3.0) / 6.0\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=4):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc1 = nn.Conv3d(in_channels, in_channels // reduction, kernel_size=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Conv3d(in_channels // reduction, in_channels, kernel_size=1)\n",
        "        self.hsigmoid = nn.Hardsigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pool(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.hsigmoid(out)\n",
        "        return x * out\n",
        "\n",
        "class ConvBNHardSwish(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, padding=1):\n",
        "        super(ConvBNHardSwish, self).__init__()\n",
        "        self.conv = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm3d(out_planes)\n",
        "        self.activation = HardSwish()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.bn(self.conv(x)))\n",
        "\n",
        "class MobileNetV3(nn.Module):\n",
        "    def __init__(self, input_shape=(1368, 64, 64, 3), num_classes=min_frames):\n",
        "        super(MobileNetV3, self).__init__()\n",
        "        self.layer1 = ConvBNHardSwish(input_shape[3], 16, stride=2, padding=1)\n",
        "        self.layer2 = ConvBNHardSwish(16, 16, padding=1)\n",
        "        self.layer3 = SEBlock(16)\n",
        "        self.layer4 = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "        # 이후 레이어를 추가하고 필요에 따라 SEBlock 및 HardSwish 적용...\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(16, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        # 추가 레이어를 통한 포워드 패스...\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoOiSpxc5WD-",
        "outputId": "e78c5759-09de-495f-a319-0f636f172535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "video_train shape: (24, 1368, 64, 64, 3)\n",
            "video_val shape: (9, 1368, 64, 64, 3)\n",
            "video_test shape: (9, 1368, 64, 64, 3)\n",
            "ppg_train shape: (24, 1368)\n",
            "ppg_val shape: (9, 1368)\n",
            "ppg_test shape: (9, 1368)\n"
          ]
        }
      ],
      "source": [
        "# 데이터를 학습, 검증, 테스트 세트로 분할\n",
        "video_train, video_test, ppg_train, ppg_test = train_test_split(video_data, ppg_data, test_size=0.2, random_state=42)\n",
        "video_train, video_val, ppg_train, ppg_val = train_test_split(video_train, ppg_train, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n",
        "\n",
        "print(f\"video_train shape: {video_train.shape}\")\n",
        "print(f\"video_val shape: {video_val.shape}\")\n",
        "print(f\"video_test shape: {video_test.shape}\")\n",
        "print(f\"ppg_train shape: {ppg_train.shape}\")\n",
        "print(f\"ppg_val shape: {ppg_val.shape}\")\n",
        "print(f\"ppg_test shape: {ppg_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5Var895VLm5"
      },
      "outputs": [],
      "source": [
        "class VideoPPGDataset(Dataset):\n",
        "    def __init__(self, video_data, ppg_data):\n",
        "        \"\"\"\n",
        "        video_data: 비디오 데이터 배열 (샘플 수, 프레임 수, 높이, 너비, 채널)\n",
        "        ppg_data: PPG 신호 데이터 배열 (샘플 수, 프레임 수)\n",
        "        \"\"\"\n",
        "        self.video_data = video_data\n",
        "        self.ppg_data = ppg_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = self.video_data[idx]\n",
        "        ppg = self.ppg_data[idx]\n",
        "        return torch.from_numpy(video).permute(3, 0, 1, 2).float(), torch.from_numpy(ppg).float()  # 채널 첫 번째로 이동\n",
        "\n",
        "# 데이터셋 인스턴스 생성\n",
        "train_dataset = VideoPPGDataset(video_train, ppg_train)\n",
        "val_dataset = VideoPPGDataset(video_val, ppg_val)\n",
        "test_dataset = VideoPPGDataset(video_test, ppg_test)\n",
        "\n",
        "# DataLoader 인스턴스 생성\n",
        "batch_size = 8  # 예시로 배치 크기를 4로 설정\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8EAo1HGi52r",
        "outputId": "9ede642e-9f13-41c7-93a6-86a56361778c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First batch video shape: torch.Size([8, 3, 1368, 64, 64])\n",
            "First batch PPG shape: torch.Size([8, 1368])\n"
          ]
        }
      ],
      "source": [
        "first_batch_video, first_batch_ppg = next(iter(train_loader))\n",
        "\n",
        "print(\"First batch video shape:\", first_batch_video.shape)\n",
        "print(\"First batch PPG shape:\", first_batch_ppg.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIZxVR-D3XYz",
        "outputId": "c9892cb2-c318-4fee-e929-8f099eb2f790"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8, 1, 1368])) that is different to the input size (torch.Size([8, 1368])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1, 1368])) that is different to the input size (torch.Size([1, 1368])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 5001, Train Loss: 595.4276, Val Loss: 228.6072\n",
            "Epoch 5002, Train Loss: 636.7734, Val Loss: 172.1490\n",
            "Epoch 5003, Train Loss: 625.2060, Val Loss: 171.1980\n",
            "Epoch 5004, Train Loss: 595.5276, Val Loss: 176.9480\n",
            "Epoch 5005, Train Loss: 612.2348, Val Loss: 212.5731\n",
            "Epoch 5006, Train Loss: 625.9302, Val Loss: 196.9124\n",
            "Epoch 5007, Train Loss: 579.0916, Val Loss: 187.7225\n",
            "Epoch 5008, Train Loss: 611.4926, Val Loss: 231.1128\n",
            "Epoch 5009, Train Loss: 609.1077, Val Loss: 249.2121\n",
            "Epoch 5010, Train Loss: 562.3197, Val Loss: 195.8646\n",
            "Epoch 5011, Train Loss: 625.4269, Val Loss: 183.5852\n",
            "Epoch 5012, Train Loss: 618.3223, Val Loss: 178.2148\n",
            "Epoch 5013, Train Loss: 606.8198, Val Loss: 178.3981\n",
            "Epoch 5014, Train Loss: 605.0153, Val Loss: 222.3175\n",
            "Epoch 5015, Train Loss: 607.5606, Val Loss: 283.3474\n",
            "Epoch 5016, Train Loss: 606.3025, Val Loss: 242.0696\n",
            "Epoch 5017, Train Loss: 612.8688, Val Loss: 216.7557\n",
            "Epoch 5018, Train Loss: 611.1920, Val Loss: 192.5040\n",
            "Epoch 5019, Train Loss: 611.9577, Val Loss: 205.6771\n",
            "Epoch 5020, Train Loss: 603.5880, Val Loss: 253.8799\n",
            "Epoch 5021, Train Loss: 593.2486, Val Loss: 227.0969\n",
            "Epoch 5022, Train Loss: 615.4290, Val Loss: 181.7522\n",
            "Epoch 5023, Train Loss: 606.6391, Val Loss: 203.4043\n",
            "Epoch 5024, Train Loss: 625.5723, Val Loss: 251.1290\n",
            "Epoch 5025, Train Loss: 614.0364, Val Loss: 270.7651\n",
            "Epoch 5026, Train Loss: 545.5733, Val Loss: 339.9601\n",
            "Epoch 5027, Train Loss: 631.3167, Val Loss: 323.6385\n",
            "Epoch 5028, Train Loss: 611.4988, Val Loss: 218.3193\n",
            "Epoch 5029, Train Loss: 614.1387, Val Loss: 206.2516\n",
            "Epoch 5030, Train Loss: 625.6821, Val Loss: 223.8801\n",
            "Epoch 5031, Train Loss: 625.3356, Val Loss: 184.8456\n",
            "Epoch 5032, Train Loss: 595.6507, Val Loss: 173.9619\n",
            "Epoch 5033, Train Loss: 611.4078, Val Loss: 175.2082\n",
            "Epoch 5034, Train Loss: 627.2853, Val Loss: 171.1875\n",
            "Epoch 5035, Train Loss: 589.8309, Val Loss: 174.4050\n",
            "Epoch 5036, Train Loss: 610.5939, Val Loss: 174.8835\n",
            "Epoch 5037, Train Loss: 601.2577, Val Loss: 180.9435\n",
            "Epoch 5038, Train Loss: 624.8138, Val Loss: 186.6988\n",
            "Epoch 5039, Train Loss: 616.0006, Val Loss: 186.1525\n",
            "Epoch 5040, Train Loss: 601.2409, Val Loss: 191.1431\n",
            "Epoch 5041, Train Loss: 600.3155, Val Loss: 196.9827\n",
            "Epoch 5042, Train Loss: 591.9108, Val Loss: 228.3476\n",
            "Epoch 5043, Train Loss: 608.9090, Val Loss: 220.1886\n",
            "Epoch 5044, Train Loss: 601.9783, Val Loss: 222.4903\n",
            "Epoch 5045, Train Loss: 630.8525, Val Loss: 201.6680\n",
            "Epoch 5046, Train Loss: 602.9273, Val Loss: 210.1868\n",
            "Epoch 5047, Train Loss: 614.4219, Val Loss: 269.3335\n",
            "Epoch 5048, Train Loss: 614.7970, Val Loss: 204.8471\n",
            "Epoch 5049, Train Loss: 619.8679, Val Loss: 191.8398\n",
            "Epoch 5050, Train Loss: 619.9742, Val Loss: 194.5684\n",
            "Epoch 5051, Train Loss: 619.3707, Val Loss: 180.7780\n",
            "Epoch 5052, Train Loss: 622.1963, Val Loss: 222.8405\n",
            "Epoch 5053, Train Loss: 623.9003, Val Loss: 347.5380\n",
            "Epoch 5054, Train Loss: 608.9983, Val Loss: 433.6721\n",
            "Epoch 5055, Train Loss: 574.6953, Val Loss: 222.1251\n",
            "Epoch 5056, Train Loss: 626.2527, Val Loss: 246.1994\n",
            "Epoch 5057, Train Loss: 626.3270, Val Loss: 284.4077\n",
            "Epoch 5058, Train Loss: 617.3432, Val Loss: 346.9688\n",
            "Epoch 5059, Train Loss: 625.5334, Val Loss: 301.2282\n",
            "Epoch 5060, Train Loss: 625.4683, Val Loss: 191.0657\n",
            "Epoch 5061, Train Loss: 592.2551, Val Loss: 188.2450\n",
            "Epoch 5062, Train Loss: 611.1463, Val Loss: 238.1572\n",
            "Epoch 5063, Train Loss: 569.7451, Val Loss: 277.8096\n",
            "Epoch 5064, Train Loss: 623.3825, Val Loss: 259.8631\n",
            "Epoch 5065, Train Loss: 623.1399, Val Loss: 225.7552\n",
            "Epoch 5066, Train Loss: 611.3520, Val Loss: 212.7480\n",
            "Epoch 5067, Train Loss: 602.2069, Val Loss: 213.7692\n",
            "Epoch 5068, Train Loss: 608.7888, Val Loss: 232.1507\n",
            "Epoch 5069, Train Loss: 641.9250, Val Loss: 189.9961\n",
            "Epoch 5070, Train Loss: 619.0598, Val Loss: 249.7840\n",
            "Epoch 5071, Train Loss: 628.2231, Val Loss: 296.4915\n",
            "Epoch 5072, Train Loss: 623.4213, Val Loss: 177.4028\n",
            "Epoch 5073, Train Loss: 589.6120, Val Loss: 171.8716\n",
            "Epoch 5074, Train Loss: 610.7624, Val Loss: 174.1453\n",
            "Epoch 5075, Train Loss: 613.2943, Val Loss: 180.7252\n",
            "Epoch 5076, Train Loss: 614.3007, Val Loss: 179.9220\n",
            "Epoch 5077, Train Loss: 629.7237, Val Loss: 175.4689\n",
            "Epoch 5078, Train Loss: 621.9766, Val Loss: 177.3202\n",
            "Epoch 5079, Train Loss: 610.5524, Val Loss: 179.4624\n",
            "Epoch 5080, Train Loss: 614.2082, Val Loss: 172.2857\n",
            "Epoch 5081, Train Loss: 605.5767, Val Loss: 170.9451\n",
            "Epoch 5082, Train Loss: 620.4857, Val Loss: 172.2405\n",
            "Epoch 5083, Train Loss: 621.9249, Val Loss: 199.6592\n",
            "Epoch 5084, Train Loss: 621.8090, Val Loss: 202.6353\n",
            "Epoch 5085, Train Loss: 617.2553, Val Loss: 199.0170\n",
            "Epoch 5086, Train Loss: 604.4253, Val Loss: 228.0656\n",
            "Epoch 5087, Train Loss: 627.0941, Val Loss: 218.5623\n",
            "Epoch 5088, Train Loss: 621.4384, Val Loss: 187.5097\n",
            "Epoch 5089, Train Loss: 625.2060, Val Loss: 180.4428\n",
            "Epoch 5090, Train Loss: 627.6517, Val Loss: 178.8372\n",
            "Epoch 5091, Train Loss: 616.4619, Val Loss: 176.7302\n",
            "Epoch 5092, Train Loss: 621.3970, Val Loss: 203.1284\n",
            "Epoch 5093, Train Loss: 623.4642, Val Loss: 213.5287\n",
            "Epoch 5094, Train Loss: 614.9543, Val Loss: 204.0580\n",
            "Epoch 5095, Train Loss: 605.0900, Val Loss: 191.3456\n",
            "Epoch 5096, Train Loss: 619.5843, Val Loss: 181.2359\n",
            "Epoch 5097, Train Loss: 625.3977, Val Loss: 179.7889\n",
            "Epoch 5098, Train Loss: 627.6950, Val Loss: 252.1450\n",
            "Epoch 5099, Train Loss: 596.5735, Val Loss: 416.6589\n",
            "Epoch 5100, Train Loss: 613.2121, Val Loss: 349.6789\n",
            "Epoch 5101, Train Loss: 598.4590, Val Loss: 285.7699\n",
            "Epoch 5102, Train Loss: 594.9926, Val Loss: 233.0889\n",
            "Epoch 5103, Train Loss: 596.8371, Val Loss: 197.8768\n",
            "Epoch 5104, Train Loss: 606.2724, Val Loss: 212.2715\n",
            "Epoch 5105, Train Loss: 628.2220, Val Loss: 222.3441\n",
            "Epoch 5106, Train Loss: 584.7088, Val Loss: 179.5850\n",
            "Epoch 5107, Train Loss: 604.2065, Val Loss: 176.7255\n",
            "Epoch 5108, Train Loss: 607.9181, Val Loss: 179.1826\n",
            "Epoch 5109, Train Loss: 616.3747, Val Loss: 205.5146\n",
            "Epoch 5110, Train Loss: 623.1384, Val Loss: 226.7728\n",
            "Epoch 5111, Train Loss: 595.0487, Val Loss: 215.0496\n",
            "Epoch 5112, Train Loss: 604.6347, Val Loss: 209.1545\n",
            "Epoch 5113, Train Loss: 626.8940, Val Loss: 196.1658\n",
            "Epoch 5114, Train Loss: 610.2074, Val Loss: 204.3603\n",
            "Epoch 5115, Train Loss: 612.3258, Val Loss: 238.9977\n",
            "Epoch 5116, Train Loss: 618.6746, Val Loss: 258.6810\n",
            "Epoch 5117, Train Loss: 612.4028, Val Loss: 257.8154\n",
            "Epoch 5118, Train Loss: 585.2201, Val Loss: 270.6787\n",
            "Epoch 5119, Train Loss: 608.9938, Val Loss: 235.3477\n",
            "Epoch 5120, Train Loss: 620.0048, Val Loss: 180.2408\n",
            "Epoch 5121, Train Loss: 594.0236, Val Loss: 177.8025\n",
            "Epoch 5122, Train Loss: 615.7039, Val Loss: 219.7231\n",
            "Epoch 5123, Train Loss: 624.0115, Val Loss: 196.0776\n",
            "Epoch 5124, Train Loss: 622.7846, Val Loss: 187.1362\n",
            "Epoch 5125, Train Loss: 562.4384, Val Loss: 213.3411\n",
            "Epoch 5126, Train Loss: 634.4605, Val Loss: 205.9539\n",
            "Epoch 5127, Train Loss: 625.5254, Val Loss: 186.3378\n",
            "Epoch 5128, Train Loss: 600.8902, Val Loss: 176.7985\n",
            "Epoch 5129, Train Loss: 603.6855, Val Loss: 189.7039\n",
            "Epoch 5130, Train Loss: 624.1705, Val Loss: 244.7117\n",
            "Epoch 5131, Train Loss: 625.1390, Val Loss: 225.2689\n",
            "Epoch 5132, Train Loss: 617.8165, Val Loss: 200.6932\n",
            "Epoch 5133, Train Loss: 582.0514, Val Loss: 198.4078\n",
            "Epoch 5134, Train Loss: 574.6874, Val Loss: 219.9540\n",
            "Epoch 5135, Train Loss: 612.5658, Val Loss: 219.8548\n",
            "Epoch 5136, Train Loss: 618.2994, Val Loss: 187.1184\n",
            "Epoch 5137, Train Loss: 614.9634, Val Loss: 185.9868\n",
            "Epoch 5138, Train Loss: 607.8459, Val Loss: 192.7117\n",
            "Epoch 5139, Train Loss: 606.2948, Val Loss: 221.1631\n",
            "Epoch 5140, Train Loss: 623.5177, Val Loss: 213.9081\n",
            "Epoch 5141, Train Loss: 621.4019, Val Loss: 212.5361\n",
            "Epoch 5142, Train Loss: 600.3033, Val Loss: 232.0385\n",
            "Epoch 5143, Train Loss: 620.4773, Val Loss: 245.5923\n",
            "Epoch 5144, Train Loss: 556.7112, Val Loss: 267.0690\n",
            "Epoch 5145, Train Loss: 618.7282, Val Loss: 179.1477\n",
            "Epoch 5146, Train Loss: 625.1854, Val Loss: 178.4848\n",
            "Epoch 5147, Train Loss: 591.7609, Val Loss: 173.8977\n",
            "Epoch 5148, Train Loss: 596.7419, Val Loss: 200.7640\n",
            "Epoch 5149, Train Loss: 580.2355, Val Loss: 297.3043\n",
            "Epoch 5150, Train Loss: 587.3265, Val Loss: 294.0915\n",
            "Epoch 5151, Train Loss: 621.0834, Val Loss: 213.9550\n",
            "Epoch 5152, Train Loss: 615.6600, Val Loss: 177.7823\n",
            "Epoch 5153, Train Loss: 591.1066, Val Loss: 194.6452\n",
            "Epoch 5154, Train Loss: 621.6070, Val Loss: 235.9465\n",
            "Epoch 5155, Train Loss: 618.5003, Val Loss: 237.0108\n",
            "Epoch 5156, Train Loss: 621.2650, Val Loss: 190.2742\n",
            "Epoch 5157, Train Loss: 630.7821, Val Loss: 204.5148\n",
            "Epoch 5158, Train Loss: 624.6972, Val Loss: 261.1594\n",
            "Epoch 5159, Train Loss: 624.9219, Val Loss: 253.3316\n",
            "Epoch 5160, Train Loss: 603.4211, Val Loss: 181.4246\n",
            "Epoch 5161, Train Loss: 617.1976, Val Loss: 177.3482\n",
            "Epoch 5162, Train Loss: 584.2288, Val Loss: 184.8102\n",
            "Epoch 5163, Train Loss: 620.0271, Val Loss: 214.3180\n",
            "Epoch 5164, Train Loss: 611.1537, Val Loss: 221.8145\n",
            "Epoch 5165, Train Loss: 633.3125, Val Loss: 192.6792\n",
            "Epoch 5166, Train Loss: 622.2913, Val Loss: 185.8149\n",
            "Epoch 5167, Train Loss: 610.4846, Val Loss: 178.2092\n",
            "Epoch 5168, Train Loss: 618.5040, Val Loss: 183.6970\n",
            "Epoch 5169, Train Loss: 617.7972, Val Loss: 186.5521\n",
            "Epoch 5170, Train Loss: 622.9365, Val Loss: 185.5108\n",
            "Epoch 5171, Train Loss: 623.9004, Val Loss: 176.6541\n",
            "Epoch 5172, Train Loss: 624.0440, Val Loss: 181.4649\n",
            "Epoch 5173, Train Loss: 607.4909, Val Loss: 194.0958\n",
            "Epoch 5174, Train Loss: 563.9623, Val Loss: 325.7213\n",
            "Epoch 5175, Train Loss: 595.0316, Val Loss: 568.4561\n",
            "Epoch 5176, Train Loss: 627.9552, Val Loss: 269.5962\n",
            "Epoch 5177, Train Loss: 629.9553, Val Loss: 177.9750\n",
            "Epoch 5178, Train Loss: 622.8503, Val Loss: 170.4070\n",
            "Epoch 5179, Train Loss: 625.9929, Val Loss: 188.7627\n",
            "Epoch 5180, Train Loss: 616.1925, Val Loss: 299.3663\n",
            "Epoch 5181, Train Loss: 608.6801, Val Loss: 314.0684\n",
            "Epoch 5182, Train Loss: 624.4045, Val Loss: 229.9800\n",
            "Epoch 5183, Train Loss: 621.8177, Val Loss: 193.7872\n",
            "Epoch 5184, Train Loss: 597.4331, Val Loss: 194.8222\n",
            "Epoch 5185, Train Loss: 628.2281, Val Loss: 230.1049\n",
            "Epoch 5186, Train Loss: 608.6185, Val Loss: 241.9693\n",
            "Epoch 5187, Train Loss: 622.9782, Val Loss: 231.3020\n",
            "Epoch 5188, Train Loss: 622.3260, Val Loss: 225.5025\n",
            "Epoch 5189, Train Loss: 626.9621, Val Loss: 218.3890\n",
            "Epoch 5190, Train Loss: 571.7793, Val Loss: 243.1609\n",
            "Epoch 5191, Train Loss: 610.3827, Val Loss: 325.9783\n",
            "Epoch 5192, Train Loss: 630.2318, Val Loss: 241.4995\n",
            "Epoch 5193, Train Loss: 611.7892, Val Loss: 180.7192\n",
            "Epoch 5194, Train Loss: 615.1181, Val Loss: 178.3417\n",
            "Epoch 5195, Train Loss: 619.3985, Val Loss: 171.9027\n",
            "Epoch 5196, Train Loss: 623.8922, Val Loss: 203.8690\n",
            "Epoch 5197, Train Loss: 609.8064, Val Loss: 205.2885\n",
            "Epoch 5198, Train Loss: 582.8703, Val Loss: 188.0998\n",
            "Epoch 5199, Train Loss: 621.6231, Val Loss: 184.8431\n",
            "Epoch 5200, Train Loss: 556.6880, Val Loss: 186.3869\n",
            "Epoch 5201, Train Loss: 625.4191, Val Loss: 240.1338\n",
            "Epoch 5202, Train Loss: 612.8185, Val Loss: 214.5856\n",
            "Epoch 5203, Train Loss: 623.1607, Val Loss: 194.2729\n",
            "Epoch 5204, Train Loss: 611.9588, Val Loss: 188.4638\n",
            "Epoch 5205, Train Loss: 570.4119, Val Loss: 200.3040\n",
            "Epoch 5206, Train Loss: 618.6284, Val Loss: 207.5595\n",
            "Epoch 5207, Train Loss: 623.7459, Val Loss: 214.7997\n",
            "Epoch 5208, Train Loss: 606.3001, Val Loss: 210.0430\n",
            "Epoch 5209, Train Loss: 619.0886, Val Loss: 208.1739\n",
            "Epoch 5210, Train Loss: 595.5869, Val Loss: 219.1243\n",
            "Epoch 5211, Train Loss: 600.4156, Val Loss: 240.6218\n",
            "Epoch 5212, Train Loss: 623.4223, Val Loss: 328.2295\n",
            "Epoch 5213, Train Loss: 614.1183, Val Loss: 378.6609\n",
            "Epoch 5214, Train Loss: 622.6239, Val Loss: 211.1107\n",
            "Epoch 5215, Train Loss: 614.8894, Val Loss: 206.2421\n",
            "Epoch 5216, Train Loss: 631.7316, Val Loss: 259.9418\n",
            "Epoch 5217, Train Loss: 634.7912, Val Loss: 266.3980\n",
            "Epoch 5218, Train Loss: 616.4430, Val Loss: 218.0831\n",
            "Epoch 5219, Train Loss: 620.9451, Val Loss: 225.2065\n",
            "Epoch 5220, Train Loss: 633.2442, Val Loss: 255.5173\n",
            "Epoch 5221, Train Loss: 604.4104, Val Loss: 296.6767\n",
            "Epoch 5222, Train Loss: 623.8021, Val Loss: 309.8161\n",
            "Epoch 5223, Train Loss: 613.9490, Val Loss: 216.4146\n",
            "Epoch 5224, Train Loss: 624.6483, Val Loss: 179.6610\n",
            "Epoch 5225, Train Loss: 609.8341, Val Loss: 175.8676\n",
            "Epoch 5226, Train Loss: 612.7124, Val Loss: 232.5274\n",
            "Epoch 5227, Train Loss: 604.3250, Val Loss: 319.8935\n",
            "Epoch 5228, Train Loss: 624.2455, Val Loss: 315.1641\n",
            "Epoch 5229, Train Loss: 570.3745, Val Loss: 237.0758\n",
            "Epoch 5230, Train Loss: 587.9492, Val Loss: 185.8248\n",
            "Epoch 5231, Train Loss: 630.0780, Val Loss: 177.0358\n",
            "Epoch 5232, Train Loss: 624.9588, Val Loss: 176.5679\n",
            "Epoch 5233, Train Loss: 616.4392, Val Loss: 200.0104\n",
            "Epoch 5234, Train Loss: 611.7667, Val Loss: 234.4735\n",
            "Epoch 5235, Train Loss: 621.7837, Val Loss: 245.8029\n",
            "Epoch 5236, Train Loss: 616.5123, Val Loss: 206.5357\n",
            "Epoch 5237, Train Loss: 593.9765, Val Loss: 175.8795\n",
            "Epoch 5238, Train Loss: 629.3382, Val Loss: 182.7025\n",
            "Epoch 5239, Train Loss: 619.2603, Val Loss: 214.1478\n",
            "Epoch 5240, Train Loss: 627.8271, Val Loss: 219.6862\n",
            "Epoch 5241, Train Loss: 603.2911, Val Loss: 199.5864\n",
            "Epoch 5242, Train Loss: 623.2706, Val Loss: 177.7379\n",
            "Epoch 5243, Train Loss: 621.3136, Val Loss: 173.1553\n",
            "Epoch 5244, Train Loss: 616.0472, Val Loss: 177.9280\n",
            "Epoch 5245, Train Loss: 617.0481, Val Loss: 197.7096\n",
            "Epoch 5246, Train Loss: 610.2786, Val Loss: 227.3149\n",
            "Epoch 5247, Train Loss: 634.8525, Val Loss: 267.8087\n",
            "Epoch 5248, Train Loss: 632.1987, Val Loss: 215.5956\n",
            "Epoch 5249, Train Loss: 616.9385, Val Loss: 195.9397\n",
            "Epoch 5250, Train Loss: 626.6286, Val Loss: 191.3516\n",
            "Epoch 5251, Train Loss: 616.1565, Val Loss: 198.4987\n",
            "Epoch 5252, Train Loss: 588.3256, Val Loss: 217.5397\n",
            "Epoch 5253, Train Loss: 626.1900, Val Loss: 258.6013\n",
            "Epoch 5254, Train Loss: 625.1481, Val Loss: 307.4796\n",
            "Epoch 5255, Train Loss: 612.7274, Val Loss: 334.6750\n",
            "Epoch 5256, Train Loss: 631.8058, Val Loss: 283.3129\n",
            "Epoch 5257, Train Loss: 585.7628, Val Loss: 182.6982\n",
            "Epoch 5258, Train Loss: 594.5887, Val Loss: 173.6548\n",
            "Epoch 5259, Train Loss: 621.2963, Val Loss: 170.5453\n",
            "Epoch 5260, Train Loss: 604.2768, Val Loss: 174.1932\n",
            "Epoch 5261, Train Loss: 618.2154, Val Loss: 200.8161\n",
            "Epoch 5262, Train Loss: 616.7761, Val Loss: 221.4046\n",
            "Epoch 5263, Train Loss: 625.1011, Val Loss: 232.2057\n",
            "Epoch 5264, Train Loss: 618.2115, Val Loss: 238.8296\n",
            "Epoch 5265, Train Loss: 631.6988, Val Loss: 249.7029\n",
            "Epoch 5266, Train Loss: 617.7195, Val Loss: 239.0359\n",
            "Epoch 5267, Train Loss: 607.2626, Val Loss: 202.8544\n",
            "Epoch 5268, Train Loss: 619.0946, Val Loss: 200.4974\n",
            "Epoch 5269, Train Loss: 608.4195, Val Loss: 203.5027\n",
            "Epoch 5270, Train Loss: 613.5948, Val Loss: 238.0106\n",
            "Epoch 5271, Train Loss: 608.6421, Val Loss: 224.8283\n",
            "Epoch 5272, Train Loss: 615.2681, Val Loss: 196.5342\n",
            "Epoch 5273, Train Loss: 616.2410, Val Loss: 186.4767\n",
            "Epoch 5274, Train Loss: 604.5605, Val Loss: 206.2629\n",
            "Epoch 5275, Train Loss: 609.5192, Val Loss: 193.5973\n",
            "Epoch 5276, Train Loss: 624.4407, Val Loss: 201.3082\n",
            "Epoch 5277, Train Loss: 603.7047, Val Loss: 214.5560\n",
            "Epoch 5278, Train Loss: 617.1748, Val Loss: 173.0915\n",
            "Epoch 5279, Train Loss: 606.9841, Val Loss: 172.1881\n",
            "Epoch 5280, Train Loss: 613.2812, Val Loss: 176.9408\n",
            "Epoch 5281, Train Loss: 600.3361, Val Loss: 219.3798\n",
            "Epoch 5282, Train Loss: 621.1704, Val Loss: 247.3399\n",
            "Epoch 5283, Train Loss: 629.9668, Val Loss: 186.9509\n",
            "Epoch 5284, Train Loss: 619.2918, Val Loss: 171.3801\n",
            "Epoch 5285, Train Loss: 615.6230, Val Loss: 175.9530\n",
            "Epoch 5286, Train Loss: 577.2934, Val Loss: 230.1155\n",
            "Epoch 5287, Train Loss: 572.4901, Val Loss: 280.6214\n",
            "Epoch 5288, Train Loss: 596.9756, Val Loss: 405.0144\n",
            "Epoch 5289, Train Loss: 588.8829, Val Loss: 214.9147\n",
            "Epoch 5290, Train Loss: 612.4878, Val Loss: 202.8330\n",
            "Epoch 5291, Train Loss: 586.2427, Val Loss: 236.7561\n",
            "Epoch 5292, Train Loss: 618.5342, Val Loss: 272.0043\n",
            "Epoch 5293, Train Loss: 612.0151, Val Loss: 211.3888\n",
            "Epoch 5294, Train Loss: 616.5070, Val Loss: 203.9534\n",
            "Epoch 5295, Train Loss: 609.0890, Val Loss: 209.1554\n",
            "Epoch 5296, Train Loss: 630.6815, Val Loss: 207.1191\n",
            "Epoch 5297, Train Loss: 601.4967, Val Loss: 224.1157\n",
            "Epoch 5298, Train Loss: 626.5815, Val Loss: 228.2706\n",
            "Epoch 5299, Train Loss: 624.2641, Val Loss: 236.4603\n",
            "Epoch 5300, Train Loss: 584.5559, Val Loss: 205.6312\n",
            "Epoch 5301, Train Loss: 602.9529, Val Loss: 188.9424\n",
            "Epoch 5302, Train Loss: 580.7762, Val Loss: 184.9439\n",
            "Epoch 5303, Train Loss: 619.5333, Val Loss: 205.9505\n",
            "Epoch 5304, Train Loss: 614.0117, Val Loss: 238.4532\n",
            "Epoch 5305, Train Loss: 625.4217, Val Loss: 278.1664\n",
            "Epoch 5306, Train Loss: 595.1850, Val Loss: 175.0087\n",
            "Epoch 5307, Train Loss: 635.5581, Val Loss: 173.8213\n",
            "Epoch 5308, Train Loss: 625.3737, Val Loss: 180.2624\n",
            "Epoch 5309, Train Loss: 621.9058, Val Loss: 266.7156\n",
            "Epoch 5310, Train Loss: 610.8620, Val Loss: 271.9088\n",
            "Epoch 5311, Train Loss: 616.6020, Val Loss: 209.9563\n",
            "Epoch 5312, Train Loss: 624.8828, Val Loss: 176.4666\n",
            "Epoch 5313, Train Loss: 611.6015, Val Loss: 172.1808\n",
            "Epoch 5314, Train Loss: 615.9260, Val Loss: 176.4848\n",
            "Epoch 5315, Train Loss: 628.0214, Val Loss: 182.1125\n",
            "Epoch 5316, Train Loss: 615.9230, Val Loss: 184.0221\n",
            "Epoch 5317, Train Loss: 608.3324, Val Loss: 201.8178\n",
            "Epoch 5318, Train Loss: 621.0424, Val Loss: 215.6783\n",
            "Epoch 5319, Train Loss: 558.2456, Val Loss: 231.9787\n",
            "Epoch 5320, Train Loss: 613.8551, Val Loss: 222.4878\n",
            "Epoch 5321, Train Loss: 616.8345, Val Loss: 212.1446\n",
            "Epoch 5322, Train Loss: 605.8977, Val Loss: 209.3369\n",
            "Epoch 5323, Train Loss: 621.8047, Val Loss: 195.6516\n",
            "Epoch 5324, Train Loss: 616.3704, Val Loss: 188.9903\n",
            "Epoch 5325, Train Loss: 594.5089, Val Loss: 188.6201\n",
            "Epoch 5326, Train Loss: 599.9720, Val Loss: 199.0868\n",
            "Epoch 5327, Train Loss: 620.8852, Val Loss: 214.7031\n",
            "Epoch 5328, Train Loss: 588.4058, Val Loss: 209.4625\n",
            "Epoch 5329, Train Loss: 572.4234, Val Loss: 192.6697\n",
            "Epoch 5330, Train Loss: 601.8897, Val Loss: 179.3525\n",
            "Epoch 5331, Train Loss: 622.2741, Val Loss: 178.0393\n",
            "Epoch 5332, Train Loss: 631.7726, Val Loss: 183.5987\n",
            "Epoch 5333, Train Loss: 627.7255, Val Loss: 185.8321\n",
            "Epoch 5334, Train Loss: 622.5261, Val Loss: 226.7411\n",
            "Epoch 5335, Train Loss: 581.4547, Val Loss: 291.9884\n",
            "Epoch 5336, Train Loss: 606.5199, Val Loss: 296.2140\n",
            "Epoch 5337, Train Loss: 615.0150, Val Loss: 284.0281\n",
            "Epoch 5338, Train Loss: 591.6385, Val Loss: 229.3192\n",
            "Epoch 5339, Train Loss: 626.0586, Val Loss: 195.0475\n",
            "Epoch 5340, Train Loss: 619.5055, Val Loss: 173.8168\n",
            "Epoch 5341, Train Loss: 622.0923, Val Loss: 201.2784\n",
            "Epoch 5342, Train Loss: 625.7612, Val Loss: 250.1186\n",
            "Epoch 5343, Train Loss: 631.3027, Val Loss: 222.8375\n",
            "Epoch 5344, Train Loss: 606.7423, Val Loss: 194.3505\n",
            "Epoch 5345, Train Loss: 625.5861, Val Loss: 197.7225\n",
            "Epoch 5346, Train Loss: 603.5724, Val Loss: 208.8290\n",
            "Epoch 5347, Train Loss: 636.7622, Val Loss: 216.9034\n",
            "Epoch 5348, Train Loss: 602.3315, Val Loss: 208.8263\n",
            "Epoch 5349, Train Loss: 584.8404, Val Loss: 219.7079\n",
            "Epoch 5350, Train Loss: 584.1011, Val Loss: 240.6077\n",
            "Epoch 5351, Train Loss: 583.8948, Val Loss: 235.6985\n",
            "Epoch 5352, Train Loss: 633.8101, Val Loss: 211.9868\n",
            "Epoch 5353, Train Loss: 603.7031, Val Loss: 216.9775\n",
            "Epoch 5354, Train Loss: 603.1759, Val Loss: 254.1352\n",
            "Epoch 5355, Train Loss: 588.4971, Val Loss: 269.4629\n",
            "Epoch 5356, Train Loss: 621.6606, Val Loss: 191.6753\n",
            "Epoch 5357, Train Loss: 608.1490, Val Loss: 177.6515\n",
            "Epoch 5358, Train Loss: 618.4902, Val Loss: 177.4496\n",
            "Epoch 5359, Train Loss: 586.9397, Val Loss: 217.0853\n",
            "Epoch 5360, Train Loss: 613.4414, Val Loss: 239.7410\n",
            "Epoch 5361, Train Loss: 616.3114, Val Loss: 209.8622\n",
            "Epoch 5362, Train Loss: 601.2703, Val Loss: 217.5970\n",
            "Epoch 5363, Train Loss: 605.9807, Val Loss: 214.5436\n",
            "Epoch 5364, Train Loss: 587.7393, Val Loss: 204.3324\n",
            "Epoch 5365, Train Loss: 603.8050, Val Loss: 254.8311\n",
            "Epoch 5366, Train Loss: 610.9017, Val Loss: 218.8043\n",
            "Epoch 5367, Train Loss: 613.6313, Val Loss: 212.8047\n",
            "Epoch 5368, Train Loss: 573.5221, Val Loss: 238.2317\n",
            "Epoch 5369, Train Loss: 579.1004, Val Loss: 277.3386\n",
            "Epoch 5370, Train Loss: 616.5285, Val Loss: 213.4434\n",
            "Epoch 5371, Train Loss: 610.3780, Val Loss: 219.3682\n",
            "Epoch 5372, Train Loss: 569.8494, Val Loss: 193.1316\n",
            "Epoch 5373, Train Loss: 619.0780, Val Loss: 275.6416\n",
            "Epoch 5374, Train Loss: 628.8478, Val Loss: 277.1139\n",
            "Epoch 5375, Train Loss: 629.9670, Val Loss: 210.8862\n",
            "Epoch 5376, Train Loss: 624.4577, Val Loss: 176.4421\n",
            "Epoch 5377, Train Loss: 570.6500, Val Loss: 165.7403\n",
            "Epoch 5378, Train Loss: 612.0519, Val Loss: 176.9752\n",
            "Epoch 5379, Train Loss: 577.8961, Val Loss: 242.4791\n",
            "Epoch 5380, Train Loss: 639.7100, Val Loss: 218.8123\n",
            "Epoch 5381, Train Loss: 602.4371, Val Loss: 173.2545\n",
            "Epoch 5382, Train Loss: 630.1664, Val Loss: 176.8633\n",
            "Epoch 5383, Train Loss: 622.8581, Val Loss: 173.5250\n",
            "Epoch 5384, Train Loss: 592.8305, Val Loss: 181.9707\n",
            "Epoch 5385, Train Loss: 617.2425, Val Loss: 323.6944\n",
            "Epoch 5386, Train Loss: 589.2290, Val Loss: 446.4546\n",
            "Epoch 5387, Train Loss: 628.7328, Val Loss: 466.3741\n",
            "Epoch 5388, Train Loss: 608.9871, Val Loss: 261.7046\n",
            "Epoch 5389, Train Loss: 629.7823, Val Loss: 181.6475\n",
            "Epoch 5390, Train Loss: 622.8387, Val Loss: 181.9201\n",
            "Epoch 5391, Train Loss: 619.3013, Val Loss: 182.4366\n",
            "Epoch 5392, Train Loss: 606.2595, Val Loss: 185.4305\n",
            "Epoch 5393, Train Loss: 596.6547, Val Loss: 177.6081\n",
            "Epoch 5394, Train Loss: 594.2716, Val Loss: 184.1213\n",
            "Epoch 5395, Train Loss: 624.2033, Val Loss: 199.2787\n",
            "Epoch 5396, Train Loss: 577.6052, Val Loss: 238.1768\n",
            "Epoch 5397, Train Loss: 619.0556, Val Loss: 243.8600\n",
            "Epoch 5398, Train Loss: 574.2447, Val Loss: 234.2022\n",
            "Epoch 5399, Train Loss: 621.5422, Val Loss: 230.8596\n",
            "Epoch 5400, Train Loss: 601.8487, Val Loss: 219.1358\n",
            "Epoch 5401, Train Loss: 641.5108, Val Loss: 225.3874\n",
            "Epoch 5402, Train Loss: 630.0840, Val Loss: 173.9780\n",
            "Epoch 5403, Train Loss: 601.3306, Val Loss: 181.6918\n",
            "Epoch 5404, Train Loss: 591.4955, Val Loss: 198.2182\n",
            "Epoch 5405, Train Loss: 619.4809, Val Loss: 183.3488\n",
            "Epoch 5406, Train Loss: 575.1927, Val Loss: 201.8459\n",
            "Epoch 5407, Train Loss: 615.3047, Val Loss: 216.4084\n",
            "Epoch 5408, Train Loss: 617.1229, Val Loss: 223.5743\n",
            "Epoch 5409, Train Loss: 611.7914, Val Loss: 215.8454\n",
            "Epoch 5410, Train Loss: 624.2871, Val Loss: 242.6318\n",
            "Epoch 5411, Train Loss: 597.5762, Val Loss: 298.5818\n",
            "Epoch 5412, Train Loss: 596.4665, Val Loss: 315.5532\n",
            "Epoch 5413, Train Loss: 617.6722, Val Loss: 288.7535\n",
            "Epoch 5414, Train Loss: 595.0597, Val Loss: 211.4374\n",
            "Epoch 5415, Train Loss: 592.7306, Val Loss: 178.9045\n",
            "Epoch 5416, Train Loss: 604.1689, Val Loss: 185.6630\n",
            "Epoch 5417, Train Loss: 628.5878, Val Loss: 227.0325\n",
            "Epoch 5418, Train Loss: 613.0360, Val Loss: 263.3642\n",
            "Epoch 5419, Train Loss: 626.8739, Val Loss: 296.4195\n",
            "Epoch 5420, Train Loss: 555.9332, Val Loss: 209.1462\n",
            "Epoch 5421, Train Loss: 606.2783, Val Loss: 182.6891\n",
            "Epoch 5422, Train Loss: 554.5626, Val Loss: 179.1029\n",
            "Epoch 5423, Train Loss: 618.5586, Val Loss: 179.6628\n",
            "Epoch 5424, Train Loss: 616.4424, Val Loss: 174.6201\n",
            "Epoch 5425, Train Loss: 606.8059, Val Loss: 175.5139\n",
            "Epoch 5426, Train Loss: 612.6420, Val Loss: 205.5689\n",
            "Epoch 5427, Train Loss: 619.4067, Val Loss: 244.8027\n",
            "Epoch 5428, Train Loss: 628.1024, Val Loss: 226.7896\n",
            "Epoch 5429, Train Loss: 598.6765, Val Loss: 200.4599\n",
            "Epoch 5430, Train Loss: 586.2005, Val Loss: 200.5433\n",
            "Epoch 5431, Train Loss: 628.2231, Val Loss: 196.1305\n",
            "Epoch 5432, Train Loss: 551.5935, Val Loss: 189.7146\n",
            "Epoch 5433, Train Loss: 601.4199, Val Loss: 233.6035\n",
            "Epoch 5434, Train Loss: 624.7911, Val Loss: 289.3261\n",
            "Epoch 5435, Train Loss: 612.4819, Val Loss: 241.1214\n",
            "Epoch 5436, Train Loss: 594.2628, Val Loss: 201.0726\n",
            "Epoch 5437, Train Loss: 618.7602, Val Loss: 172.1788\n",
            "Epoch 5438, Train Loss: 590.7904, Val Loss: 178.3565\n",
            "Epoch 5439, Train Loss: 624.2688, Val Loss: 194.3392\n",
            "Epoch 5440, Train Loss: 543.9576, Val Loss: 196.0601\n",
            "Epoch 5441, Train Loss: 625.6187, Val Loss: 172.3532\n",
            "Epoch 5442, Train Loss: 592.1836, Val Loss: 173.2776\n",
            "Epoch 5443, Train Loss: 620.0297, Val Loss: 202.3637\n",
            "Epoch 5444, Train Loss: 604.6031, Val Loss: 223.1687\n",
            "Epoch 5445, Train Loss: 617.0629, Val Loss: 205.4786\n",
            "Epoch 5446, Train Loss: 605.0725, Val Loss: 193.2961\n",
            "Epoch 5447, Train Loss: 634.5968, Val Loss: 193.1467\n",
            "Epoch 5448, Train Loss: 620.7479, Val Loss: 194.8221\n",
            "Epoch 5449, Train Loss: 622.5559, Val Loss: 193.6497\n",
            "Epoch 5450, Train Loss: 627.2278, Val Loss: 187.1380\n",
            "Epoch 5451, Train Loss: 614.1763, Val Loss: 187.7479\n",
            "Epoch 5452, Train Loss: 612.2608, Val Loss: 182.4367\n",
            "Epoch 5453, Train Loss: 628.4664, Val Loss: 175.9108\n",
            "Epoch 5454, Train Loss: 623.0161, Val Loss: 210.0089\n",
            "Epoch 5455, Train Loss: 598.9746, Val Loss: 224.8850\n",
            "Epoch 5456, Train Loss: 599.0569, Val Loss: 213.3923\n",
            "Epoch 5457, Train Loss: 617.3519, Val Loss: 205.8837\n",
            "Epoch 5458, Train Loss: 608.8082, Val Loss: 193.3509\n",
            "Epoch 5459, Train Loss: 583.6424, Val Loss: 198.8841\n",
            "Epoch 5460, Train Loss: 623.3264, Val Loss: 200.3139\n",
            "Epoch 5461, Train Loss: 587.4787, Val Loss: 278.2238\n",
            "Epoch 5462, Train Loss: 589.8661, Val Loss: 320.2311\n",
            "Epoch 5463, Train Loss: 617.5679, Val Loss: 287.4485\n",
            "Epoch 5464, Train Loss: 609.6986, Val Loss: 185.5735\n",
            "Epoch 5465, Train Loss: 595.9431, Val Loss: 193.1573\n",
            "Epoch 5466, Train Loss: 614.2414, Val Loss: 282.0136\n",
            "Epoch 5467, Train Loss: 620.9152, Val Loss: 375.2604\n",
            "Epoch 5468, Train Loss: 619.9071, Val Loss: 292.2864\n",
            "Epoch 5469, Train Loss: 578.5949, Val Loss: 173.0645\n",
            "Epoch 5470, Train Loss: 618.9333, Val Loss: 173.4365\n",
            "Epoch 5471, Train Loss: 632.5601, Val Loss: 180.7530\n",
            "Epoch 5472, Train Loss: 589.7742, Val Loss: 184.5225\n",
            "Epoch 5473, Train Loss: 632.3470, Val Loss: 182.9692\n",
            "Epoch 5474, Train Loss: 630.0057, Val Loss: 176.1700\n",
            "Epoch 5475, Train Loss: 592.2599, Val Loss: 176.4731\n",
            "Epoch 5476, Train Loss: 607.3813, Val Loss: 212.8823\n",
            "Epoch 5477, Train Loss: 624.2626, Val Loss: 245.0772\n",
            "Epoch 5478, Train Loss: 627.3558, Val Loss: 252.6686\n",
            "Epoch 5479, Train Loss: 595.8730, Val Loss: 238.9459\n",
            "Epoch 5480, Train Loss: 598.1189, Val Loss: 234.9710\n",
            "Epoch 5481, Train Loss: 570.5103, Val Loss: 225.8360\n",
            "Epoch 5482, Train Loss: 621.0149, Val Loss: 232.4078\n",
            "Epoch 5483, Train Loss: 609.3168, Val Loss: 198.1467\n",
            "Epoch 5484, Train Loss: 619.4103, Val Loss: 174.1386\n",
            "Epoch 5485, Train Loss: 611.1216, Val Loss: 167.3060\n",
            "Epoch 5486, Train Loss: 620.4181, Val Loss: 173.3425\n",
            "Epoch 5487, Train Loss: 624.0846, Val Loss: 176.4817\n",
            "Epoch 5488, Train Loss: 622.9671, Val Loss: 181.6686\n",
            "Epoch 5489, Train Loss: 621.2474, Val Loss: 172.7066\n",
            "Epoch 5490, Train Loss: 628.5619, Val Loss: 179.0287\n",
            "Epoch 5491, Train Loss: 573.2730, Val Loss: 221.8276\n",
            "Epoch 5492, Train Loss: 633.6017, Val Loss: 235.5287\n",
            "Epoch 5493, Train Loss: 616.1023, Val Loss: 220.5486\n",
            "Epoch 5494, Train Loss: 616.0463, Val Loss: 220.7578\n",
            "Epoch 5495, Train Loss: 607.3066, Val Loss: 240.6192\n",
            "Epoch 5496, Train Loss: 608.6810, Val Loss: 285.3502\n",
            "Epoch 5497, Train Loss: 617.5120, Val Loss: 246.8780\n",
            "Epoch 5498, Train Loss: 616.7023, Val Loss: 215.9241\n",
            "Epoch 5499, Train Loss: 582.9313, Val Loss: 265.8705\n",
            "Epoch 5500, Train Loss: 574.2897, Val Loss: 228.0041\n",
            "Epoch 5501, Train Loss: 620.5940, Val Loss: 367.6490\n",
            "Epoch 5502, Train Loss: 611.3707, Val Loss: 340.0497\n",
            "Epoch 5503, Train Loss: 627.2639, Val Loss: 237.6268\n",
            "Epoch 5504, Train Loss: 632.0161, Val Loss: 173.4715\n",
            "Epoch 5505, Train Loss: 605.7200, Val Loss: 168.6354\n",
            "Epoch 5506, Train Loss: 626.3481, Val Loss: 167.0359\n",
            "Epoch 5507, Train Loss: 612.0191, Val Loss: 166.6811\n",
            "Epoch 5508, Train Loss: 612.2737, Val Loss: 182.9659\n",
            "Epoch 5509, Train Loss: 614.5260, Val Loss: 198.0304\n",
            "Epoch 5510, Train Loss: 615.4119, Val Loss: 182.0893\n",
            "Epoch 5511, Train Loss: 615.2961, Val Loss: 183.2283\n",
            "Epoch 5512, Train Loss: 624.7016, Val Loss: 203.0710\n",
            "Epoch 5513, Train Loss: 642.2148, Val Loss: 203.9366\n",
            "Epoch 5514, Train Loss: 622.0104, Val Loss: 195.9521\n",
            "Epoch 5515, Train Loss: 603.3732, Val Loss: 173.9519\n",
            "Epoch 5516, Train Loss: 559.3040, Val Loss: 197.4161\n",
            "Epoch 5517, Train Loss: 620.2345, Val Loss: 224.2541\n",
            "Epoch 5518, Train Loss: 607.2588, Val Loss: 217.6269\n",
            "Epoch 5519, Train Loss: 624.6334, Val Loss: 195.0525\n",
            "Epoch 5520, Train Loss: 611.0174, Val Loss: 175.2382\n",
            "Epoch 5521, Train Loss: 623.1974, Val Loss: 173.8848\n",
            "Epoch 5522, Train Loss: 624.3176, Val Loss: 172.1589\n",
            "Epoch 5523, Train Loss: 624.2841, Val Loss: 171.8338\n",
            "Epoch 5524, Train Loss: 589.6522, Val Loss: 204.1871\n",
            "Epoch 5525, Train Loss: 620.0389, Val Loss: 234.8774\n",
            "Epoch 5526, Train Loss: 552.1069, Val Loss: 294.0876\n",
            "Epoch 5527, Train Loss: 606.9211, Val Loss: 253.9858\n",
            "Epoch 5528, Train Loss: 615.8479, Val Loss: 196.8717\n",
            "Epoch 5529, Train Loss: 624.0652, Val Loss: 194.3881\n",
            "Epoch 5530, Train Loss: 591.1331, Val Loss: 256.7790\n",
            "Epoch 5531, Train Loss: 590.8742, Val Loss: 257.7145\n",
            "Epoch 5532, Train Loss: 624.7906, Val Loss: 198.0792\n",
            "Epoch 5533, Train Loss: 608.2708, Val Loss: 179.0912\n",
            "Epoch 5534, Train Loss: 614.2754, Val Loss: 182.9288\n",
            "Epoch 5535, Train Loss: 624.4449, Val Loss: 169.8924\n",
            "Epoch 5536, Train Loss: 626.7576, Val Loss: 170.2434\n",
            "Epoch 5537, Train Loss: 621.5765, Val Loss: 179.2519\n",
            "Epoch 5538, Train Loss: 621.2021, Val Loss: 189.3719\n",
            "Epoch 5539, Train Loss: 601.3450, Val Loss: 190.8865\n",
            "Epoch 5540, Train Loss: 611.7502, Val Loss: 203.6647\n",
            "Epoch 5541, Train Loss: 559.7627, Val Loss: 221.1320\n",
            "Epoch 5542, Train Loss: 593.3796, Val Loss: 239.2835\n",
            "Epoch 5543, Train Loss: 619.0838, Val Loss: 342.6977\n",
            "Epoch 5544, Train Loss: 609.2521, Val Loss: 381.8585\n",
            "Epoch 5545, Train Loss: 611.1272, Val Loss: 277.2433\n",
            "Epoch 5546, Train Loss: 576.3263, Val Loss: 173.3622\n",
            "Epoch 5547, Train Loss: 595.1889, Val Loss: 178.8546\n",
            "Epoch 5548, Train Loss: 628.8079, Val Loss: 315.0958\n",
            "Epoch 5549, Train Loss: 626.0790, Val Loss: 446.3384\n",
            "Epoch 5550, Train Loss: 597.3907, Val Loss: 333.1016\n",
            "Epoch 5551, Train Loss: 614.2894, Val Loss: 288.7467\n",
            "Epoch 5552, Train Loss: 629.0250, Val Loss: 303.0119\n",
            "Epoch 5553, Train Loss: 577.3570, Val Loss: 251.0524\n",
            "Epoch 5554, Train Loss: 608.1878, Val Loss: 221.4645\n",
            "Epoch 5555, Train Loss: 612.5398, Val Loss: 199.5156\n",
            "Epoch 5556, Train Loss: 609.5001, Val Loss: 184.9965\n",
            "Epoch 5557, Train Loss: 574.5390, Val Loss: 261.6881\n",
            "Epoch 5558, Train Loss: 623.3690, Val Loss: 351.0522\n",
            "Epoch 5559, Train Loss: 630.3449, Val Loss: 446.1184\n",
            "Epoch 5560, Train Loss: 611.7741, Val Loss: 247.6214\n",
            "Epoch 5561, Train Loss: 584.5143, Val Loss: 179.4395\n",
            "Epoch 5562, Train Loss: 615.7881, Val Loss: 193.8806\n",
            "Epoch 5563, Train Loss: 612.6517, Val Loss: 168.7417\n",
            "Epoch 5564, Train Loss: 589.0219, Val Loss: 208.0689\n",
            "Epoch 5565, Train Loss: 597.6707, Val Loss: 287.5343\n",
            "Epoch 5566, Train Loss: 603.8038, Val Loss: 237.6929\n",
            "Epoch 5567, Train Loss: 618.9231, Val Loss: 236.3752\n",
            "Epoch 5568, Train Loss: 627.2081, Val Loss: 217.8550\n",
            "Epoch 5569, Train Loss: 627.0720, Val Loss: 185.6669\n",
            "Epoch 5570, Train Loss: 596.0262, Val Loss: 175.3361\n",
            "Epoch 5571, Train Loss: 621.2673, Val Loss: 181.9643\n",
            "Epoch 5572, Train Loss: 615.8249, Val Loss: 174.2833\n",
            "Epoch 5573, Train Loss: 601.5365, Val Loss: 174.7276\n",
            "Epoch 5574, Train Loss: 592.6402, Val Loss: 213.9953\n",
            "Epoch 5575, Train Loss: 596.8160, Val Loss: 226.6729\n",
            "Epoch 5576, Train Loss: 623.0612, Val Loss: 187.2106\n",
            "Epoch 5577, Train Loss: 619.3902, Val Loss: 171.9492\n",
            "Epoch 5578, Train Loss: 616.4541, Val Loss: 168.7367\n",
            "Epoch 5579, Train Loss: 619.4017, Val Loss: 171.8711\n",
            "Epoch 5580, Train Loss: 624.1993, Val Loss: 182.5653\n",
            "Epoch 5581, Train Loss: 625.9674, Val Loss: 184.5020\n",
            "Epoch 5582, Train Loss: 626.9155, Val Loss: 218.5747\n",
            "Epoch 5583, Train Loss: 615.1461, Val Loss: 226.8548\n",
            "Epoch 5584, Train Loss: 610.3097, Val Loss: 188.9633\n",
            "Epoch 5585, Train Loss: 620.1476, Val Loss: 179.4364\n",
            "Epoch 5586, Train Loss: 622.6287, Val Loss: 243.5027\n",
            "Epoch 5587, Train Loss: 629.3830, Val Loss: 278.0461\n",
            "Epoch 5588, Train Loss: 624.8354, Val Loss: 229.9885\n",
            "Epoch 5589, Train Loss: 629.6666, Val Loss: 220.5340\n",
            "Epoch 5590, Train Loss: 600.4812, Val Loss: 212.0881\n",
            "Epoch 5591, Train Loss: 619.2952, Val Loss: 217.0182\n",
            "Epoch 5592, Train Loss: 590.2637, Val Loss: 264.9721\n",
            "Epoch 5593, Train Loss: 605.3626, Val Loss: 302.1615\n",
            "Epoch 5594, Train Loss: 624.7265, Val Loss: 266.1917\n",
            "Epoch 5595, Train Loss: 619.4662, Val Loss: 204.9478\n",
            "Epoch 5596, Train Loss: 610.9591, Val Loss: 183.1235\n",
            "Epoch 5597, Train Loss: 612.0628, Val Loss: 186.4686\n",
            "Epoch 5598, Train Loss: 615.4496, Val Loss: 180.1490\n",
            "Epoch 5599, Train Loss: 548.3346, Val Loss: 179.4527\n",
            "Epoch 5600, Train Loss: 607.1653, Val Loss: 218.2574\n",
            "Epoch 5601, Train Loss: 607.7773, Val Loss: 231.7547\n",
            "Epoch 5602, Train Loss: 629.2632, Val Loss: 185.7826\n",
            "Epoch 5603, Train Loss: 611.7974, Val Loss: 179.6273\n",
            "Epoch 5604, Train Loss: 622.8805, Val Loss: 179.7734\n",
            "Epoch 5605, Train Loss: 565.2964, Val Loss: 221.4559\n",
            "Epoch 5606, Train Loss: 612.8547, Val Loss: 264.0643\n",
            "Epoch 5607, Train Loss: 622.6094, Val Loss: 221.9863\n",
            "Epoch 5608, Train Loss: 618.1280, Val Loss: 172.9756\n",
            "Epoch 5609, Train Loss: 615.8121, Val Loss: 179.3640\n",
            "Epoch 5610, Train Loss: 614.3477, Val Loss: 243.5529\n",
            "Epoch 5611, Train Loss: 607.8654, Val Loss: 321.4910\n",
            "Epoch 5612, Train Loss: 607.8495, Val Loss: 401.9551\n",
            "Epoch 5613, Train Loss: 616.6702, Val Loss: 349.3592\n",
            "Epoch 5614, Train Loss: 615.3149, Val Loss: 328.3302\n",
            "Epoch 5615, Train Loss: 617.9839, Val Loss: 201.7459\n",
            "Epoch 5616, Train Loss: 578.3630, Val Loss: 205.6258\n",
            "Epoch 5617, Train Loss: 619.1328, Val Loss: 240.0424\n",
            "Epoch 5618, Train Loss: 612.2087, Val Loss: 173.2064\n",
            "Epoch 5619, Train Loss: 627.6574, Val Loss: 173.0701\n",
            "Epoch 5620, Train Loss: 618.2092, Val Loss: 167.3721\n",
            "Epoch 5621, Train Loss: 614.3850, Val Loss: 188.2216\n",
            "Epoch 5622, Train Loss: 625.4095, Val Loss: 276.8774\n",
            "Epoch 5623, Train Loss: 594.9614, Val Loss: 199.3472\n",
            "Epoch 5624, Train Loss: 623.7410, Val Loss: 304.4022\n",
            "Epoch 5625, Train Loss: 557.7590, Val Loss: 389.0657\n",
            "Epoch 5626, Train Loss: 608.7946, Val Loss: 258.2567\n",
            "Epoch 5627, Train Loss: 616.7406, Val Loss: 248.5275\n",
            "Epoch 5628, Train Loss: 601.6552, Val Loss: 180.4546\n",
            "Epoch 5629, Train Loss: 599.2789, Val Loss: 229.7237\n",
            "Epoch 5630, Train Loss: 571.7077, Val Loss: 323.2372\n",
            "Epoch 5631, Train Loss: 628.3707, Val Loss: 245.8058\n",
            "Epoch 5632, Train Loss: 590.2185, Val Loss: 185.9167\n",
            "Epoch 5633, Train Loss: 632.5768, Val Loss: 173.5398\n",
            "Epoch 5634, Train Loss: 613.9523, Val Loss: 173.3035\n",
            "Epoch 5635, Train Loss: 581.0864, Val Loss: 180.3852\n",
            "Epoch 5636, Train Loss: 634.0420, Val Loss: 179.1613\n",
            "Epoch 5637, Train Loss: 608.9481, Val Loss: 182.5202\n",
            "Epoch 5638, Train Loss: 614.3444, Val Loss: 187.5955\n",
            "Epoch 5639, Train Loss: 589.6827, Val Loss: 269.3080\n",
            "Epoch 5640, Train Loss: 624.3776, Val Loss: 414.5368\n",
            "Epoch 5641, Train Loss: 600.8042, Val Loss: 521.0907\n",
            "Epoch 5642, Train Loss: 595.9090, Val Loss: 335.2120\n",
            "Epoch 5643, Train Loss: 610.9944, Val Loss: 183.0919\n",
            "Epoch 5644, Train Loss: 624.7756, Val Loss: 184.3186\n",
            "Epoch 5645, Train Loss: 606.6413, Val Loss: 220.8558\n",
            "Epoch 5646, Train Loss: 611.6489, Val Loss: 260.6097\n",
            "Epoch 5647, Train Loss: 593.7801, Val Loss: 193.1527\n",
            "Epoch 5648, Train Loss: 625.1939, Val Loss: 197.0061\n",
            "Epoch 5649, Train Loss: 620.5805, Val Loss: 236.9141\n",
            "Epoch 5650, Train Loss: 633.2528, Val Loss: 338.2172\n",
            "Epoch 5651, Train Loss: 614.6622, Val Loss: 349.2142\n",
            "Epoch 5652, Train Loss: 612.5894, Val Loss: 259.5723\n",
            "Epoch 5653, Train Loss: 615.6409, Val Loss: 207.8022\n",
            "Epoch 5654, Train Loss: 621.0024, Val Loss: 248.1732\n",
            "Epoch 5655, Train Loss: 627.8506, Val Loss: 189.5615\n",
            "Epoch 5656, Train Loss: 625.7721, Val Loss: 245.4812\n",
            "Epoch 5657, Train Loss: 629.9855, Val Loss: 323.1718\n",
            "Epoch 5658, Train Loss: 597.5636, Val Loss: 267.7261\n",
            "Epoch 5659, Train Loss: 602.9646, Val Loss: 269.8099\n",
            "Epoch 5660, Train Loss: 598.2823, Val Loss: 236.7816\n",
            "Epoch 5661, Train Loss: 532.5663, Val Loss: 222.7036\n",
            "Epoch 5662, Train Loss: 627.8032, Val Loss: 245.6251\n",
            "Epoch 5663, Train Loss: 612.9209, Val Loss: 231.0807\n",
            "Epoch 5664, Train Loss: 613.1605, Val Loss: 199.7419\n",
            "Epoch 5665, Train Loss: 621.8511, Val Loss: 189.1424\n",
            "Epoch 5666, Train Loss: 625.5449, Val Loss: 218.7000\n",
            "Epoch 5667, Train Loss: 600.4866, Val Loss: 292.6191\n",
            "Epoch 5668, Train Loss: 616.8333, Val Loss: 381.4644\n",
            "Epoch 5669, Train Loss: 633.2016, Val Loss: 242.7277\n",
            "Epoch 5670, Train Loss: 622.2914, Val Loss: 182.7025\n",
            "Epoch 5671, Train Loss: 623.3151, Val Loss: 172.6571\n",
            "Epoch 5672, Train Loss: 621.3843, Val Loss: 170.1669\n",
            "Epoch 5673, Train Loss: 604.7390, Val Loss: 179.3733\n",
            "Epoch 5674, Train Loss: 618.9994, Val Loss: 193.8180\n",
            "Epoch 5675, Train Loss: 617.1905, Val Loss: 192.8237\n",
            "Epoch 5676, Train Loss: 554.5635, Val Loss: 220.2331\n",
            "Epoch 5677, Train Loss: 588.3568, Val Loss: 273.5947\n",
            "Epoch 5678, Train Loss: 619.6441, Val Loss: 293.3697\n",
            "Epoch 5679, Train Loss: 628.7826, Val Loss: 216.2937\n",
            "Epoch 5680, Train Loss: 623.7128, Val Loss: 173.1785\n",
            "Epoch 5681, Train Loss: 595.9522, Val Loss: 172.1166\n",
            "Epoch 5682, Train Loss: 626.8815, Val Loss: 182.7479\n",
            "Epoch 5683, Train Loss: 614.7228, Val Loss: 210.5492\n",
            "Epoch 5684, Train Loss: 580.0090, Val Loss: 208.8899\n",
            "Epoch 5685, Train Loss: 615.7327, Val Loss: 197.6510\n",
            "Epoch 5686, Train Loss: 616.7330, Val Loss: 195.6951\n",
            "Epoch 5687, Train Loss: 615.8350, Val Loss: 194.7238\n",
            "Epoch 5688, Train Loss: 624.5520, Val Loss: 185.6853\n",
            "Epoch 5689, Train Loss: 626.7098, Val Loss: 197.9943\n",
            "Epoch 5690, Train Loss: 604.7102, Val Loss: 184.4414\n",
            "Epoch 5691, Train Loss: 588.5338, Val Loss: 198.2906\n",
            "Epoch 5692, Train Loss: 614.7352, Val Loss: 195.2386\n",
            "Epoch 5693, Train Loss: 625.4038, Val Loss: 205.8498\n",
            "Epoch 5694, Train Loss: 599.6172, Val Loss: 195.8604\n",
            "Epoch 5695, Train Loss: 618.1817, Val Loss: 187.7865\n",
            "Epoch 5696, Train Loss: 623.8483, Val Loss: 183.6131\n",
            "Epoch 5697, Train Loss: 608.9961, Val Loss: 177.4315\n",
            "Epoch 5698, Train Loss: 617.9906, Val Loss: 181.5133\n",
            "Epoch 5699, Train Loss: 622.6059, Val Loss: 193.1961\n",
            "Epoch 5700, Train Loss: 581.1710, Val Loss: 272.4075\n",
            "Epoch 5701, Train Loss: 592.9080, Val Loss: 440.1040\n",
            "Epoch 5702, Train Loss: 614.5699, Val Loss: 392.6317\n",
            "Epoch 5703, Train Loss: 594.5137, Val Loss: 240.7835\n",
            "Epoch 5704, Train Loss: 592.8388, Val Loss: 215.5706\n",
            "Epoch 5705, Train Loss: 618.2681, Val Loss: 201.1852\n",
            "Epoch 5706, Train Loss: 616.1227, Val Loss: 216.9425\n",
            "Epoch 5707, Train Loss: 616.9599, Val Loss: 199.9265\n",
            "Epoch 5708, Train Loss: 600.2603, Val Loss: 190.0984\n",
            "Epoch 5709, Train Loss: 620.1229, Val Loss: 196.6861\n",
            "Epoch 5710, Train Loss: 581.6932, Val Loss: 179.0326\n",
            "Epoch 5711, Train Loss: 592.4219, Val Loss: 182.3293\n",
            "Epoch 5712, Train Loss: 624.0592, Val Loss: 175.4419\n",
            "Epoch 5713, Train Loss: 598.2916, Val Loss: 176.2777\n",
            "Epoch 5714, Train Loss: 620.1711, Val Loss: 180.4272\n",
            "Epoch 5715, Train Loss: 625.6465, Val Loss: 204.3420\n",
            "Epoch 5716, Train Loss: 618.1417, Val Loss: 252.2886\n",
            "Epoch 5717, Train Loss: 626.2357, Val Loss: 215.8693\n",
            "Epoch 5718, Train Loss: 612.4327, Val Loss: 185.7000\n",
            "Epoch 5719, Train Loss: 621.7537, Val Loss: 178.2414\n",
            "Epoch 5720, Train Loss: 613.6926, Val Loss: 178.0064\n",
            "Epoch 5721, Train Loss: 625.0257, Val Loss: 196.6758\n",
            "Epoch 5722, Train Loss: 617.9036, Val Loss: 209.8809\n",
            "Epoch 5723, Train Loss: 613.2816, Val Loss: 212.3425\n",
            "Epoch 5724, Train Loss: 622.9973, Val Loss: 203.7174\n",
            "Epoch 5725, Train Loss: 603.4705, Val Loss: 203.6442\n",
            "Epoch 5726, Train Loss: 603.7345, Val Loss: 194.6670\n",
            "Epoch 5727, Train Loss: 591.6848, Val Loss: 212.9913\n",
            "Epoch 5728, Train Loss: 618.7160, Val Loss: 261.6474\n",
            "Epoch 5729, Train Loss: 622.6689, Val Loss: 261.9220\n",
            "Epoch 5730, Train Loss: 598.5995, Val Loss: 240.5328\n",
            "Epoch 5731, Train Loss: 614.6112, Val Loss: 211.9897\n",
            "Epoch 5732, Train Loss: 605.8960, Val Loss: 204.4493\n",
            "Epoch 5733, Train Loss: 606.1699, Val Loss: 280.2200\n",
            "Epoch 5734, Train Loss: 584.2170, Val Loss: 300.7651\n",
            "Epoch 5735, Train Loss: 610.4886, Val Loss: 226.1924\n",
            "Epoch 5736, Train Loss: 610.5786, Val Loss: 189.0333\n",
            "Epoch 5737, Train Loss: 594.1888, Val Loss: 187.3821\n",
            "Epoch 5738, Train Loss: 625.5097, Val Loss: 196.2643\n",
            "Epoch 5739, Train Loss: 621.8738, Val Loss: 216.6737\n",
            "Epoch 5740, Train Loss: 607.1632, Val Loss: 213.6009\n",
            "Epoch 5741, Train Loss: 630.9242, Val Loss: 202.7779\n",
            "Epoch 5742, Train Loss: 619.1044, Val Loss: 205.9324\n",
            "Epoch 5743, Train Loss: 584.5872, Val Loss: 194.0207\n",
            "Epoch 5744, Train Loss: 595.8621, Val Loss: 192.4231\n",
            "Epoch 5745, Train Loss: 626.6671, Val Loss: 213.4351\n",
            "Epoch 5746, Train Loss: 586.4606, Val Loss: 219.9236\n",
            "Epoch 5747, Train Loss: 613.2889, Val Loss: 190.6840\n",
            "Epoch 5748, Train Loss: 623.8433, Val Loss: 179.6336\n",
            "Epoch 5749, Train Loss: 596.7698, Val Loss: 197.6103\n",
            "Epoch 5750, Train Loss: 630.3781, Val Loss: 200.2695\n",
            "Epoch 5751, Train Loss: 612.5740, Val Loss: 187.1248\n",
            "Epoch 5752, Train Loss: 626.2025, Val Loss: 173.7705\n",
            "Epoch 5753, Train Loss: 610.8613, Val Loss: 190.7849\n",
            "Epoch 5754, Train Loss: 622.5435, Val Loss: 249.2747\n",
            "Epoch 5755, Train Loss: 624.2557, Val Loss: 260.2837\n",
            "Epoch 5756, Train Loss: 600.6972, Val Loss: 212.4640\n",
            "Epoch 5757, Train Loss: 622.5375, Val Loss: 186.6854\n",
            "Epoch 5758, Train Loss: 621.2928, Val Loss: 175.7861\n",
            "Epoch 5759, Train Loss: 613.1136, Val Loss: 179.0283\n",
            "Epoch 5760, Train Loss: 605.9154, Val Loss: 194.0583\n",
            "Epoch 5761, Train Loss: 622.3940, Val Loss: 205.0424\n",
            "Epoch 5762, Train Loss: 629.7437, Val Loss: 219.4287\n",
            "Epoch 5763, Train Loss: 615.7786, Val Loss: 236.1235\n",
            "Epoch 5764, Train Loss: 625.4243, Val Loss: 216.4899\n",
            "Epoch 5765, Train Loss: 607.1218, Val Loss: 223.8940\n",
            "Epoch 5766, Train Loss: 613.0627, Val Loss: 214.5982\n",
            "Epoch 5767, Train Loss: 603.6826, Val Loss: 230.5133\n",
            "Epoch 5768, Train Loss: 594.4759, Val Loss: 218.2234\n",
            "Epoch 5769, Train Loss: 614.9634, Val Loss: 205.2578\n",
            "Epoch 5770, Train Loss: 564.1862, Val Loss: 237.5861\n",
            "Epoch 5771, Train Loss: 615.7301, Val Loss: 324.9948\n",
            "Epoch 5772, Train Loss: 612.8190, Val Loss: 268.4969\n",
            "Epoch 5773, Train Loss: 594.0164, Val Loss: 191.1276\n",
            "Epoch 5774, Train Loss: 624.3239, Val Loss: 195.9571\n",
            "Epoch 5775, Train Loss: 625.9530, Val Loss: 244.2388\n",
            "Epoch 5776, Train Loss: 631.9489, Val Loss: 225.8908\n",
            "Epoch 5777, Train Loss: 597.8662, Val Loss: 196.1599\n",
            "Epoch 5778, Train Loss: 600.3873, Val Loss: 171.5320\n",
            "Epoch 5779, Train Loss: 581.6100, Val Loss: 173.7368\n",
            "Epoch 5780, Train Loss: 621.2319, Val Loss: 193.1093\n",
            "Epoch 5781, Train Loss: 621.0427, Val Loss: 210.2901\n",
            "Epoch 5782, Train Loss: 623.6529, Val Loss: 223.8461\n",
            "Epoch 5783, Train Loss: 588.8502, Val Loss: 247.7943\n",
            "Epoch 5784, Train Loss: 631.2309, Val Loss: 185.6064\n",
            "Epoch 5785, Train Loss: 616.9382, Val Loss: 171.5634\n",
            "Epoch 5786, Train Loss: 615.2927, Val Loss: 170.7105\n",
            "Epoch 5787, Train Loss: 627.8051, Val Loss: 174.7074\n",
            "Epoch 5788, Train Loss: 600.9842, Val Loss: 183.8212\n",
            "Epoch 5789, Train Loss: 615.9482, Val Loss: 189.5513\n",
            "Epoch 5790, Train Loss: 612.9048, Val Loss: 182.6577\n",
            "Epoch 5791, Train Loss: 606.2355, Val Loss: 175.6758\n",
            "Epoch 5792, Train Loss: 582.5200, Val Loss: 177.8173\n",
            "Epoch 5793, Train Loss: 622.1439, Val Loss: 183.1758\n",
            "Epoch 5794, Train Loss: 573.4353, Val Loss: 191.6454\n",
            "Epoch 5795, Train Loss: 624.4461, Val Loss: 185.2252\n",
            "Epoch 5796, Train Loss: 619.3740, Val Loss: 188.4409\n",
            "Epoch 5797, Train Loss: 607.5945, Val Loss: 212.9737\n",
            "Epoch 5798, Train Loss: 620.3972, Val Loss: 222.5556\n",
            "Epoch 5799, Train Loss: 619.3836, Val Loss: 313.9188\n",
            "Epoch 5800, Train Loss: 575.4329, Val Loss: 280.2894\n",
            "Epoch 5801, Train Loss: 621.3599, Val Loss: 173.6613\n",
            "Epoch 5802, Train Loss: 632.1052, Val Loss: 176.4162\n",
            "Epoch 5803, Train Loss: 605.1133, Val Loss: 216.3633\n",
            "Epoch 5804, Train Loss: 615.2906, Val Loss: 240.9853\n",
            "Epoch 5805, Train Loss: 626.4767, Val Loss: 211.6798\n",
            "Epoch 5806, Train Loss: 606.6129, Val Loss: 197.4907\n",
            "Epoch 5807, Train Loss: 626.4674, Val Loss: 181.3440\n",
            "Epoch 5808, Train Loss: 606.5596, Val Loss: 177.5681\n",
            "Epoch 5809, Train Loss: 611.7394, Val Loss: 193.6536\n",
            "Epoch 5810, Train Loss: 616.6524, Val Loss: 205.8355\n",
            "Epoch 5811, Train Loss: 617.6905, Val Loss: 201.6311\n",
            "Epoch 5812, Train Loss: 629.6872, Val Loss: 188.7292\n",
            "Epoch 5813, Train Loss: 599.3658, Val Loss: 194.3380\n",
            "Epoch 5814, Train Loss: 624.4704, Val Loss: 246.3878\n",
            "Epoch 5815, Train Loss: 603.1823, Val Loss: 250.8459\n",
            "Epoch 5816, Train Loss: 626.9061, Val Loss: 245.4536\n",
            "Epoch 5817, Train Loss: 618.4944, Val Loss: 197.5228\n",
            "Epoch 5818, Train Loss: 613.8701, Val Loss: 179.2179\n",
            "Epoch 5819, Train Loss: 610.5339, Val Loss: 193.5530\n",
            "Epoch 5820, Train Loss: 599.7203, Val Loss: 191.1011\n",
            "Epoch 5821, Train Loss: 603.9384, Val Loss: 187.9137\n",
            "Epoch 5822, Train Loss: 615.4130, Val Loss: 191.4033\n",
            "Epoch 5823, Train Loss: 577.3255, Val Loss: 261.9610\n",
            "Epoch 5824, Train Loss: 624.2993, Val Loss: 301.6329\n",
            "Epoch 5825, Train Loss: 613.9684, Val Loss: 184.9635\n",
            "Epoch 5826, Train Loss: 626.2425, Val Loss: 171.2760\n",
            "Epoch 5827, Train Loss: 585.3455, Val Loss: 178.4544\n",
            "Epoch 5828, Train Loss: 607.5853, Val Loss: 218.4074\n",
            "Epoch 5829, Train Loss: 626.4215, Val Loss: 182.9544\n",
            "Epoch 5830, Train Loss: 622.2923, Val Loss: 174.2605\n",
            "Epoch 5831, Train Loss: 628.1113, Val Loss: 174.1855\n",
            "Epoch 5832, Train Loss: 628.6037, Val Loss: 166.0672\n",
            "Epoch 5833, Train Loss: 605.6088, Val Loss: 190.4411\n",
            "Epoch 5834, Train Loss: 553.1930, Val Loss: 575.2215\n",
            "Epoch 5835, Train Loss: 617.9952, Val Loss: 392.9024\n",
            "Epoch 5836, Train Loss: 624.1423, Val Loss: 203.1325\n",
            "Epoch 5837, Train Loss: 625.6096, Val Loss: 292.1217\n",
            "Epoch 5838, Train Loss: 621.6160, Val Loss: 173.8367\n",
            "Epoch 5839, Train Loss: 611.8731, Val Loss: 530.4257\n",
            "Epoch 5840, Train Loss: 601.7615, Val Loss: 740.1564\n",
            "Epoch 5841, Train Loss: 628.7011, Val Loss: 552.8873\n",
            "Epoch 5842, Train Loss: 608.6755, Val Loss: 220.4740\n",
            "Epoch 5843, Train Loss: 590.3808, Val Loss: 186.0070\n",
            "Epoch 5844, Train Loss: 562.5189, Val Loss: 180.8196\n",
            "Epoch 5845, Train Loss: 624.2336, Val Loss: 190.1427\n",
            "Epoch 5846, Train Loss: 620.4628, Val Loss: 186.7734\n",
            "Epoch 5847, Train Loss: 589.9804, Val Loss: 180.5239\n",
            "Epoch 5848, Train Loss: 626.7729, Val Loss: 186.7496\n",
            "Epoch 5849, Train Loss: 626.8115, Val Loss: 190.7955\n",
            "Epoch 5850, Train Loss: 623.0726, Val Loss: 202.3899\n",
            "Epoch 5851, Train Loss: 609.7250, Val Loss: 193.8549\n",
            "Epoch 5852, Train Loss: 619.9130, Val Loss: 179.6353\n",
            "Epoch 5853, Train Loss: 609.8065, Val Loss: 176.6727\n",
            "Epoch 5854, Train Loss: 606.8155, Val Loss: 179.9468\n",
            "Epoch 5855, Train Loss: 597.5671, Val Loss: 181.7767\n",
            "Epoch 5856, Train Loss: 625.8548, Val Loss: 182.2115\n",
            "Epoch 5857, Train Loss: 621.7678, Val Loss: 198.9437\n",
            "Epoch 5858, Train Loss: 624.2909, Val Loss: 206.4960\n",
            "Epoch 5859, Train Loss: 586.9485, Val Loss: 201.8791\n",
            "Epoch 5860, Train Loss: 622.8092, Val Loss: 203.0965\n",
            "Epoch 5861, Train Loss: 617.6319, Val Loss: 208.4721\n",
            "Epoch 5862, Train Loss: 611.9270, Val Loss: 191.2238\n",
            "Epoch 5863, Train Loss: 611.3324, Val Loss: 190.3864\n",
            "Epoch 5864, Train Loss: 610.0953, Val Loss: 193.7696\n",
            "Epoch 5865, Train Loss: 569.4565, Val Loss: 213.9607\n",
            "Epoch 5866, Train Loss: 602.6251, Val Loss: 234.6791\n",
            "Epoch 5867, Train Loss: 616.0972, Val Loss: 252.3839\n",
            "Epoch 5868, Train Loss: 626.5196, Val Loss: 251.4162\n",
            "Epoch 5869, Train Loss: 624.8963, Val Loss: 267.1716\n",
            "Epoch 5870, Train Loss: 607.8668, Val Loss: 232.2165\n",
            "Epoch 5871, Train Loss: 592.5149, Val Loss: 191.1873\n",
            "Epoch 5872, Train Loss: 624.0820, Val Loss: 173.2086\n",
            "Epoch 5873, Train Loss: 625.0624, Val Loss: 173.1583\n",
            "Epoch 5874, Train Loss: 617.4555, Val Loss: 185.3590\n",
            "Epoch 5875, Train Loss: 612.6916, Val Loss: 192.4782\n",
            "Epoch 5876, Train Loss: 603.8987, Val Loss: 206.6823\n",
            "Epoch 5877, Train Loss: 622.5626, Val Loss: 215.8520\n",
            "Epoch 5878, Train Loss: 617.1179, Val Loss: 249.4315\n",
            "Epoch 5879, Train Loss: 624.6020, Val Loss: 282.7904\n",
            "Epoch 5880, Train Loss: 615.0642, Val Loss: 299.7178\n",
            "Epoch 5881, Train Loss: 623.6847, Val Loss: 267.6630\n",
            "Epoch 5882, Train Loss: 616.1490, Val Loss: 191.4225\n",
            "Epoch 5883, Train Loss: 579.5378, Val Loss: 175.1222\n",
            "Epoch 5884, Train Loss: 618.9687, Val Loss: 173.6286\n",
            "Epoch 5885, Train Loss: 610.1643, Val Loss: 170.5467\n",
            "Epoch 5886, Train Loss: 618.7413, Val Loss: 187.8875\n",
            "Epoch 5887, Train Loss: 605.9989, Val Loss: 237.8138\n",
            "Epoch 5888, Train Loss: 603.0600, Val Loss: 255.2126\n",
            "Epoch 5889, Train Loss: 594.4984, Val Loss: 212.8711\n",
            "Epoch 5890, Train Loss: 624.1147, Val Loss: 184.1247\n",
            "Epoch 5891, Train Loss: 608.4013, Val Loss: 178.2463\n",
            "Epoch 5892, Train Loss: 573.6281, Val Loss: 183.4467\n",
            "Epoch 5893, Train Loss: 613.9365, Val Loss: 196.4115\n",
            "Epoch 5894, Train Loss: 596.7916, Val Loss: 220.9649\n",
            "Epoch 5895, Train Loss: 622.3805, Val Loss: 258.5059\n",
            "Epoch 5896, Train Loss: 621.5030, Val Loss: 239.6132\n",
            "Epoch 5897, Train Loss: 636.2628, Val Loss: 246.9731\n",
            "Epoch 5898, Train Loss: 629.9903, Val Loss: 337.2659\n",
            "Epoch 5899, Train Loss: 602.4460, Val Loss: 340.3114\n",
            "Epoch 5900, Train Loss: 626.5438, Val Loss: 228.8659\n",
            "Epoch 5901, Train Loss: 606.3926, Val Loss: 183.4577\n",
            "Epoch 5902, Train Loss: 620.9762, Val Loss: 177.6885\n",
            "Epoch 5903, Train Loss: 617.4010, Val Loss: 196.4043\n",
            "Epoch 5904, Train Loss: 627.4179, Val Loss: 236.0563\n",
            "Epoch 5905, Train Loss: 623.2689, Val Loss: 233.2369\n",
            "Epoch 5906, Train Loss: 627.5167, Val Loss: 265.6564\n",
            "Epoch 5907, Train Loss: 625.6248, Val Loss: 320.4858\n",
            "Epoch 5908, Train Loss: 632.7791, Val Loss: 293.8314\n",
            "Epoch 5909, Train Loss: 590.4963, Val Loss: 206.4944\n",
            "Epoch 5910, Train Loss: 595.3012, Val Loss: 180.8010\n",
            "Epoch 5911, Train Loss: 615.9371, Val Loss: 181.6267\n",
            "Epoch 5912, Train Loss: 631.5723, Val Loss: 177.4518\n",
            "Epoch 5913, Train Loss: 602.9242, Val Loss: 173.9082\n",
            "Epoch 5914, Train Loss: 620.4630, Val Loss: 171.1134\n",
            "Epoch 5915, Train Loss: 627.2684, Val Loss: 199.9051\n",
            "Epoch 5916, Train Loss: 624.8446, Val Loss: 226.5645\n",
            "Epoch 5917, Train Loss: 607.4524, Val Loss: 223.3679\n",
            "Epoch 5918, Train Loss: 617.0347, Val Loss: 192.6611\n",
            "Epoch 5919, Train Loss: 595.0363, Val Loss: 186.7634\n",
            "Epoch 5920, Train Loss: 620.0550, Val Loss: 191.1932\n",
            "Epoch 5921, Train Loss: 584.6152, Val Loss: 213.9972\n",
            "Epoch 5922, Train Loss: 593.0664, Val Loss: 227.5342\n",
            "Epoch 5923, Train Loss: 621.0981, Val Loss: 232.7568\n",
            "Epoch 5924, Train Loss: 611.2166, Val Loss: 224.2957\n",
            "Epoch 5925, Train Loss: 630.5134, Val Loss: 196.0484\n",
            "Epoch 5926, Train Loss: 618.6948, Val Loss: 190.2817\n",
            "Epoch 5927, Train Loss: 607.4909, Val Loss: 191.7593\n",
            "Epoch 5928, Train Loss: 609.0641, Val Loss: 199.9476\n",
            "Epoch 5929, Train Loss: 627.2889, Val Loss: 186.0070\n",
            "Epoch 5930, Train Loss: 589.5494, Val Loss: 170.0601\n",
            "Epoch 5931, Train Loss: 621.7308, Val Loss: 171.1251\n",
            "Epoch 5932, Train Loss: 620.2380, Val Loss: 172.8795\n",
            "Epoch 5933, Train Loss: 622.6274, Val Loss: 174.3656\n",
            "Epoch 5934, Train Loss: 610.2392, Val Loss: 173.3747\n",
            "Epoch 5935, Train Loss: 619.0633, Val Loss: 172.5208\n",
            "Epoch 5936, Train Loss: 594.4549, Val Loss: 172.1935\n",
            "Epoch 5937, Train Loss: 599.8195, Val Loss: 188.0099\n",
            "Epoch 5938, Train Loss: 624.2414, Val Loss: 216.6633\n",
            "Epoch 5939, Train Loss: 618.0567, Val Loss: 223.8597\n",
            "Epoch 5940, Train Loss: 586.9548, Val Loss: 223.2646\n",
            "Epoch 5941, Train Loss: 616.4428, Val Loss: 218.0000\n",
            "Epoch 5942, Train Loss: 623.3493, Val Loss: 235.9113\n",
            "Epoch 5943, Train Loss: 626.6223, Val Loss: 210.1706\n",
            "Epoch 5944, Train Loss: 615.6231, Val Loss: 209.7393\n",
            "Epoch 5945, Train Loss: 636.5349, Val Loss: 215.9820\n",
            "Epoch 5946, Train Loss: 578.5508, Val Loss: 196.8640\n",
            "Epoch 5947, Train Loss: 623.1124, Val Loss: 178.6162\n",
            "Epoch 5948, Train Loss: 628.3421, Val Loss: 174.9635\n",
            "Epoch 5949, Train Loss: 617.6777, Val Loss: 174.9603\n",
            "Epoch 5950, Train Loss: 596.9542, Val Loss: 214.8734\n",
            "Epoch 5951, Train Loss: 575.7525, Val Loss: 215.8521\n",
            "Epoch 5952, Train Loss: 593.5252, Val Loss: 192.6722\n",
            "Epoch 5953, Train Loss: 610.7090, Val Loss: 180.3326\n",
            "Epoch 5954, Train Loss: 606.6699, Val Loss: 181.2083\n",
            "Epoch 5955, Train Loss: 569.5356, Val Loss: 193.8074\n",
            "Epoch 5956, Train Loss: 584.1393, Val Loss: 224.4018\n",
            "Epoch 5957, Train Loss: 619.3759, Val Loss: 210.9638\n",
            "Epoch 5958, Train Loss: 624.7337, Val Loss: 187.8247\n",
            "Epoch 5959, Train Loss: 561.9986, Val Loss: 203.4976\n",
            "Epoch 5960, Train Loss: 617.0601, Val Loss: 226.8392\n",
            "Epoch 5961, Train Loss: 626.0063, Val Loss: 237.7389\n",
            "Epoch 5962, Train Loss: 604.5534, Val Loss: 219.3771\n",
            "Epoch 5963, Train Loss: 618.9882, Val Loss: 242.2023\n",
            "Epoch 5964, Train Loss: 597.7372, Val Loss: 266.7103\n",
            "Epoch 5965, Train Loss: 620.8866, Val Loss: 302.3512\n",
            "Epoch 5966, Train Loss: 583.1803, Val Loss: 304.2229\n",
            "Epoch 5967, Train Loss: 588.1397, Val Loss: 216.1629\n",
            "Epoch 5968, Train Loss: 628.2465, Val Loss: 189.6706\n",
            "Epoch 5969, Train Loss: 604.3129, Val Loss: 248.3626\n",
            "Epoch 5970, Train Loss: 586.1015, Val Loss: 381.2700\n",
            "Epoch 5971, Train Loss: 631.5766, Val Loss: 323.1511\n",
            "Epoch 5972, Train Loss: 607.7396, Val Loss: 182.7088\n",
            "Epoch 5973, Train Loss: 600.9100, Val Loss: 176.5738\n",
            "Epoch 5974, Train Loss: 620.6525, Val Loss: 172.9448\n",
            "Epoch 5975, Train Loss: 586.6118, Val Loss: 187.7012\n",
            "Epoch 5976, Train Loss: 619.3595, Val Loss: 222.5713\n",
            "Epoch 5977, Train Loss: 560.1931, Val Loss: 268.1164\n",
            "Epoch 5978, Train Loss: 594.9825, Val Loss: 265.7000\n",
            "Epoch 5979, Train Loss: 595.0026, Val Loss: 238.5066\n",
            "Epoch 5980, Train Loss: 625.5340, Val Loss: 208.7262\n",
            "Epoch 5981, Train Loss: 621.1600, Val Loss: 194.5921\n",
            "Epoch 5982, Train Loss: 599.8484, Val Loss: 194.7105\n",
            "Epoch 5983, Train Loss: 629.7420, Val Loss: 309.0745\n",
            "Epoch 5984, Train Loss: 598.7695, Val Loss: 356.3988\n",
            "Epoch 5985, Train Loss: 624.1736, Val Loss: 241.9936\n",
            "Epoch 5986, Train Loss: 614.9885, Val Loss: 197.1892\n",
            "Epoch 5987, Train Loss: 591.7487, Val Loss: 187.9085\n",
            "Epoch 5988, Train Loss: 606.7817, Val Loss: 184.4092\n",
            "Epoch 5989, Train Loss: 626.0553, Val Loss: 212.8947\n",
            "Epoch 5990, Train Loss: 592.8340, Val Loss: 216.6202\n",
            "Epoch 5991, Train Loss: 616.6194, Val Loss: 193.4630\n",
            "Epoch 5992, Train Loss: 625.5155, Val Loss: 189.9382\n",
            "Epoch 5993, Train Loss: 615.4974, Val Loss: 195.9283\n",
            "Epoch 5994, Train Loss: 633.7112, Val Loss: 224.7868\n",
            "Epoch 5995, Train Loss: 625.2993, Val Loss: 347.7114\n",
            "Epoch 5996, Train Loss: 611.9811, Val Loss: 382.5970\n",
            "Epoch 5997, Train Loss: 610.1303, Val Loss: 263.3711\n",
            "Epoch 5998, Train Loss: 594.8068, Val Loss: 191.6171\n",
            "Epoch 5999, Train Loss: 614.5085, Val Loss: 169.9839\n",
            "Epoch 6000, Train Loss: 626.2999, Val Loss: 169.9791\n",
            "Epoch 6001, Train Loss: 611.7317, Val Loss: 190.0491\n",
            "Epoch 6002, Train Loss: 621.5409, Val Loss: 223.0019\n",
            "Epoch 6003, Train Loss: 605.1107, Val Loss: 191.8976\n",
            "Epoch 6004, Train Loss: 581.6367, Val Loss: 198.0615\n",
            "Epoch 6005, Train Loss: 591.6023, Val Loss: 234.3128\n",
            "Epoch 6006, Train Loss: 611.4275, Val Loss: 274.4313\n",
            "Epoch 6007, Train Loss: 623.7693, Val Loss: 384.1556\n",
            "Epoch 6008, Train Loss: 623.5835, Val Loss: 336.3623\n",
            "Epoch 6009, Train Loss: 581.1500, Val Loss: 322.6701\n",
            "Epoch 6010, Train Loss: 636.6958, Val Loss: 266.0179\n",
            "Epoch 6011, Train Loss: 608.9956, Val Loss: 254.9851\n",
            "Epoch 6012, Train Loss: 571.0902, Val Loss: 177.9744\n",
            "Epoch 6013, Train Loss: 619.8032, Val Loss: 177.2200\n",
            "Epoch 6014, Train Loss: 566.5113, Val Loss: 185.6175\n",
            "Epoch 6015, Train Loss: 611.9654, Val Loss: 186.8856\n",
            "Epoch 6016, Train Loss: 629.2096, Val Loss: 190.8466\n",
            "Epoch 6017, Train Loss: 620.7684, Val Loss: 184.3088\n",
            "Epoch 6018, Train Loss: 608.7998, Val Loss: 178.3473\n",
            "Epoch 6019, Train Loss: 584.7489, Val Loss: 173.6478\n",
            "Epoch 6020, Train Loss: 609.0597, Val Loss: 175.3924\n",
            "Epoch 6021, Train Loss: 604.5141, Val Loss: 178.6901\n",
            "Epoch 6022, Train Loss: 612.9593, Val Loss: 175.7910\n",
            "Epoch 6023, Train Loss: 626.3367, Val Loss: 171.8123\n",
            "Epoch 6024, Train Loss: 599.9881, Val Loss: 171.3323\n",
            "Epoch 6025, Train Loss: 588.5384, Val Loss: 174.8907\n",
            "Epoch 6026, Train Loss: 627.6200, Val Loss: 180.9748\n",
            "Epoch 6027, Train Loss: 605.6578, Val Loss: 179.8587\n",
            "Epoch 6028, Train Loss: 612.4469, Val Loss: 171.9436\n",
            "Epoch 6029, Train Loss: 623.0610, Val Loss: 176.0579\n",
            "Epoch 6030, Train Loss: 581.5868, Val Loss: 202.9636\n",
            "Epoch 6031, Train Loss: 606.5118, Val Loss: 325.3711\n",
            "Epoch 6032, Train Loss: 626.7808, Val Loss: 361.0026\n",
            "Epoch 6033, Train Loss: 641.4695, Val Loss: 194.6200\n",
            "Epoch 6034, Train Loss: 613.3188, Val Loss: 187.9785\n",
            "Epoch 6035, Train Loss: 614.0616, Val Loss: 210.4691\n",
            "Epoch 6036, Train Loss: 621.2691, Val Loss: 256.7746\n",
            "Epoch 6037, Train Loss: 616.9768, Val Loss: 266.2799\n",
            "Epoch 6038, Train Loss: 633.2480, Val Loss: 232.1803\n",
            "Epoch 6039, Train Loss: 615.0151, Val Loss: 225.5187\n",
            "Epoch 6040, Train Loss: 636.7438, Val Loss: 231.8070\n",
            "Epoch 6041, Train Loss: 627.9496, Val Loss: 221.6933\n",
            "Epoch 6042, Train Loss: 624.2966, Val Loss: 230.7343\n",
            "Epoch 6043, Train Loss: 623.1735, Val Loss: 223.0903\n",
            "Epoch 6044, Train Loss: 585.5136, Val Loss: 294.0501\n",
            "Epoch 6045, Train Loss: 593.4807, Val Loss: 282.2898\n",
            "Epoch 6046, Train Loss: 617.6526, Val Loss: 302.8455\n",
            "Epoch 6047, Train Loss: 588.3569, Val Loss: 182.4648\n",
            "Epoch 6048, Train Loss: 613.1747, Val Loss: 171.5771\n",
            "Epoch 6049, Train Loss: 581.6725, Val Loss: 184.8520\n",
            "Epoch 6050, Train Loss: 628.5197, Val Loss: 198.5903\n",
            "Epoch 6051, Train Loss: 598.2693, Val Loss: 188.2179\n",
            "Epoch 6052, Train Loss: 602.9731, Val Loss: 195.8410\n",
            "Epoch 6053, Train Loss: 582.0231, Val Loss: 195.8596\n",
            "Epoch 6054, Train Loss: 612.4436, Val Loss: 185.2757\n",
            "Epoch 6055, Train Loss: 624.0507, Val Loss: 188.6446\n",
            "Epoch 6056, Train Loss: 601.4287, Val Loss: 185.3160\n",
            "Epoch 6057, Train Loss: 623.1702, Val Loss: 189.8339\n",
            "Epoch 6058, Train Loss: 620.0194, Val Loss: 193.9149\n",
            "Epoch 6059, Train Loss: 610.8433, Val Loss: 211.3111\n",
            "Epoch 6060, Train Loss: 608.4875, Val Loss: 239.4029\n",
            "Epoch 6061, Train Loss: 626.8516, Val Loss: 215.7238\n",
            "Epoch 6062, Train Loss: 597.7585, Val Loss: 206.8476\n",
            "Epoch 6063, Train Loss: 614.9291, Val Loss: 214.3908\n",
            "Epoch 6064, Train Loss: 606.3055, Val Loss: 177.9280\n",
            "Epoch 6065, Train Loss: 618.8517, Val Loss: 173.1248\n",
            "Epoch 6066, Train Loss: 533.2388, Val Loss: 199.9385\n",
            "Epoch 6067, Train Loss: 614.7976, Val Loss: 182.9560\n",
            "Epoch 6068, Train Loss: 579.3988, Val Loss: 181.0425\n",
            "Epoch 6069, Train Loss: 614.8263, Val Loss: 181.8270\n",
            "Epoch 6070, Train Loss: 580.1059, Val Loss: 196.8442\n",
            "Epoch 6071, Train Loss: 598.9171, Val Loss: 228.4250\n",
            "Epoch 6072, Train Loss: 611.1777, Val Loss: 231.7815\n",
            "Epoch 6073, Train Loss: 608.8200, Val Loss: 257.5921\n",
            "Epoch 6074, Train Loss: 617.9404, Val Loss: 264.8854\n",
            "Epoch 6075, Train Loss: 609.0453, Val Loss: 195.9516\n",
            "Epoch 6076, Train Loss: 599.1863, Val Loss: 178.3918\n",
            "Epoch 6077, Train Loss: 634.8075, Val Loss: 199.0900\n",
            "Epoch 6078, Train Loss: 627.7052, Val Loss: 184.6235\n",
            "Epoch 6079, Train Loss: 616.4917, Val Loss: 197.7173\n",
            "Epoch 6080, Train Loss: 619.7263, Val Loss: 219.4550\n",
            "Epoch 6081, Train Loss: 583.0323, Val Loss: 204.5089\n",
            "Epoch 6082, Train Loss: 622.0562, Val Loss: 209.4141\n",
            "Epoch 6083, Train Loss: 568.4469, Val Loss: 280.6086\n",
            "Epoch 6084, Train Loss: 583.4276, Val Loss: 259.2804\n",
            "Epoch 6085, Train Loss: 618.3500, Val Loss: 208.5427\n",
            "Epoch 6086, Train Loss: 589.7391, Val Loss: 184.8906\n",
            "Epoch 6087, Train Loss: 609.9617, Val Loss: 181.7020\n",
            "Epoch 6088, Train Loss: 628.3174, Val Loss: 191.9253\n",
            "Epoch 6089, Train Loss: 618.9629, Val Loss: 186.6579\n",
            "Epoch 6090, Train Loss: 627.0957, Val Loss: 176.5597\n",
            "Epoch 6091, Train Loss: 628.6667, Val Loss: 173.8249\n",
            "Epoch 6092, Train Loss: 576.7671, Val Loss: 181.2658\n",
            "Epoch 6093, Train Loss: 587.7402, Val Loss: 323.3243\n",
            "Epoch 6094, Train Loss: 614.5685, Val Loss: 412.5625\n",
            "Epoch 6095, Train Loss: 587.7814, Val Loss: 207.5697\n",
            "Epoch 6096, Train Loss: 624.1215, Val Loss: 208.4428\n",
            "Epoch 6097, Train Loss: 623.2455, Val Loss: 271.3249\n",
            "Epoch 6098, Train Loss: 622.4178, Val Loss: 333.1444\n",
            "Epoch 6099, Train Loss: 626.5608, Val Loss: 251.7240\n",
            "Epoch 6100, Train Loss: 609.2250, Val Loss: 215.7353\n",
            "Epoch 6101, Train Loss: 564.6860, Val Loss: 179.9559\n",
            "Epoch 6102, Train Loss: 622.4808, Val Loss: 170.1876\n",
            "Epoch 6103, Train Loss: 614.5509, Val Loss: 171.3251\n",
            "Epoch 6104, Train Loss: 616.5866, Val Loss: 171.9311\n",
            "Epoch 6105, Train Loss: 619.3857, Val Loss: 170.1975\n",
            "Epoch 6106, Train Loss: 618.4863, Val Loss: 168.2627\n",
            "Epoch 6107, Train Loss: 629.7445, Val Loss: 183.5253\n",
            "Epoch 6108, Train Loss: 596.7533, Val Loss: 197.3841\n",
            "Epoch 6109, Train Loss: 607.9488, Val Loss: 191.6031\n",
            "Epoch 6110, Train Loss: 626.4596, Val Loss: 191.5883\n",
            "Epoch 6111, Train Loss: 625.9658, Val Loss: 200.0691\n",
            "Epoch 6112, Train Loss: 613.6183, Val Loss: 212.9789\n",
            "Epoch 6113, Train Loss: 628.9125, Val Loss: 187.4446\n",
            "Epoch 6114, Train Loss: 597.6276, Val Loss: 183.2475\n",
            "Epoch 6115, Train Loss: 601.1869, Val Loss: 255.0808\n",
            "Epoch 6116, Train Loss: 609.4014, Val Loss: 383.2771\n",
            "Epoch 6117, Train Loss: 626.5441, Val Loss: 352.7841\n",
            "Epoch 6118, Train Loss: 622.2283, Val Loss: 289.0064\n",
            "Epoch 6119, Train Loss: 604.6597, Val Loss: 262.6869\n",
            "Epoch 6120, Train Loss: 624.7512, Val Loss: 198.0627\n",
            "Epoch 6121, Train Loss: 610.0933, Val Loss: 187.1607\n",
            "Epoch 6122, Train Loss: 609.6719, Val Loss: 219.7563\n",
            "Epoch 6123, Train Loss: 613.7840, Val Loss: 267.2490\n",
            "Epoch 6124, Train Loss: 620.4234, Val Loss: 283.4986\n",
            "Epoch 6125, Train Loss: 579.2702, Val Loss: 222.5244\n",
            "Epoch 6126, Train Loss: 627.6917, Val Loss: 199.7975\n",
            "Epoch 6127, Train Loss: 622.5300, Val Loss: 209.9408\n",
            "Epoch 6128, Train Loss: 588.0409, Val Loss: 222.4894\n",
            "Epoch 6129, Train Loss: 615.9061, Val Loss: 204.1266\n",
            "Epoch 6130, Train Loss: 610.7961, Val Loss: 180.8057\n",
            "Epoch 6131, Train Loss: 626.0899, Val Loss: 177.0828\n",
            "Epoch 6132, Train Loss: 628.7207, Val Loss: 212.8568\n",
            "Epoch 6133, Train Loss: 623.2507, Val Loss: 243.8515\n",
            "Epoch 6134, Train Loss: 614.3109, Val Loss: 224.5327\n",
            "Epoch 6135, Train Loss: 634.0876, Val Loss: 262.4580\n",
            "Epoch 6136, Train Loss: 616.2704, Val Loss: 317.2192\n",
            "Epoch 6137, Train Loss: 627.0995, Val Loss: 252.0086\n",
            "Epoch 6138, Train Loss: 621.0670, Val Loss: 206.0371\n",
            "Epoch 6139, Train Loss: 624.6439, Val Loss: 178.5007\n",
            "Epoch 6140, Train Loss: 602.4538, Val Loss: 174.7206\n",
            "Epoch 6141, Train Loss: 588.8789, Val Loss: 187.6405\n",
            "Epoch 6142, Train Loss: 598.2919, Val Loss: 194.6686\n",
            "Epoch 6143, Train Loss: 620.5227, Val Loss: 188.4568\n",
            "Epoch 6144, Train Loss: 558.2661, Val Loss: 190.9686\n",
            "Epoch 6145, Train Loss: 614.4808, Val Loss: 234.6553\n",
            "Epoch 6146, Train Loss: 588.2980, Val Loss: 208.7590\n",
            "Epoch 6147, Train Loss: 615.3195, Val Loss: 187.9248\n",
            "Epoch 6148, Train Loss: 613.3553, Val Loss: 184.8683\n",
            "Epoch 6149, Train Loss: 617.4242, Val Loss: 220.5537\n",
            "Epoch 6150, Train Loss: 614.5629, Val Loss: 202.1281\n",
            "Epoch 6151, Train Loss: 616.0131, Val Loss: 183.3003\n",
            "Epoch 6152, Train Loss: 600.9111, Val Loss: 172.4399\n",
            "Epoch 6153, Train Loss: 604.2394, Val Loss: 178.9335\n",
            "Epoch 6154, Train Loss: 618.6382, Val Loss: 178.6657\n",
            "Epoch 6155, Train Loss: 610.3907, Val Loss: 183.0620\n",
            "Epoch 6156, Train Loss: 599.5487, Val Loss: 204.1826\n",
            "Epoch 6157, Train Loss: 622.5077, Val Loss: 195.2381\n",
            "Epoch 6158, Train Loss: 626.6611, Val Loss: 177.7276\n",
            "Epoch 6159, Train Loss: 612.7909, Val Loss: 172.9573\n",
            "Epoch 6160, Train Loss: 627.1788, Val Loss: 169.0610\n",
            "Epoch 6161, Train Loss: 624.1730, Val Loss: 167.3531\n",
            "Epoch 6162, Train Loss: 611.0878, Val Loss: 170.2052\n",
            "Epoch 6163, Train Loss: 610.0399, Val Loss: 180.9900\n",
            "Epoch 6164, Train Loss: 601.7570, Val Loss: 227.3699\n",
            "Epoch 6165, Train Loss: 611.2257, Val Loss: 318.6286\n",
            "Epoch 6166, Train Loss: 601.2357, Val Loss: 382.1136\n",
            "Epoch 6167, Train Loss: 615.8811, Val Loss: 343.0257\n",
            "Epoch 6168, Train Loss: 624.7261, Val Loss: 295.9714\n",
            "Epoch 6169, Train Loss: 562.9699, Val Loss: 343.5188\n",
            "Epoch 6170, Train Loss: 607.6522, Val Loss: 324.6890\n",
            "Epoch 6171, Train Loss: 621.8869, Val Loss: 342.3600\n",
            "Epoch 6172, Train Loss: 612.4373, Val Loss: 387.2465\n",
            "Epoch 6173, Train Loss: 626.3663, Val Loss: 480.0248\n",
            "Epoch 6174, Train Loss: 627.4166, Val Loss: 326.4401\n",
            "Epoch 6175, Train Loss: 622.3803, Val Loss: 213.2688\n",
            "Epoch 6176, Train Loss: 565.7506, Val Loss: 196.0976\n",
            "Epoch 6177, Train Loss: 627.1160, Val Loss: 230.3117\n",
            "Epoch 6178, Train Loss: 632.2030, Val Loss: 192.5209\n",
            "Epoch 6179, Train Loss: 605.1790, Val Loss: 176.5725\n",
            "Epoch 6180, Train Loss: 621.3645, Val Loss: 184.3062\n",
            "Epoch 6181, Train Loss: 622.1847, Val Loss: 172.6675\n",
            "Epoch 6182, Train Loss: 616.7057, Val Loss: 171.9666\n",
            "Epoch 6183, Train Loss: 612.1107, Val Loss: 175.0861\n",
            "Epoch 6184, Train Loss: 620.3245, Val Loss: 176.8746\n",
            "Epoch 6185, Train Loss: 620.1790, Val Loss: 176.4364\n",
            "Epoch 6186, Train Loss: 619.0681, Val Loss: 175.0843\n",
            "Epoch 6187, Train Loss: 603.3703, Val Loss: 204.7605\n",
            "Epoch 6188, Train Loss: 620.4275, Val Loss: 258.5742\n",
            "Epoch 6189, Train Loss: 623.2894, Val Loss: 229.5539\n",
            "Epoch 6190, Train Loss: 588.6494, Val Loss: 195.7687\n",
            "Epoch 6191, Train Loss: 597.1118, Val Loss: 188.3906\n",
            "Epoch 6192, Train Loss: 619.4963, Val Loss: 192.9412\n",
            "Epoch 6193, Train Loss: 623.1937, Val Loss: 206.2449\n",
            "Epoch 6194, Train Loss: 612.5568, Val Loss: 217.6298\n",
            "Epoch 6195, Train Loss: 622.7878, Val Loss: 201.7670\n",
            "Epoch 6196, Train Loss: 605.8902, Val Loss: 194.6038\n",
            "Epoch 6197, Train Loss: 612.1947, Val Loss: 249.0397\n",
            "Epoch 6198, Train Loss: 608.1630, Val Loss: 313.2468\n",
            "Epoch 6199, Train Loss: 609.7801, Val Loss: 276.2156\n",
            "Epoch 6200, Train Loss: 607.0008, Val Loss: 269.4789\n",
            "Epoch 6201, Train Loss: 628.0904, Val Loss: 251.5583\n",
            "Epoch 6202, Train Loss: 610.2373, Val Loss: 183.0805\n",
            "Epoch 6203, Train Loss: 597.4009, Val Loss: 184.3073\n",
            "Epoch 6204, Train Loss: 578.6808, Val Loss: 222.2102\n",
            "Epoch 6205, Train Loss: 624.5382, Val Loss: 259.8621\n",
            "Epoch 6206, Train Loss: 621.8459, Val Loss: 260.0805\n",
            "Epoch 6207, Train Loss: 582.0056, Val Loss: 222.5401\n",
            "Epoch 6208, Train Loss: 625.3483, Val Loss: 213.0402\n",
            "Epoch 6209, Train Loss: 622.4629, Val Loss: 178.5695\n",
            "Epoch 6210, Train Loss: 626.5987, Val Loss: 174.9261\n",
            "Epoch 6211, Train Loss: 614.0812, Val Loss: 180.8875\n",
            "Epoch 6212, Train Loss: 594.6707, Val Loss: 184.1076\n",
            "Epoch 6213, Train Loss: 596.3330, Val Loss: 177.1517\n",
            "Epoch 6214, Train Loss: 598.8382, Val Loss: 216.4088\n",
            "Epoch 6215, Train Loss: 618.4518, Val Loss: 295.6868\n",
            "Epoch 6216, Train Loss: 572.8875, Val Loss: 325.6016\n",
            "Epoch 6217, Train Loss: 624.8414, Val Loss: 233.3760\n",
            "Epoch 6218, Train Loss: 613.4618, Val Loss: 197.7878\n",
            "Epoch 6219, Train Loss: 605.4988, Val Loss: 186.9647\n",
            "Epoch 6220, Train Loss: 619.6075, Val Loss: 200.3893\n",
            "Epoch 6221, Train Loss: 623.6328, Val Loss: 279.3982\n",
            "Epoch 6222, Train Loss: 621.3809, Val Loss: 246.4336\n",
            "Epoch 6223, Train Loss: 585.0833, Val Loss: 237.3347\n",
            "Epoch 6224, Train Loss: 606.7358, Val Loss: 236.8818\n",
            "Epoch 6225, Train Loss: 589.6680, Val Loss: 224.7166\n",
            "Epoch 6226, Train Loss: 624.0298, Val Loss: 264.9877\n",
            "Epoch 6227, Train Loss: 609.4456, Val Loss: 174.0582\n",
            "Epoch 6228, Train Loss: 629.7703, Val Loss: 225.5038\n",
            "Epoch 6229, Train Loss: 621.6528, Val Loss: 299.0131\n",
            "Epoch 6230, Train Loss: 620.5049, Val Loss: 257.3840\n",
            "Epoch 6231, Train Loss: 599.4859, Val Loss: 221.6212\n",
            "Epoch 6232, Train Loss: 628.6477, Val Loss: 220.8429\n",
            "Epoch 6233, Train Loss: 610.4333, Val Loss: 179.7786\n",
            "Epoch 6234, Train Loss: 622.9438, Val Loss: 178.2302\n",
            "Epoch 6235, Train Loss: 598.4828, Val Loss: 217.7799\n",
            "Epoch 6236, Train Loss: 596.7131, Val Loss: 231.6195\n",
            "Epoch 6237, Train Loss: 623.9835, Val Loss: 191.3899\n",
            "Epoch 6238, Train Loss: 613.8200, Val Loss: 176.6399\n",
            "Epoch 6239, Train Loss: 619.3194, Val Loss: 176.6171\n",
            "Epoch 6240, Train Loss: 603.2266, Val Loss: 177.8331\n",
            "Epoch 6241, Train Loss: 631.9181, Val Loss: 181.9998\n",
            "Epoch 6242, Train Loss: 622.8642, Val Loss: 204.7436\n",
            "Epoch 6243, Train Loss: 563.1879, Val Loss: 265.6763\n",
            "Epoch 6244, Train Loss: 619.8130, Val Loss: 335.4042\n",
            "Epoch 6245, Train Loss: 621.9542, Val Loss: 275.3473\n",
            "Epoch 6246, Train Loss: 615.2323, Val Loss: 241.1310\n",
            "Epoch 6247, Train Loss: 584.1228, Val Loss: 273.8623\n",
            "Epoch 6248, Train Loss: 608.6217, Val Loss: 294.9420\n",
            "Epoch 6249, Train Loss: 598.9505, Val Loss: 194.8793\n",
            "Epoch 6250, Train Loss: 594.5651, Val Loss: 171.8572\n",
            "Epoch 6251, Train Loss: 590.3206, Val Loss: 209.8978\n",
            "Epoch 6252, Train Loss: 619.1023, Val Loss: 236.4487\n",
            "Epoch 6253, Train Loss: 630.0071, Val Loss: 199.1113\n",
            "Epoch 6254, Train Loss: 618.8114, Val Loss: 184.6224\n",
            "Epoch 6255, Train Loss: 620.2491, Val Loss: 189.6266\n",
            "Epoch 6256, Train Loss: 584.1251, Val Loss: 183.7135\n",
            "Epoch 6257, Train Loss: 618.4674, Val Loss: 178.4343\n",
            "Epoch 6258, Train Loss: 596.4823, Val Loss: 178.8928\n",
            "Epoch 6259, Train Loss: 625.3532, Val Loss: 175.5290\n",
            "Epoch 6260, Train Loss: 629.0023, Val Loss: 173.5368\n",
            "Epoch 6261, Train Loss: 623.4469, Val Loss: 171.2832\n",
            "Epoch 6262, Train Loss: 615.1487, Val Loss: 182.9633\n",
            "Epoch 6263, Train Loss: 625.5228, Val Loss: 207.0268\n",
            "Epoch 6264, Train Loss: 619.8365, Val Loss: 181.9198\n",
            "Epoch 6265, Train Loss: 599.0682, Val Loss: 178.6341\n",
            "Epoch 6266, Train Loss: 620.5239, Val Loss: 193.2476\n",
            "Epoch 6267, Train Loss: 603.7017, Val Loss: 197.6949\n",
            "Epoch 6268, Train Loss: 593.0690, Val Loss: 211.5317\n",
            "Epoch 6269, Train Loss: 584.3734, Val Loss: 219.8861\n",
            "Epoch 6270, Train Loss: 618.8003, Val Loss: 209.1073\n",
            "Epoch 6271, Train Loss: 571.7735, Val Loss: 199.2474\n",
            "Epoch 6272, Train Loss: 594.3625, Val Loss: 226.1869\n",
            "Epoch 6273, Train Loss: 580.6540, Val Loss: 249.3327\n",
            "Epoch 6274, Train Loss: 609.7492, Val Loss: 225.7767\n",
            "Epoch 6275, Train Loss: 617.4672, Val Loss: 231.8386\n",
            "Epoch 6276, Train Loss: 616.0933, Val Loss: 242.4246\n",
            "Epoch 6277, Train Loss: 612.6450, Val Loss: 285.3989\n",
            "Epoch 6278, Train Loss: 593.6483, Val Loss: 318.1567\n",
            "Epoch 6279, Train Loss: 621.8378, Val Loss: 298.7219\n",
            "Epoch 6280, Train Loss: 608.6539, Val Loss: 229.3057\n",
            "Epoch 6281, Train Loss: 622.1531, Val Loss: 225.8410\n",
            "Epoch 6282, Train Loss: 630.5233, Val Loss: 189.0627\n",
            "Epoch 6283, Train Loss: 618.7481, Val Loss: 188.1466\n",
            "Epoch 6284, Train Loss: 622.4740, Val Loss: 182.6021\n",
            "Epoch 6285, Train Loss: 593.7530, Val Loss: 170.3396\n",
            "Epoch 6286, Train Loss: 626.6727, Val Loss: 181.6014\n",
            "Epoch 6287, Train Loss: 559.1760, Val Loss: 329.4115\n",
            "Epoch 6288, Train Loss: 619.3652, Val Loss: 465.9876\n",
            "Epoch 6289, Train Loss: 608.1974, Val Loss: 367.4910\n",
            "Epoch 6290, Train Loss: 621.2956, Val Loss: 184.6193\n",
            "Epoch 6291, Train Loss: 614.4559, Val Loss: 177.4770\n",
            "Epoch 6292, Train Loss: 574.3987, Val Loss: 249.4700\n",
            "Epoch 6293, Train Loss: 594.6653, Val Loss: 254.6321\n",
            "Epoch 6294, Train Loss: 620.3800, Val Loss: 262.4945\n",
            "Epoch 6295, Train Loss: 525.0506, Val Loss: 243.8148\n",
            "Epoch 6296, Train Loss: 590.0927, Val Loss: 220.4856\n",
            "Epoch 6297, Train Loss: 622.0090, Val Loss: 174.9712\n",
            "Epoch 6298, Train Loss: 598.1872, Val Loss: 177.9893\n",
            "Epoch 6299, Train Loss: 612.8680, Val Loss: 172.2148\n",
            "Epoch 6300, Train Loss: 637.4974, Val Loss: 168.1084\n",
            "Epoch 6301, Train Loss: 576.1186, Val Loss: 177.8305\n",
            "Epoch 6302, Train Loss: 623.9586, Val Loss: 236.8403\n",
            "Epoch 6303, Train Loss: 631.9210, Val Loss: 227.2335\n",
            "Epoch 6304, Train Loss: 630.2862, Val Loss: 169.9838\n",
            "Epoch 6305, Train Loss: 614.8884, Val Loss: 169.7870\n",
            "Epoch 6306, Train Loss: 625.9092, Val Loss: 173.0617\n",
            "Epoch 6307, Train Loss: 619.5580, Val Loss: 171.6160\n",
            "Epoch 6308, Train Loss: 616.9563, Val Loss: 169.1188\n",
            "Epoch 6309, Train Loss: 613.4089, Val Loss: 180.6788\n",
            "Epoch 6310, Train Loss: 614.8112, Val Loss: 218.0951\n",
            "Epoch 6311, Train Loss: 564.5803, Val Loss: 262.0577\n",
            "Epoch 6312, Train Loss: 620.5445, Val Loss: 384.7515\n",
            "Epoch 6313, Train Loss: 615.1941, Val Loss: 301.7279\n",
            "Epoch 6314, Train Loss: 609.6503, Val Loss: 196.7329\n",
            "Epoch 6315, Train Loss: 628.5684, Val Loss: 169.7210\n",
            "Epoch 6316, Train Loss: 623.5950, Val Loss: 170.4663\n",
            "Epoch 6317, Train Loss: 604.7541, Val Loss: 237.5481\n",
            "Epoch 6318, Train Loss: 626.5664, Val Loss: 281.3366\n",
            "Epoch 6319, Train Loss: 607.1391, Val Loss: 242.3251\n",
            "Epoch 6320, Train Loss: 610.9332, Val Loss: 202.4253\n",
            "Epoch 6321, Train Loss: 618.9292, Val Loss: 173.3063\n",
            "Epoch 6322, Train Loss: 616.9598, Val Loss: 171.4368\n",
            "Epoch 6323, Train Loss: 616.7909, Val Loss: 188.5870\n",
            "Epoch 6324, Train Loss: 618.9035, Val Loss: 227.0783\n",
            "Epoch 6325, Train Loss: 621.7093, Val Loss: 208.0119\n",
            "Epoch 6326, Train Loss: 632.1455, Val Loss: 184.6487\n",
            "Epoch 6327, Train Loss: 601.6017, Val Loss: 168.4314\n",
            "Epoch 6328, Train Loss: 603.8439, Val Loss: 174.4639\n",
            "Epoch 6329, Train Loss: 627.2542, Val Loss: 244.9935\n",
            "Epoch 6330, Train Loss: 607.5841, Val Loss: 312.4092\n",
            "Epoch 6331, Train Loss: 608.3708, Val Loss: 266.4076\n",
            "Epoch 6332, Train Loss: 548.7996, Val Loss: 279.5849\n",
            "Epoch 6333, Train Loss: 597.1442, Val Loss: 241.8471\n",
            "Epoch 6334, Train Loss: 585.2572, Val Loss: 214.2295\n",
            "Epoch 6335, Train Loss: 625.3969, Val Loss: 188.2896\n",
            "Epoch 6336, Train Loss: 609.6832, Val Loss: 183.7959\n",
            "Epoch 6337, Train Loss: 617.7101, Val Loss: 198.1118\n",
            "Epoch 6338, Train Loss: 618.8494, Val Loss: 204.6768\n",
            "Epoch 6339, Train Loss: 607.1174, Val Loss: 292.0289\n",
            "Epoch 6340, Train Loss: 624.8378, Val Loss: 251.4066\n",
            "Epoch 6341, Train Loss: 610.6758, Val Loss: 194.5819\n",
            "Epoch 6342, Train Loss: 586.0512, Val Loss: 204.9971\n",
            "Epoch 6343, Train Loss: 623.5282, Val Loss: 200.9058\n",
            "Epoch 6344, Train Loss: 629.2997, Val Loss: 175.6248\n",
            "Epoch 6345, Train Loss: 608.4574, Val Loss: 178.7292\n",
            "Epoch 6346, Train Loss: 622.7116, Val Loss: 180.9447\n",
            "Epoch 6347, Train Loss: 612.6757, Val Loss: 189.9596\n",
            "Epoch 6348, Train Loss: 607.4595, Val Loss: 205.9680\n",
            "Epoch 6349, Train Loss: 585.6264, Val Loss: 216.5160\n",
            "Epoch 6350, Train Loss: 620.9724, Val Loss: 197.8184\n",
            "Epoch 6351, Train Loss: 622.8806, Val Loss: 182.9812\n",
            "Epoch 6352, Train Loss: 630.6575, Val Loss: 178.0265\n",
            "Epoch 6353, Train Loss: 577.9991, Val Loss: 219.7144\n",
            "Epoch 6354, Train Loss: 622.1790, Val Loss: 306.2771\n",
            "Epoch 6355, Train Loss: 614.6409, Val Loss: 321.1693\n",
            "Epoch 6356, Train Loss: 611.3752, Val Loss: 263.2159\n",
            "Epoch 6357, Train Loss: 531.8722, Val Loss: 195.4084\n",
            "Epoch 6358, Train Loss: 614.5413, Val Loss: 220.9455\n",
            "Epoch 6359, Train Loss: 593.5844, Val Loss: 211.3632\n",
            "Epoch 6360, Train Loss: 599.1398, Val Loss: 176.6063\n",
            "Epoch 6361, Train Loss: 600.8332, Val Loss: 172.6832\n",
            "Epoch 6362, Train Loss: 597.3223, Val Loss: 181.9154\n",
            "Epoch 6363, Train Loss: 626.0367, Val Loss: 219.7981\n",
            "Epoch 6364, Train Loss: 624.5612, Val Loss: 201.3794\n",
            "Epoch 6365, Train Loss: 613.3597, Val Loss: 172.6571\n",
            "Epoch 6366, Train Loss: 617.0657, Val Loss: 166.1187\n",
            "Epoch 6367, Train Loss: 608.0129, Val Loss: 180.0221\n",
            "Epoch 6368, Train Loss: 591.4205, Val Loss: 196.1064\n",
            "Epoch 6369, Train Loss: 584.1181, Val Loss: 208.8650\n",
            "Epoch 6370, Train Loss: 603.7579, Val Loss: 212.8876\n",
            "Epoch 6371, Train Loss: 597.8325, Val Loss: 248.0430\n",
            "Epoch 6372, Train Loss: 603.4167, Val Loss: 253.8354\n",
            "Epoch 6373, Train Loss: 619.9599, Val Loss: 217.1204\n",
            "Epoch 6374, Train Loss: 618.8666, Val Loss: 185.9028\n",
            "Epoch 6375, Train Loss: 558.2458, Val Loss: 190.2050\n",
            "Epoch 6376, Train Loss: 545.2664, Val Loss: 245.1761\n",
            "Epoch 6377, Train Loss: 626.6977, Val Loss: 252.2706\n",
            "Epoch 6378, Train Loss: 620.7654, Val Loss: 244.3813\n",
            "Epoch 6379, Train Loss: 612.8146, Val Loss: 202.0455\n",
            "Epoch 6380, Train Loss: 625.5747, Val Loss: 201.7998\n",
            "Epoch 6381, Train Loss: 608.5287, Val Loss: 206.6877\n",
            "Epoch 6382, Train Loss: 625.0136, Val Loss: 203.1455\n",
            "Epoch 6383, Train Loss: 615.9197, Val Loss: 189.0617\n",
            "Epoch 6384, Train Loss: 626.7400, Val Loss: 181.9009\n",
            "Epoch 6385, Train Loss: 605.3817, Val Loss: 179.8319\n",
            "Epoch 6386, Train Loss: 602.3391, Val Loss: 181.4365\n",
            "Epoch 6387, Train Loss: 575.1880, Val Loss: 196.4039\n",
            "Epoch 6388, Train Loss: 604.2270, Val Loss: 198.7047\n",
            "Epoch 6389, Train Loss: 622.2425, Val Loss: 185.5132\n",
            "Epoch 6390, Train Loss: 602.1058, Val Loss: 187.0778\n",
            "Epoch 6391, Train Loss: 603.4959, Val Loss: 206.1348\n",
            "Epoch 6392, Train Loss: 555.9643, Val Loss: 263.6401\n",
            "Epoch 6393, Train Loss: 638.8093, Val Loss: 287.5841\n",
            "Epoch 6394, Train Loss: 613.6163, Val Loss: 212.6413\n",
            "Epoch 6395, Train Loss: 614.4612, Val Loss: 194.0149\n",
            "Epoch 6396, Train Loss: 601.2185, Val Loss: 192.3285\n",
            "Epoch 6397, Train Loss: 617.5639, Val Loss: 223.1722\n",
            "Epoch 6398, Train Loss: 594.5655, Val Loss: 318.8946\n",
            "Epoch 6399, Train Loss: 627.8766, Val Loss: 237.9981\n",
            "Epoch 6400, Train Loss: 615.6966, Val Loss: 182.4828\n",
            "Epoch 6401, Train Loss: 619.4912, Val Loss: 179.1809\n",
            "Epoch 6402, Train Loss: 615.8986, Val Loss: 193.7893\n",
            "Epoch 6403, Train Loss: 610.3204, Val Loss: 217.1039\n",
            "Epoch 6404, Train Loss: 573.5612, Val Loss: 234.0341\n",
            "Epoch 6405, Train Loss: 619.4288, Val Loss: 240.8270\n",
            "Epoch 6406, Train Loss: 625.8655, Val Loss: 199.2792\n",
            "Epoch 6407, Train Loss: 597.8714, Val Loss: 188.8702\n",
            "Epoch 6408, Train Loss: 621.7320, Val Loss: 194.5791\n",
            "Epoch 6409, Train Loss: 621.1513, Val Loss: 191.8029\n",
            "Epoch 6410, Train Loss: 600.5385, Val Loss: 170.4313\n",
            "Epoch 6411, Train Loss: 621.3253, Val Loss: 174.1388\n",
            "Epoch 6412, Train Loss: 603.3730, Val Loss: 179.4677\n",
            "Epoch 6413, Train Loss: 616.5898, Val Loss: 176.6514\n",
            "Epoch 6414, Train Loss: 619.3594, Val Loss: 172.2495\n",
            "Epoch 6415, Train Loss: 615.2500, Val Loss: 173.0974\n",
            "Epoch 6416, Train Loss: 592.1036, Val Loss: 203.7084\n",
            "Epoch 6417, Train Loss: 613.4162, Val Loss: 240.2534\n",
            "Epoch 6418, Train Loss: 607.2150, Val Loss: 190.2898\n",
            "Epoch 6419, Train Loss: 612.9944, Val Loss: 173.2326\n",
            "Epoch 6420, Train Loss: 605.8113, Val Loss: 190.1096\n",
            "Epoch 6421, Train Loss: 592.0812, Val Loss: 249.5152\n",
            "Epoch 6422, Train Loss: 612.8431, Val Loss: 267.2515\n",
            "Epoch 6423, Train Loss: 588.5233, Val Loss: 191.0973\n",
            "Epoch 6424, Train Loss: 626.8548, Val Loss: 182.0938\n",
            "Epoch 6425, Train Loss: 615.6913, Val Loss: 189.5941\n",
            "Epoch 6426, Train Loss: 612.7846, Val Loss: 242.2952\n",
            "Epoch 6427, Train Loss: 601.8267, Val Loss: 281.9444\n",
            "Epoch 6428, Train Loss: 610.0166, Val Loss: 247.8982\n",
            "Epoch 6429, Train Loss: 580.2459, Val Loss: 206.1553\n",
            "Epoch 6430, Train Loss: 596.5970, Val Loss: 210.2224\n",
            "Epoch 6431, Train Loss: 589.1859, Val Loss: 200.8700\n",
            "Epoch 6432, Train Loss: 617.4389, Val Loss: 225.8869\n",
            "Epoch 6433, Train Loss: 599.0367, Val Loss: 216.2301\n",
            "Epoch 6434, Train Loss: 615.4499, Val Loss: 209.6161\n",
            "Epoch 6435, Train Loss: 615.5039, Val Loss: 253.6758\n",
            "Epoch 6436, Train Loss: 618.3548, Val Loss: 311.9317\n",
            "Epoch 6437, Train Loss: 628.0789, Val Loss: 311.8146\n",
            "Epoch 6438, Train Loss: 619.7395, Val Loss: 197.4915\n",
            "Epoch 6439, Train Loss: 604.8864, Val Loss: 177.0002\n",
            "Epoch 6440, Train Loss: 627.1395, Val Loss: 187.0430\n",
            "Epoch 6441, Train Loss: 619.6442, Val Loss: 198.7426\n",
            "Epoch 6442, Train Loss: 622.0288, Val Loss: 210.4874\n",
            "Epoch 6443, Train Loss: 580.8796, Val Loss: 225.0362\n",
            "Epoch 6444, Train Loss: 606.5617, Val Loss: 226.3634\n",
            "Epoch 6445, Train Loss: 617.0278, Val Loss: 193.0160\n",
            "Epoch 6446, Train Loss: 617.7774, Val Loss: 173.9849\n",
            "Epoch 6447, Train Loss: 583.6080, Val Loss: 174.8089\n",
            "Epoch 6448, Train Loss: 620.7416, Val Loss: 186.1434\n",
            "Epoch 6449, Train Loss: 607.3382, Val Loss: 214.8296\n",
            "Epoch 6450, Train Loss: 600.3565, Val Loss: 262.5276\n",
            "Epoch 6451, Train Loss: 623.9199, Val Loss: 212.9631\n",
            "Epoch 6452, Train Loss: 612.9217, Val Loss: 180.5168\n",
            "Epoch 6453, Train Loss: 603.1528, Val Loss: 218.3718\n",
            "Epoch 6454, Train Loss: 624.1115, Val Loss: 292.9870\n",
            "Epoch 6455, Train Loss: 601.9636, Val Loss: 273.1795\n",
            "Epoch 6456, Train Loss: 579.0927, Val Loss: 235.3827\n",
            "Epoch 6457, Train Loss: 614.3760, Val Loss: 227.6572\n",
            "Epoch 6458, Train Loss: 611.7097, Val Loss: 205.3259\n",
            "Epoch 6459, Train Loss: 620.4285, Val Loss: 220.7446\n",
            "Epoch 6460, Train Loss: 607.4640, Val Loss: 348.6938\n",
            "Epoch 6461, Train Loss: 612.9194, Val Loss: 201.3179\n",
            "Epoch 6462, Train Loss: 603.9594, Val Loss: 173.0406\n",
            "Epoch 6463, Train Loss: 618.1983, Val Loss: 166.3196\n",
            "Epoch 6464, Train Loss: 591.4990, Val Loss: 171.8307\n",
            "Epoch 6465, Train Loss: 611.8340, Val Loss: 176.6575\n",
            "Epoch 6466, Train Loss: 624.9267, Val Loss: 176.4133\n",
            "Epoch 6467, Train Loss: 614.5092, Val Loss: 171.8918\n",
            "Epoch 6468, Train Loss: 559.8173, Val Loss: 215.9623\n",
            "Epoch 6469, Train Loss: 620.5865, Val Loss: 205.9989\n",
            "Epoch 6470, Train Loss: 623.2838, Val Loss: 182.7871\n",
            "Epoch 6471, Train Loss: 622.4285, Val Loss: 172.6547\n",
            "Epoch 6472, Train Loss: 625.7221, Val Loss: 189.2376\n",
            "Epoch 6473, Train Loss: 623.5904, Val Loss: 201.0396\n",
            "Epoch 6474, Train Loss: 617.8390, Val Loss: 222.8944\n",
            "Epoch 6475, Train Loss: 624.1630, Val Loss: 222.7449\n",
            "Epoch 6476, Train Loss: 549.7546, Val Loss: 216.8495\n",
            "Epoch 6477, Train Loss: 605.4420, Val Loss: 181.1020\n",
            "Epoch 6478, Train Loss: 625.0641, Val Loss: 177.4753\n",
            "Epoch 6479, Train Loss: 600.1537, Val Loss: 180.9592\n",
            "Epoch 6480, Train Loss: 620.7892, Val Loss: 178.5160\n",
            "Epoch 6481, Train Loss: 589.6611, Val Loss: 176.6642\n",
            "Epoch 6482, Train Loss: 610.6536, Val Loss: 250.2054\n",
            "Epoch 6483, Train Loss: 609.6441, Val Loss: 287.4750\n",
            "Epoch 6484, Train Loss: 611.3049, Val Loss: 222.5689\n",
            "Epoch 6485, Train Loss: 614.2071, Val Loss: 191.6428\n",
            "Epoch 6486, Train Loss: 633.6185, Val Loss: 217.7582\n",
            "Epoch 6487, Train Loss: 573.9486, Val Loss: 188.3791\n",
            "Epoch 6488, Train Loss: 621.0043, Val Loss: 183.3442\n",
            "Epoch 6489, Train Loss: 622.3751, Val Loss: 205.3664\n",
            "Epoch 6490, Train Loss: 600.4192, Val Loss: 218.0723\n",
            "Epoch 6491, Train Loss: 599.8151, Val Loss: 216.1162\n",
            "Epoch 6492, Train Loss: 620.8951, Val Loss: 219.9655\n",
            "Epoch 6493, Train Loss: 598.5386, Val Loss: 221.4639\n",
            "Epoch 6494, Train Loss: 628.8595, Val Loss: 269.8878\n",
            "Epoch 6495, Train Loss: 589.8832, Val Loss: 253.5485\n",
            "Epoch 6496, Train Loss: 570.5556, Val Loss: 181.2372\n",
            "Epoch 6497, Train Loss: 634.0430, Val Loss: 174.3869\n",
            "Epoch 6498, Train Loss: 615.5313, Val Loss: 175.0639\n",
            "Epoch 6499, Train Loss: 622.3945, Val Loss: 206.0573\n",
            "Epoch 6500, Train Loss: 597.9472, Val Loss: 324.7775\n",
            "Epoch 6501, Train Loss: 629.2255, Val Loss: 360.7793\n",
            "Epoch 6502, Train Loss: 606.1943, Val Loss: 189.2213\n",
            "Epoch 6503, Train Loss: 625.1896, Val Loss: 171.7732\n",
            "Epoch 6504, Train Loss: 614.7952, Val Loss: 188.4360\n",
            "Epoch 6505, Train Loss: 585.4446, Val Loss: 228.9227\n",
            "Epoch 6506, Train Loss: 550.4078, Val Loss: 211.1251\n",
            "Epoch 6507, Train Loss: 623.5117, Val Loss: 171.6986\n",
            "Epoch 6508, Train Loss: 614.7795, Val Loss: 170.2668\n",
            "Epoch 6509, Train Loss: 624.2465, Val Loss: 200.1714\n",
            "Epoch 6510, Train Loss: 612.0324, Val Loss: 271.8325\n",
            "Epoch 6511, Train Loss: 604.7604, Val Loss: 261.8997\n",
            "Epoch 6512, Train Loss: 628.7861, Val Loss: 191.7747\n",
            "Epoch 6513, Train Loss: 634.8745, Val Loss: 169.8877\n",
            "Epoch 6514, Train Loss: 581.7363, Val Loss: 168.3803\n",
            "Epoch 6515, Train Loss: 572.6359, Val Loss: 256.6093\n",
            "Epoch 6516, Train Loss: 621.7003, Val Loss: 283.9833\n",
            "Epoch 6517, Train Loss: 629.6372, Val Loss: 229.7759\n",
            "Epoch 6518, Train Loss: 602.9914, Val Loss: 177.7163\n",
            "Epoch 6519, Train Loss: 623.7599, Val Loss: 174.0325\n",
            "Epoch 6520, Train Loss: 601.7011, Val Loss: 181.0301\n",
            "Epoch 6521, Train Loss: 608.9011, Val Loss: 181.3154\n",
            "Epoch 6522, Train Loss: 624.1681, Val Loss: 195.1281\n",
            "Epoch 6523, Train Loss: 626.4474, Val Loss: 187.4072\n",
            "Epoch 6524, Train Loss: 600.0542, Val Loss: 178.5240\n",
            "Epoch 6525, Train Loss: 623.4840, Val Loss: 177.1769\n",
            "Epoch 6526, Train Loss: 623.2528, Val Loss: 181.6165\n",
            "Epoch 6527, Train Loss: 608.7682, Val Loss: 203.3278\n",
            "Epoch 6528, Train Loss: 621.6071, Val Loss: 213.9674\n",
            "Epoch 6529, Train Loss: 611.9843, Val Loss: 198.1590\n",
            "Epoch 6530, Train Loss: 594.6819, Val Loss: 234.6098\n",
            "Epoch 6531, Train Loss: 619.0504, Val Loss: 293.3457\n",
            "Epoch 6532, Train Loss: 608.0818, Val Loss: 274.2790\n",
            "Epoch 6533, Train Loss: 574.4800, Val Loss: 220.7956\n",
            "Epoch 6534, Train Loss: 615.6951, Val Loss: 184.9383\n",
            "Epoch 6535, Train Loss: 615.7257, Val Loss: 192.2569\n",
            "Epoch 6536, Train Loss: 622.2689, Val Loss: 225.3126\n",
            "Epoch 6537, Train Loss: 590.8447, Val Loss: 235.8673\n",
            "Epoch 6538, Train Loss: 625.6557, Val Loss: 198.8457\n",
            "Epoch 6539, Train Loss: 612.8972, Val Loss: 184.3732\n",
            "Epoch 6540, Train Loss: 620.2617, Val Loss: 176.5200\n",
            "Epoch 6541, Train Loss: 617.7213, Val Loss: 177.5521\n",
            "Epoch 6542, Train Loss: 620.3649, Val Loss: 192.7324\n",
            "Epoch 6543, Train Loss: 616.2917, Val Loss: 203.3859\n",
            "Epoch 6544, Train Loss: 635.7235, Val Loss: 180.2054\n",
            "Epoch 6545, Train Loss: 603.3246, Val Loss: 170.6535\n",
            "Epoch 6546, Train Loss: 627.5429, Val Loss: 167.8199\n",
            "Epoch 6547, Train Loss: 588.6432, Val Loss: 191.2882\n",
            "Epoch 6548, Train Loss: 599.8887, Val Loss: 334.1504\n",
            "Epoch 6549, Train Loss: 619.6604, Val Loss: 311.3228\n",
            "Epoch 6550, Train Loss: 621.7765, Val Loss: 245.6797\n",
            "Epoch 6551, Train Loss: 579.7921, Val Loss: 259.8351\n",
            "Epoch 6552, Train Loss: 598.0173, Val Loss: 230.5309\n",
            "Epoch 6553, Train Loss: 593.8336, Val Loss: 249.4058\n",
            "Epoch 6554, Train Loss: 603.3513, Val Loss: 242.6439\n",
            "Epoch 6555, Train Loss: 608.4399, Val Loss: 241.3366\n",
            "Epoch 6556, Train Loss: 615.8701, Val Loss: 237.8716\n",
            "Epoch 6557, Train Loss: 634.6602, Val Loss: 178.6100\n",
            "Epoch 6558, Train Loss: 617.4198, Val Loss: 165.8894\n",
            "Epoch 6559, Train Loss: 623.0026, Val Loss: 169.4788\n",
            "Epoch 6560, Train Loss: 625.4633, Val Loss: 171.8178\n",
            "Epoch 6561, Train Loss: 595.1173, Val Loss: 185.7801\n",
            "Epoch 6562, Train Loss: 625.2767, Val Loss: 212.2845\n",
            "Epoch 6563, Train Loss: 592.2940, Val Loss: 190.0137\n",
            "Epoch 6564, Train Loss: 567.0233, Val Loss: 258.0149\n",
            "Epoch 6565, Train Loss: 622.1758, Val Loss: 306.5232\n",
            "Epoch 6566, Train Loss: 627.7725, Val Loss: 185.7326\n",
            "Epoch 6567, Train Loss: 620.6771, Val Loss: 212.6390\n",
            "Epoch 6568, Train Loss: 567.4549, Val Loss: 182.3601\n",
            "Epoch 6569, Train Loss: 590.2513, Val Loss: 196.0917\n",
            "Epoch 6570, Train Loss: 577.4865, Val Loss: 269.6272\n",
            "Epoch 6571, Train Loss: 591.1265, Val Loss: 305.9770\n",
            "Epoch 6572, Train Loss: 613.1319, Val Loss: 282.8612\n",
            "Epoch 6573, Train Loss: 622.7706, Val Loss: 288.2954\n",
            "Epoch 6574, Train Loss: 582.4431, Val Loss: 323.8264\n",
            "Epoch 6575, Train Loss: 625.2132, Val Loss: 306.6248\n",
            "Epoch 6576, Train Loss: 589.7015, Val Loss: 183.4867\n",
            "Epoch 6577, Train Loss: 628.9819, Val Loss: 182.3196\n",
            "Epoch 6578, Train Loss: 583.5657, Val Loss: 177.8945\n",
            "Epoch 6579, Train Loss: 609.0325, Val Loss: 175.0194\n",
            "Epoch 6580, Train Loss: 619.0156, Val Loss: 229.8788\n",
            "Epoch 6581, Train Loss: 611.0126, Val Loss: 292.5697\n",
            "Epoch 6582, Train Loss: 604.2425, Val Loss: 323.9084\n",
            "Epoch 6583, Train Loss: 612.9369, Val Loss: 380.6487\n",
            "Epoch 6584, Train Loss: 604.6951, Val Loss: 315.9264\n",
            "Epoch 6585, Train Loss: 596.3415, Val Loss: 192.4865\n",
            "Epoch 6586, Train Loss: 621.5985, Val Loss: 184.3596\n",
            "Epoch 6587, Train Loss: 624.6203, Val Loss: 197.7897\n",
            "Epoch 6588, Train Loss: 616.7030, Val Loss: 173.7507\n",
            "Epoch 6589, Train Loss: 618.5003, Val Loss: 164.8545\n",
            "Epoch 6590, Train Loss: 597.8495, Val Loss: 174.8217\n",
            "Epoch 6591, Train Loss: 613.5261, Val Loss: 230.5566\n",
            "Epoch 6592, Train Loss: 589.1673, Val Loss: 273.9945\n",
            "Epoch 6593, Train Loss: 616.1365, Val Loss: 282.9491\n",
            "Epoch 6594, Train Loss: 622.8082, Val Loss: 324.2503\n",
            "Epoch 6595, Train Loss: 603.8613, Val Loss: 258.7802\n",
            "Epoch 6596, Train Loss: 615.2811, Val Loss: 182.7101\n",
            "Epoch 6597, Train Loss: 592.5300, Val Loss: 177.8500\n",
            "Epoch 6598, Train Loss: 607.6637, Val Loss: 187.5338\n",
            "Epoch 6599, Train Loss: 619.4886, Val Loss: 249.9868\n",
            "Epoch 6600, Train Loss: 626.4052, Val Loss: 221.4085\n",
            "Epoch 6601, Train Loss: 611.5714, Val Loss: 222.2094\n",
            "Epoch 6602, Train Loss: 623.9406, Val Loss: 220.9309\n",
            "Epoch 6603, Train Loss: 582.8742, Val Loss: 237.7196\n",
            "Epoch 6604, Train Loss: 579.1379, Val Loss: 260.6744\n",
            "Epoch 6605, Train Loss: 615.3189, Val Loss: 218.9086\n",
            "Epoch 6606, Train Loss: 565.2689, Val Loss: 195.3769\n",
            "Epoch 6607, Train Loss: 606.7516, Val Loss: 172.9461\n",
            "Epoch 6608, Train Loss: 615.7319, Val Loss: 187.3180\n",
            "Epoch 6609, Train Loss: 625.5600, Val Loss: 175.0886\n",
            "Epoch 6610, Train Loss: 577.3421, Val Loss: 226.6663\n",
            "Epoch 6611, Train Loss: 600.5487, Val Loss: 292.6028\n",
            "Epoch 6612, Train Loss: 626.1615, Val Loss: 273.9791\n",
            "Epoch 6613, Train Loss: 619.0003, Val Loss: 203.1358\n",
            "Epoch 6614, Train Loss: 627.5394, Val Loss: 196.5286\n",
            "Epoch 6615, Train Loss: 566.7479, Val Loss: 223.9448\n",
            "Epoch 6616, Train Loss: 625.8820, Val Loss: 272.0365\n",
            "Epoch 6617, Train Loss: 600.4736, Val Loss: 273.2925\n",
            "Epoch 6618, Train Loss: 579.5540, Val Loss: 189.8853\n",
            "Epoch 6619, Train Loss: 619.7727, Val Loss: 170.2186\n",
            "Epoch 6620, Train Loss: 598.6488, Val Loss: 170.5684\n",
            "Epoch 6621, Train Loss: 600.9527, Val Loss: 181.4872\n",
            "Epoch 6622, Train Loss: 614.9908, Val Loss: 199.8959\n",
            "Epoch 6623, Train Loss: 627.2509, Val Loss: 191.9304\n",
            "Epoch 6624, Train Loss: 573.8174, Val Loss: 210.5895\n",
            "Epoch 6625, Train Loss: 611.1203, Val Loss: 228.1334\n",
            "Epoch 6626, Train Loss: 600.8569, Val Loss: 233.7603\n",
            "Epoch 6627, Train Loss: 615.1023, Val Loss: 230.4854\n",
            "Epoch 6628, Train Loss: 580.8890, Val Loss: 293.2622\n",
            "Epoch 6629, Train Loss: 631.4863, Val Loss: 337.6448\n",
            "Epoch 6630, Train Loss: 620.3676, Val Loss: 265.0525\n",
            "Epoch 6631, Train Loss: 596.0194, Val Loss: 192.0186\n",
            "Epoch 6632, Train Loss: 616.4128, Val Loss: 171.4171\n",
            "Epoch 6633, Train Loss: 627.6287, Val Loss: 169.0117\n",
            "Epoch 6634, Train Loss: 618.3263, Val Loss: 175.9582\n",
            "Epoch 6635, Train Loss: 622.2874, Val Loss: 188.4911\n",
            "Epoch 6636, Train Loss: 601.3318, Val Loss: 188.8041\n",
            "Epoch 6637, Train Loss: 602.0726, Val Loss: 180.4563\n",
            "Epoch 6638, Train Loss: 599.4547, Val Loss: 183.8573\n",
            "Epoch 6639, Train Loss: 621.4676, Val Loss: 196.7704\n",
            "Epoch 6640, Train Loss: 612.1129, Val Loss: 203.2856\n",
            "Epoch 6641, Train Loss: 618.8678, Val Loss: 197.9282\n",
            "Epoch 6642, Train Loss: 615.2335, Val Loss: 196.5828\n",
            "Epoch 6643, Train Loss: 619.9158, Val Loss: 195.5056\n",
            "Epoch 6644, Train Loss: 598.1711, Val Loss: 189.9793\n",
            "Epoch 6645, Train Loss: 599.6218, Val Loss: 203.9384\n",
            "Epoch 6646, Train Loss: 619.5022, Val Loss: 208.6741\n",
            "Epoch 6647, Train Loss: 616.3146, Val Loss: 189.9352\n",
            "Epoch 6648, Train Loss: 622.5925, Val Loss: 172.5796\n",
            "Epoch 6649, Train Loss: 591.2779, Val Loss: 169.9867\n",
            "Epoch 6650, Train Loss: 611.1022, Val Loss: 186.0196\n",
            "Epoch 6651, Train Loss: 598.3835, Val Loss: 200.0660\n",
            "Epoch 6652, Train Loss: 592.7135, Val Loss: 228.3000\n",
            "Epoch 6653, Train Loss: 624.7828, Val Loss: 220.8205\n",
            "Epoch 6654, Train Loss: 606.9921, Val Loss: 205.0975\n",
            "Epoch 6655, Train Loss: 600.9529, Val Loss: 187.0853\n",
            "Epoch 6656, Train Loss: 554.0984, Val Loss: 224.2694\n",
            "Epoch 6657, Train Loss: 578.9590, Val Loss: 283.3781\n",
            "Epoch 6658, Train Loss: 624.2449, Val Loss: 351.4228\n",
            "Epoch 6659, Train Loss: 622.4271, Val Loss: 365.1792\n",
            "Epoch 6660, Train Loss: 619.9137, Val Loss: 320.6156\n",
            "Epoch 6661, Train Loss: 610.3107, Val Loss: 211.4551\n",
            "Epoch 6662, Train Loss: 604.9267, Val Loss: 202.4452\n",
            "Epoch 6663, Train Loss: 624.5796, Val Loss: 173.3260\n",
            "Epoch 6664, Train Loss: 616.9588, Val Loss: 173.2939\n",
            "Epoch 6665, Train Loss: 594.8539, Val Loss: 178.2603\n",
            "Epoch 6666, Train Loss: 624.0130, Val Loss: 187.5503\n",
            "Epoch 6667, Train Loss: 608.7200, Val Loss: 193.3794\n",
            "Epoch 6668, Train Loss: 621.0543, Val Loss: 196.2870\n",
            "Epoch 6669, Train Loss: 623.8280, Val Loss: 185.2195\n",
            "Epoch 6670, Train Loss: 591.0858, Val Loss: 175.7467\n",
            "Epoch 6671, Train Loss: 624.8478, Val Loss: 173.2668\n",
            "Epoch 6672, Train Loss: 627.0993, Val Loss: 175.5920\n",
            "Epoch 6673, Train Loss: 624.1364, Val Loss: 175.0150\n",
            "Epoch 6674, Train Loss: 606.3967, Val Loss: 188.5247\n",
            "Epoch 6675, Train Loss: 625.2710, Val Loss: 244.0476\n",
            "Epoch 6676, Train Loss: 622.1650, Val Loss: 231.5884\n",
            "Epoch 6677, Train Loss: 622.9488, Val Loss: 198.4000\n",
            "Epoch 6678, Train Loss: 619.4029, Val Loss: 184.0743\n",
            "Epoch 6679, Train Loss: 615.6105, Val Loss: 203.0030\n",
            "Epoch 6680, Train Loss: 624.7965, Val Loss: 226.4659\n",
            "Epoch 6681, Train Loss: 609.4994, Val Loss: 270.5229\n",
            "Epoch 6682, Train Loss: 587.9209, Val Loss: 267.5509\n",
            "Epoch 6683, Train Loss: 587.3221, Val Loss: 223.4895\n",
            "Epoch 6684, Train Loss: 622.7417, Val Loss: 182.1192\n",
            "Epoch 6685, Train Loss: 619.6118, Val Loss: 177.9907\n",
            "Epoch 6686, Train Loss: 624.5884, Val Loss: 174.5539\n",
            "Epoch 6687, Train Loss: 620.8454, Val Loss: 172.9958\n",
            "Epoch 6688, Train Loss: 614.4660, Val Loss: 174.1895\n",
            "Epoch 6689, Train Loss: 614.6885, Val Loss: 186.7866\n",
            "Epoch 6690, Train Loss: 610.9993, Val Loss: 200.7952\n",
            "Epoch 6691, Train Loss: 621.6978, Val Loss: 259.2465\n",
            "Epoch 6692, Train Loss: 592.4619, Val Loss: 291.0438\n",
            "Epoch 6693, Train Loss: 604.8198, Val Loss: 251.7913\n",
            "Epoch 6694, Train Loss: 598.9886, Val Loss: 208.0972\n",
            "Epoch 6695, Train Loss: 593.5132, Val Loss: 183.0808\n",
            "Epoch 6696, Train Loss: 618.2782, Val Loss: 181.8182\n",
            "Epoch 6697, Train Loss: 623.1458, Val Loss: 197.1669\n",
            "Epoch 6698, Train Loss: 612.5656, Val Loss: 224.1449\n",
            "Epoch 6699, Train Loss: 605.9577, Val Loss: 276.2104\n",
            "Epoch 6700, Train Loss: 600.6655, Val Loss: 274.8268\n",
            "Epoch 6701, Train Loss: 590.9497, Val Loss: 220.9947\n",
            "Epoch 6702, Train Loss: 625.6734, Val Loss: 193.7220\n",
            "Epoch 6703, Train Loss: 621.0344, Val Loss: 184.2705\n",
            "Epoch 6704, Train Loss: 622.1945, Val Loss: 183.6037\n",
            "Epoch 6705, Train Loss: 592.0687, Val Loss: 187.9240\n",
            "Epoch 6706, Train Loss: 617.4023, Val Loss: 203.5997\n",
            "Epoch 6707, Train Loss: 596.2428, Val Loss: 263.8559\n",
            "Epoch 6708, Train Loss: 604.8120, Val Loss: 288.7830\n",
            "Epoch 6709, Train Loss: 611.4039, Val Loss: 234.0303\n",
            "Epoch 6710, Train Loss: 541.0042, Val Loss: 173.2317\n",
            "Epoch 6711, Train Loss: 621.2094, Val Loss: 180.4897\n",
            "Epoch 6712, Train Loss: 620.7908, Val Loss: 177.6976\n",
            "Epoch 6713, Train Loss: 620.8796, Val Loss: 179.4044\n",
            "Epoch 6714, Train Loss: 621.8610, Val Loss: 236.2959\n",
            "Epoch 6715, Train Loss: 592.6628, Val Loss: 211.6235\n",
            "Epoch 6716, Train Loss: 612.2527, Val Loss: 189.2010\n",
            "Epoch 6717, Train Loss: 571.7721, Val Loss: 181.3989\n",
            "Epoch 6718, Train Loss: 593.0866, Val Loss: 231.6991\n",
            "Epoch 6719, Train Loss: 581.6437, Val Loss: 251.7221\n",
            "Epoch 6720, Train Loss: 611.6083, Val Loss: 186.6068\n",
            "Epoch 6721, Train Loss: 603.6112, Val Loss: 185.7903\n",
            "Epoch 6722, Train Loss: 564.2785, Val Loss: 227.9820\n",
            "Epoch 6723, Train Loss: 598.8535, Val Loss: 295.7457\n",
            "Epoch 6724, Train Loss: 628.9850, Val Loss: 261.4606\n",
            "Epoch 6725, Train Loss: 593.5693, Val Loss: 229.9063\n",
            "Epoch 6726, Train Loss: 597.5224, Val Loss: 206.9920\n",
            "Epoch 6727, Train Loss: 618.9649, Val Loss: 187.2814\n",
            "Epoch 6728, Train Loss: 563.5885, Val Loss: 178.3707\n",
            "Epoch 6729, Train Loss: 578.2165, Val Loss: 292.8297\n",
            "Epoch 6730, Train Loss: 586.3334, Val Loss: 306.4883\n",
            "Epoch 6731, Train Loss: 602.7990, Val Loss: 208.4931\n",
            "Epoch 6732, Train Loss: 620.4938, Val Loss: 201.6345\n",
            "Epoch 6733, Train Loss: 611.5479, Val Loss: 227.3271\n",
            "Epoch 6734, Train Loss: 599.2471, Val Loss: 231.6904\n",
            "Epoch 6735, Train Loss: 622.0658, Val Loss: 216.2608\n",
            "Epoch 6736, Train Loss: 594.7737, Val Loss: 187.4647\n",
            "Epoch 6737, Train Loss: 626.3483, Val Loss: 194.2138\n",
            "Epoch 6738, Train Loss: 603.9042, Val Loss: 192.1964\n",
            "Epoch 6739, Train Loss: 597.8040, Val Loss: 186.4076\n",
            "Epoch 6740, Train Loss: 627.1460, Val Loss: 180.3908\n",
            "Epoch 6741, Train Loss: 605.4177, Val Loss: 189.5285\n",
            "Epoch 6742, Train Loss: 606.2503, Val Loss: 214.0588\n",
            "Epoch 6743, Train Loss: 607.1210, Val Loss: 212.0719\n",
            "Epoch 6744, Train Loss: 596.8425, Val Loss: 190.8100\n",
            "Epoch 6745, Train Loss: 580.4494, Val Loss: 232.3284\n",
            "Epoch 6746, Train Loss: 615.4673, Val Loss: 243.4120\n",
            "Epoch 6747, Train Loss: 621.3933, Val Loss: 259.7573\n",
            "Epoch 6748, Train Loss: 616.5693, Val Loss: 219.4080\n",
            "Epoch 6749, Train Loss: 561.9905, Val Loss: 236.1795\n",
            "Epoch 6750, Train Loss: 614.0179, Val Loss: 174.8742\n",
            "Epoch 6751, Train Loss: 628.5437, Val Loss: 174.3027\n",
            "Epoch 6752, Train Loss: 623.1596, Val Loss: 177.4436\n",
            "Epoch 6753, Train Loss: 625.0088, Val Loss: 210.7029\n",
            "Epoch 6754, Train Loss: 560.1834, Val Loss: 182.6620\n",
            "Epoch 6755, Train Loss: 615.4151, Val Loss: 199.9041\n",
            "Epoch 6756, Train Loss: 639.8764, Val Loss: 233.6006\n",
            "Epoch 6757, Train Loss: 598.0078, Val Loss: 226.1328\n",
            "Epoch 6758, Train Loss: 595.5877, Val Loss: 212.3040\n",
            "Epoch 6759, Train Loss: 592.0234, Val Loss: 224.3113\n",
            "Epoch 6760, Train Loss: 628.9465, Val Loss: 216.3141\n",
            "Epoch 6761, Train Loss: 615.2575, Val Loss: 175.1369\n",
            "Epoch 6762, Train Loss: 620.3768, Val Loss: 169.7187\n",
            "Epoch 6763, Train Loss: 623.1563, Val Loss: 172.4650\n",
            "Epoch 6764, Train Loss: 579.6696, Val Loss: 206.0709\n",
            "Epoch 6765, Train Loss: 616.3590, Val Loss: 264.4647\n",
            "Epoch 6766, Train Loss: 598.9905, Val Loss: 203.3038\n",
            "Epoch 6767, Train Loss: 626.0858, Val Loss: 173.7133\n",
            "Epoch 6768, Train Loss: 596.1829, Val Loss: 183.3375\n",
            "Epoch 6769, Train Loss: 570.3818, Val Loss: 271.3624\n",
            "Epoch 6770, Train Loss: 558.9475, Val Loss: 484.9018\n",
            "Epoch 6771, Train Loss: 602.5711, Val Loss: 375.7809\n",
            "Epoch 6772, Train Loss: 622.9843, Val Loss: 218.1539\n",
            "Epoch 6773, Train Loss: 599.6349, Val Loss: 208.0306\n",
            "Epoch 6774, Train Loss: 549.7454, Val Loss: 275.5820\n",
            "Epoch 6775, Train Loss: 599.0750, Val Loss: 301.3639\n",
            "Epoch 6776, Train Loss: 624.9473, Val Loss: 350.4446\n",
            "Epoch 6777, Train Loss: 611.4924, Val Loss: 268.5976\n",
            "Epoch 6778, Train Loss: 612.7661, Val Loss: 211.2972\n",
            "Epoch 6779, Train Loss: 616.4411, Val Loss: 189.9023\n",
            "Epoch 6780, Train Loss: 594.0287, Val Loss: 192.1857\n",
            "Epoch 6781, Train Loss: 600.6008, Val Loss: 181.5508\n",
            "Epoch 6782, Train Loss: 619.3191, Val Loss: 179.5617\n",
            "Epoch 6783, Train Loss: 540.0375, Val Loss: 181.5667\n",
            "Epoch 6784, Train Loss: 588.7165, Val Loss: 190.2643\n",
            "Epoch 6785, Train Loss: 631.6964, Val Loss: 182.2682\n",
            "Epoch 6786, Train Loss: 616.4677, Val Loss: 171.5487\n",
            "Epoch 6787, Train Loss: 620.0634, Val Loss: 172.1433\n",
            "Epoch 6788, Train Loss: 608.8818, Val Loss: 175.3482\n",
            "Epoch 6789, Train Loss: 622.3307, Val Loss: 172.9987\n",
            "Epoch 6790, Train Loss: 620.7253, Val Loss: 174.0132\n",
            "Epoch 6791, Train Loss: 618.7680, Val Loss: 174.8745\n",
            "Epoch 6792, Train Loss: 624.5582, Val Loss: 177.2865\n",
            "Epoch 6793, Train Loss: 620.2964, Val Loss: 191.2994\n",
            "Epoch 6794, Train Loss: 583.6670, Val Loss: 203.7697\n",
            "Epoch 6795, Train Loss: 625.9724, Val Loss: 182.7189\n",
            "Epoch 6796, Train Loss: 606.1857, Val Loss: 172.8455\n",
            "Epoch 6797, Train Loss: 618.2269, Val Loss: 172.2463\n",
            "Epoch 6798, Train Loss: 589.3305, Val Loss: 169.1809\n",
            "Epoch 6799, Train Loss: 622.2525, Val Loss: 177.0545\n",
            "Epoch 6800, Train Loss: 588.6320, Val Loss: 303.2311\n",
            "Epoch 6801, Train Loss: 616.5467, Val Loss: 266.0380\n",
            "Epoch 6802, Train Loss: 616.4580, Val Loss: 218.8218\n",
            "Epoch 6803, Train Loss: 583.6577, Val Loss: 222.3117\n",
            "Epoch 6804, Train Loss: 601.4782, Val Loss: 193.4123\n",
            "Epoch 6805, Train Loss: 590.6180, Val Loss: 195.9133\n",
            "Epoch 6806, Train Loss: 605.3648, Val Loss: 235.8125\n",
            "Epoch 6807, Train Loss: 616.9219, Val Loss: 238.6537\n",
            "Epoch 6808, Train Loss: 620.8219, Val Loss: 200.2959\n",
            "Epoch 6809, Train Loss: 604.3605, Val Loss: 191.2667\n",
            "Epoch 6810, Train Loss: 601.4235, Val Loss: 189.4417\n",
            "Epoch 6811, Train Loss: 595.7371, Val Loss: 202.0080\n",
            "Epoch 6812, Train Loss: 616.3279, Val Loss: 177.5368\n",
            "Epoch 6813, Train Loss: 568.4853, Val Loss: 176.1664\n",
            "Epoch 6814, Train Loss: 610.5649, Val Loss: 177.1950\n",
            "Epoch 6815, Train Loss: 623.2312, Val Loss: 176.8913\n",
            "Epoch 6816, Train Loss: 597.6206, Val Loss: 184.6216\n",
            "Epoch 6817, Train Loss: 611.1261, Val Loss: 246.9450\n",
            "Epoch 6818, Train Loss: 594.0154, Val Loss: 313.1806\n",
            "Epoch 6819, Train Loss: 610.0917, Val Loss: 311.8150\n",
            "Epoch 6820, Train Loss: 563.8661, Val Loss: 225.3342\n",
            "Epoch 6821, Train Loss: 613.7237, Val Loss: 184.6717\n",
            "Epoch 6822, Train Loss: 614.4294, Val Loss: 198.5458\n",
            "Epoch 6823, Train Loss: 618.5636, Val Loss: 210.5887\n",
            "Epoch 6824, Train Loss: 616.4937, Val Loss: 209.4523\n",
            "Epoch 6825, Train Loss: 612.8760, Val Loss: 193.4040\n",
            "Epoch 6826, Train Loss: 589.7273, Val Loss: 184.4989\n",
            "Epoch 6827, Train Loss: 601.9069, Val Loss: 220.6529\n",
            "Epoch 6828, Train Loss: 610.7637, Val Loss: 242.0773\n",
            "Epoch 6829, Train Loss: 575.4969, Val Loss: 236.4665\n",
            "Epoch 6830, Train Loss: 624.2677, Val Loss: 189.4825\n",
            "Epoch 6831, Train Loss: 623.7258, Val Loss: 193.0528\n",
            "Epoch 6832, Train Loss: 561.0203, Val Loss: 195.1460\n",
            "Epoch 6833, Train Loss: 592.6217, Val Loss: 179.0702\n",
            "Epoch 6834, Train Loss: 617.4300, Val Loss: 192.0895\n",
            "Epoch 6835, Train Loss: 624.6530, Val Loss: 197.3558\n",
            "Epoch 6836, Train Loss: 626.5612, Val Loss: 202.0119\n",
            "Epoch 6837, Train Loss: 628.3944, Val Loss: 188.7941\n",
            "Epoch 6838, Train Loss: 611.3125, Val Loss: 235.3589\n",
            "Epoch 6839, Train Loss: 625.8349, Val Loss: 250.1701\n",
            "Epoch 6840, Train Loss: 621.7183, Val Loss: 228.6843\n",
            "Epoch 6841, Train Loss: 627.6018, Val Loss: 209.1080\n",
            "Epoch 6842, Train Loss: 626.2007, Val Loss: 179.4201\n",
            "Epoch 6843, Train Loss: 612.8264, Val Loss: 172.9128\n",
            "Epoch 6844, Train Loss: 624.7051, Val Loss: 171.9308\n",
            "Epoch 6845, Train Loss: 625.0490, Val Loss: 182.6407\n",
            "Epoch 6846, Train Loss: 614.9105, Val Loss: 182.6300\n",
            "Epoch 6847, Train Loss: 628.3310, Val Loss: 215.8638\n",
            "Epoch 6848, Train Loss: 566.3909, Val Loss: 302.6810\n",
            "Epoch 6849, Train Loss: 626.5896, Val Loss: 336.4164\n",
            "Epoch 6850, Train Loss: 586.3467, Val Loss: 324.7704\n",
            "Epoch 6851, Train Loss: 617.6546, Val Loss: 256.5466\n",
            "Epoch 6852, Train Loss: 609.0290, Val Loss: 181.6869\n",
            "Epoch 6853, Train Loss: 611.0957, Val Loss: 178.2509\n",
            "Epoch 6854, Train Loss: 609.0706, Val Loss: 176.7717\n",
            "Epoch 6855, Train Loss: 607.0685, Val Loss: 221.6440\n",
            "Epoch 6856, Train Loss: 594.0745, Val Loss: 245.4477\n",
            "Epoch 6857, Train Loss: 614.4392, Val Loss: 202.8694\n",
            "Epoch 6858, Train Loss: 632.2496, Val Loss: 186.5874\n",
            "Epoch 6859, Train Loss: 596.7155, Val Loss: 190.8132\n",
            "Epoch 6860, Train Loss: 621.1226, Val Loss: 215.5325\n",
            "Epoch 6861, Train Loss: 576.3768, Val Loss: 259.1796\n",
            "Epoch 6862, Train Loss: 559.7881, Val Loss: 255.1684\n",
            "Epoch 6863, Train Loss: 550.7948, Val Loss: 249.2234\n",
            "Epoch 6864, Train Loss: 629.7761, Val Loss: 221.5952\n",
            "Epoch 6865, Train Loss: 622.6826, Val Loss: 276.5360\n",
            "Epoch 6866, Train Loss: 603.4148, Val Loss: 301.3409\n",
            "Epoch 6867, Train Loss: 597.5070, Val Loss: 258.3890\n",
            "Epoch 6868, Train Loss: 620.1089, Val Loss: 193.2512\n",
            "Epoch 6869, Train Loss: 626.4296, Val Loss: 176.6911\n",
            "Epoch 6870, Train Loss: 600.2647, Val Loss: 175.4142\n",
            "Epoch 6871, Train Loss: 619.3604, Val Loss: 178.7228\n",
            "Epoch 6872, Train Loss: 625.3666, Val Loss: 178.9874\n",
            "Epoch 6873, Train Loss: 612.0763, Val Loss: 182.8917\n",
            "Epoch 6874, Train Loss: 597.3158, Val Loss: 184.1477\n",
            "Epoch 6875, Train Loss: 625.6448, Val Loss: 192.7413\n",
            "Epoch 6876, Train Loss: 579.5482, Val Loss: 196.8690\n",
            "Epoch 6877, Train Loss: 595.1148, Val Loss: 210.2389\n",
            "Epoch 6878, Train Loss: 563.8188, Val Loss: 236.8453\n",
            "Epoch 6879, Train Loss: 610.4585, Val Loss: 283.0425\n",
            "Epoch 6880, Train Loss: 609.6380, Val Loss: 265.9218\n",
            "Epoch 6881, Train Loss: 611.2884, Val Loss: 226.4885\n",
            "Epoch 6882, Train Loss: 615.9303, Val Loss: 195.0711\n",
            "Epoch 6883, Train Loss: 603.8670, Val Loss: 185.3380\n",
            "Epoch 6884, Train Loss: 586.2284, Val Loss: 193.0090\n",
            "Epoch 6885, Train Loss: 608.4568, Val Loss: 184.4167\n",
            "Epoch 6886, Train Loss: 625.9779, Val Loss: 195.6266\n",
            "Epoch 6887, Train Loss: 619.8750, Val Loss: 202.2669\n",
            "Epoch 6888, Train Loss: 625.9154, Val Loss: 181.9317\n",
            "Epoch 6889, Train Loss: 597.5846, Val Loss: 191.1957\n",
            "Epoch 6890, Train Loss: 621.6072, Val Loss: 222.3274\n",
            "Epoch 6891, Train Loss: 611.6003, Val Loss: 226.5484\n",
            "Epoch 6892, Train Loss: 619.0878, Val Loss: 238.7290\n",
            "Epoch 6893, Train Loss: 580.7350, Val Loss: 298.8747\n",
            "Epoch 6894, Train Loss: 606.2900, Val Loss: 315.0625\n",
            "Epoch 6895, Train Loss: 622.4253, Val Loss: 268.5952\n",
            "Epoch 6896, Train Loss: 564.6407, Val Loss: 225.2000\n",
            "Epoch 6897, Train Loss: 621.5273, Val Loss: 183.3291\n",
            "Epoch 6898, Train Loss: 627.7569, Val Loss: 177.9584\n",
            "Epoch 6899, Train Loss: 615.0634, Val Loss: 186.5719\n",
            "Epoch 6900, Train Loss: 615.0308, Val Loss: 180.1706\n",
            "Epoch 6901, Train Loss: 610.6320, Val Loss: 170.9536\n",
            "Epoch 6902, Train Loss: 621.6172, Val Loss: 171.5732\n",
            "Epoch 6903, Train Loss: 620.0721, Val Loss: 181.1771\n",
            "Epoch 6904, Train Loss: 584.2497, Val Loss: 215.0350\n",
            "Epoch 6905, Train Loss: 623.5874, Val Loss: 221.0423\n",
            "Epoch 6906, Train Loss: 587.6655, Val Loss: 200.2535\n",
            "Epoch 6907, Train Loss: 594.1308, Val Loss: 207.8683\n",
            "Epoch 6908, Train Loss: 606.2481, Val Loss: 225.8558\n",
            "Epoch 6909, Train Loss: 616.5061, Val Loss: 241.4575\n",
            "Epoch 6910, Train Loss: 619.8300, Val Loss: 262.0365\n",
            "Epoch 6911, Train Loss: 572.3813, Val Loss: 291.7877\n",
            "Epoch 6912, Train Loss: 599.8902, Val Loss: 309.7649\n",
            "Epoch 6913, Train Loss: 610.4389, Val Loss: 233.7153\n",
            "Epoch 6914, Train Loss: 625.5334, Val Loss: 198.0990\n",
            "Epoch 6915, Train Loss: 609.1741, Val Loss: 194.1598\n",
            "Epoch 6916, Train Loss: 601.2177, Val Loss: 177.0892\n",
            "Epoch 6917, Train Loss: 623.0356, Val Loss: 176.5849\n",
            "Epoch 6918, Train Loss: 584.9427, Val Loss: 176.1540\n",
            "Epoch 6919, Train Loss: 601.9238, Val Loss: 174.4768\n",
            "Epoch 6920, Train Loss: 606.5861, Val Loss: 172.2294\n",
            "Epoch 6921, Train Loss: 584.2646, Val Loss: 219.2470\n",
            "Epoch 6922, Train Loss: 599.6196, Val Loss: 285.0862\n",
            "Epoch 6923, Train Loss: 612.8616, Val Loss: 291.7075\n",
            "Epoch 6924, Train Loss: 610.9978, Val Loss: 279.6677\n",
            "Epoch 6925, Train Loss: 615.5011, Val Loss: 214.4270\n",
            "Epoch 6926, Train Loss: 625.8721, Val Loss: 192.8316\n",
            "Epoch 6927, Train Loss: 619.3489, Val Loss: 201.6460\n",
            "Epoch 6928, Train Loss: 600.8531, Val Loss: 215.2281\n",
            "Epoch 6929, Train Loss: 618.4783, Val Loss: 213.9818\n",
            "Epoch 6930, Train Loss: 611.7757, Val Loss: 198.3716\n",
            "Epoch 6931, Train Loss: 617.9790, Val Loss: 181.1349\n",
            "Epoch 6932, Train Loss: 598.5024, Val Loss: 200.2537\n",
            "Epoch 6933, Train Loss: 619.3683, Val Loss: 233.0109\n",
            "Epoch 6934, Train Loss: 623.6475, Val Loss: 272.0344\n",
            "Epoch 6935, Train Loss: 622.6697, Val Loss: 224.4866\n",
            "Epoch 6936, Train Loss: 608.0480, Val Loss: 190.9493\n",
            "Epoch 6937, Train Loss: 608.0045, Val Loss: 174.4837\n",
            "Epoch 6938, Train Loss: 624.8817, Val Loss: 173.8960\n",
            "Epoch 6939, Train Loss: 615.5787, Val Loss: 180.5615\n",
            "Epoch 6940, Train Loss: 588.1973, Val Loss: 224.5158\n",
            "Epoch 6941, Train Loss: 622.1057, Val Loss: 251.6562\n",
            "Epoch 6942, Train Loss: 588.0006, Val Loss: 210.1330\n",
            "Epoch 6943, Train Loss: 556.5031, Val Loss: 200.1521\n",
            "Epoch 6944, Train Loss: 606.8643, Val Loss: 255.8952\n",
            "Epoch 6945, Train Loss: 602.9752, Val Loss: 220.2330\n",
            "Epoch 6946, Train Loss: 639.5028, Val Loss: 172.5755\n",
            "Epoch 6947, Train Loss: 559.1721, Val Loss: 238.1674\n",
            "Epoch 6948, Train Loss: 622.8557, Val Loss: 292.5861\n",
            "Epoch 6949, Train Loss: 600.2925, Val Loss: 182.6082\n",
            "Epoch 6950, Train Loss: 613.8429, Val Loss: 180.2435\n",
            "Epoch 6951, Train Loss: 599.1161, Val Loss: 203.3196\n",
            "Epoch 6952, Train Loss: 624.8284, Val Loss: 229.0753\n",
            "Epoch 6953, Train Loss: 619.6920, Val Loss: 213.4275\n",
            "Epoch 6954, Train Loss: 616.8357, Val Loss: 206.4111\n",
            "Epoch 6955, Train Loss: 619.5980, Val Loss: 187.2006\n",
            "Epoch 6956, Train Loss: 623.1543, Val Loss: 187.6014\n",
            "Epoch 6957, Train Loss: 588.7636, Val Loss: 232.9391\n",
            "Epoch 6958, Train Loss: 614.3503, Val Loss: 284.6244\n",
            "Epoch 6959, Train Loss: 618.1512, Val Loss: 268.3190\n",
            "Epoch 6960, Train Loss: 624.1201, Val Loss: 200.8844\n",
            "Epoch 6961, Train Loss: 606.5614, Val Loss: 192.5020\n",
            "Epoch 6962, Train Loss: 627.9651, Val Loss: 180.7685\n",
            "Epoch 6963, Train Loss: 619.5715, Val Loss: 180.5613\n",
            "Epoch 6964, Train Loss: 609.5844, Val Loss: 176.1927\n",
            "Epoch 6965, Train Loss: 619.8156, Val Loss: 173.4847\n",
            "Epoch 6966, Train Loss: 606.6616, Val Loss: 172.9747\n",
            "Epoch 6967, Train Loss: 606.7454, Val Loss: 172.9981\n",
            "Epoch 6968, Train Loss: 615.8548, Val Loss: 174.7478\n",
            "Epoch 6969, Train Loss: 603.9489, Val Loss: 184.4360\n",
            "Epoch 6970, Train Loss: 607.0880, Val Loss: 199.1101\n",
            "Epoch 6971, Train Loss: 623.7874, Val Loss: 180.2894\n",
            "Epoch 6972, Train Loss: 620.1031, Val Loss: 173.3075\n",
            "Epoch 6973, Train Loss: 627.3108, Val Loss: 176.5477\n",
            "Epoch 6974, Train Loss: 626.0547, Val Loss: 187.2928\n",
            "Epoch 6975, Train Loss: 601.4636, Val Loss: 193.2671\n",
            "Epoch 6976, Train Loss: 623.1311, Val Loss: 191.4383\n",
            "Epoch 6977, Train Loss: 594.4502, Val Loss: 228.4249\n",
            "Epoch 6978, Train Loss: 618.7560, Val Loss: 351.4911\n",
            "Epoch 6979, Train Loss: 580.8442, Val Loss: 356.4225\n",
            "Epoch 6980, Train Loss: 623.1340, Val Loss: 316.7822\n",
            "Epoch 6981, Train Loss: 601.5183, Val Loss: 227.0860\n",
            "Epoch 6982, Train Loss: 620.2317, Val Loss: 198.3833\n",
            "Epoch 6983, Train Loss: 626.4067, Val Loss: 299.5529\n",
            "Epoch 6984, Train Loss: 635.1025, Val Loss: 323.0512\n",
            "Epoch 6985, Train Loss: 621.8693, Val Loss: 196.7476\n",
            "Epoch 6986, Train Loss: 620.4832, Val Loss: 222.5083\n",
            "Epoch 6987, Train Loss: 623.3959, Val Loss: 203.3916\n",
            "Epoch 6988, Train Loss: 595.7289, Val Loss: 180.2703\n",
            "Epoch 6989, Train Loss: 619.0589, Val Loss: 183.0300\n",
            "Epoch 6990, Train Loss: 596.4965, Val Loss: 195.1356\n",
            "Epoch 6991, Train Loss: 625.9638, Val Loss: 244.5200\n",
            "Epoch 6992, Train Loss: 586.8621, Val Loss: 351.3823\n",
            "Epoch 6993, Train Loss: 618.8738, Val Loss: 430.5566\n",
            "Epoch 6994, Train Loss: 623.7354, Val Loss: 278.3818\n",
            "Epoch 6995, Train Loss: 614.8476, Val Loss: 172.8753\n",
            "Epoch 6996, Train Loss: 619.5292, Val Loss: 175.2738\n",
            "Epoch 6997, Train Loss: 592.2128, Val Loss: 181.1197\n",
            "Epoch 6998, Train Loss: 588.0177, Val Loss: 271.9176\n",
            "Epoch 6999, Train Loss: 634.7023, Val Loss: 308.6047\n",
            "Epoch 7000, Train Loss: 602.2274, Val Loss: 192.1468\n",
            "Epoch 7001, Train Loss: 598.3483, Val Loss: 183.8672\n",
            "Epoch 7002, Train Loss: 616.9827, Val Loss: 196.4360\n",
            "Epoch 7003, Train Loss: 614.2345, Val Loss: 179.5086\n",
            "Epoch 7004, Train Loss: 559.8766, Val Loss: 255.0781\n",
            "Epoch 7005, Train Loss: 613.2190, Val Loss: 298.3009\n",
            "Epoch 7006, Train Loss: 571.7378, Val Loss: 199.8675\n",
            "Epoch 7007, Train Loss: 590.8895, Val Loss: 185.9618\n",
            "Epoch 7008, Train Loss: 612.5069, Val Loss: 213.2187\n",
            "Epoch 7009, Train Loss: 597.1684, Val Loss: 270.2363\n",
            "Epoch 7010, Train Loss: 607.1699, Val Loss: 295.6181\n",
            "Epoch 7011, Train Loss: 624.2012, Val Loss: 246.5843\n",
            "Epoch 7012, Train Loss: 608.2943, Val Loss: 176.1221\n",
            "Epoch 7013, Train Loss: 609.4813, Val Loss: 174.8850\n",
            "Epoch 7014, Train Loss: 622.7752, Val Loss: 195.9713\n",
            "Epoch 7015, Train Loss: 617.9596, Val Loss: 216.7856\n",
            "Epoch 7016, Train Loss: 624.4879, Val Loss: 223.1144\n",
            "Epoch 7017, Train Loss: 602.3228, Val Loss: 188.8943\n",
            "Epoch 7018, Train Loss: 618.9782, Val Loss: 174.1039\n",
            "Epoch 7019, Train Loss: 561.9040, Val Loss: 185.9228\n",
            "Epoch 7020, Train Loss: 628.2667, Val Loss: 201.6502\n",
            "Epoch 7021, Train Loss: 596.6232, Val Loss: 189.1502\n",
            "Epoch 7022, Train Loss: 627.4461, Val Loss: 181.1702\n",
            "Epoch 7023, Train Loss: 615.6967, Val Loss: 182.1135\n",
            "Epoch 7024, Train Loss: 608.9536, Val Loss: 179.9191\n",
            "Epoch 7025, Train Loss: 597.2944, Val Loss: 178.2016\n",
            "Epoch 7026, Train Loss: 621.0815, Val Loss: 184.1150\n",
            "Epoch 7027, Train Loss: 612.8860, Val Loss: 186.6570\n",
            "Epoch 7028, Train Loss: 586.7094, Val Loss: 183.7168\n",
            "Epoch 7029, Train Loss: 591.9606, Val Loss: 185.8741\n",
            "Epoch 7030, Train Loss: 622.1501, Val Loss: 234.0636\n",
            "Epoch 7031, Train Loss: 595.5368, Val Loss: 264.7397\n",
            "Epoch 7032, Train Loss: 609.5957, Val Loss: 338.7887\n",
            "Epoch 7033, Train Loss: 580.4482, Val Loss: 264.8574\n",
            "Epoch 7034, Train Loss: 619.2344, Val Loss: 170.2738\n",
            "Epoch 7035, Train Loss: 624.3657, Val Loss: 172.3876\n",
            "Epoch 7036, Train Loss: 618.2879, Val Loss: 176.2532\n",
            "Epoch 7037, Train Loss: 618.4006, Val Loss: 173.4900\n",
            "Epoch 7038, Train Loss: 601.7967, Val Loss: 174.9989\n",
            "Epoch 7039, Train Loss: 597.0768, Val Loss: 182.0468\n",
            "Epoch 7040, Train Loss: 597.9158, Val Loss: 256.0550\n",
            "Epoch 7041, Train Loss: 621.8363, Val Loss: 325.0008\n",
            "Epoch 7042, Train Loss: 561.6562, Val Loss: 302.5878\n",
            "Epoch 7043, Train Loss: 614.4613, Val Loss: 254.3477\n",
            "Epoch 7044, Train Loss: 608.0853, Val Loss: 215.5715\n",
            "Epoch 7045, Train Loss: 562.5520, Val Loss: 226.9975\n",
            "Epoch 7046, Train Loss: 612.7724, Val Loss: 231.5687\n",
            "Epoch 7047, Train Loss: 622.7571, Val Loss: 248.0178\n",
            "Epoch 7048, Train Loss: 574.7900, Val Loss: 256.0283\n",
            "Epoch 7049, Train Loss: 624.4007, Val Loss: 268.7761\n",
            "Epoch 7050, Train Loss: 584.4643, Val Loss: 210.8479\n",
            "Epoch 7051, Train Loss: 628.3480, Val Loss: 183.5007\n",
            "Epoch 7052, Train Loss: 627.5789, Val Loss: 175.7290\n",
            "Epoch 7053, Train Loss: 624.5954, Val Loss: 171.9673\n",
            "Epoch 7054, Train Loss: 604.3981, Val Loss: 175.1913\n",
            "Epoch 7055, Train Loss: 615.5705, Val Loss: 212.9570\n",
            "Epoch 7056, Train Loss: 622.8708, Val Loss: 194.9734\n",
            "Epoch 7057, Train Loss: 597.4930, Val Loss: 188.5320\n",
            "Epoch 7058, Train Loss: 570.2852, Val Loss: 234.1260\n",
            "Epoch 7059, Train Loss: 582.3543, Val Loss: 340.7580\n",
            "Epoch 7060, Train Loss: 617.8282, Val Loss: 320.4375\n",
            "Epoch 7061, Train Loss: 564.2763, Val Loss: 320.7660\n",
            "Epoch 7062, Train Loss: 612.2451, Val Loss: 181.3867\n",
            "Epoch 7063, Train Loss: 609.2235, Val Loss: 174.8462\n",
            "Epoch 7064, Train Loss: 626.6969, Val Loss: 174.1927\n",
            "Epoch 7065, Train Loss: 591.2908, Val Loss: 170.8065\n",
            "Epoch 7066, Train Loss: 627.5233, Val Loss: 177.6021\n",
            "Epoch 7067, Train Loss: 609.2786, Val Loss: 208.2887\n",
            "Epoch 7068, Train Loss: 627.4017, Val Loss: 192.7583\n",
            "Epoch 7069, Train Loss: 623.7100, Val Loss: 175.5614\n",
            "Epoch 7070, Train Loss: 599.4908, Val Loss: 176.9226\n",
            "Epoch 7071, Train Loss: 582.7364, Val Loss: 176.4594\n",
            "Epoch 7072, Train Loss: 623.0396, Val Loss: 258.9667\n",
            "Epoch 7073, Train Loss: 602.0442, Val Loss: 297.6198\n",
            "Epoch 7074, Train Loss: 611.5770, Val Loss: 271.9341\n",
            "Epoch 7075, Train Loss: 614.6611, Val Loss: 247.9922\n",
            "Epoch 7076, Train Loss: 607.1592, Val Loss: 184.1277\n",
            "Epoch 7077, Train Loss: 603.1875, Val Loss: 171.5716\n",
            "Epoch 7078, Train Loss: 615.3321, Val Loss: 175.3012\n",
            "Epoch 7079, Train Loss: 620.7918, Val Loss: 197.2739\n",
            "Epoch 7080, Train Loss: 598.9895, Val Loss: 219.5776\n",
            "Epoch 7081, Train Loss: 592.3109, Val Loss: 237.5950\n",
            "Epoch 7082, Train Loss: 621.3106, Val Loss: 206.6688\n",
            "Epoch 7083, Train Loss: 591.6634, Val Loss: 184.4422\n",
            "Epoch 7084, Train Loss: 607.1162, Val Loss: 178.5767\n",
            "Epoch 7085, Train Loss: 623.4748, Val Loss: 175.1717\n",
            "Epoch 7086, Train Loss: 609.9323, Val Loss: 194.9080\n",
            "Epoch 7087, Train Loss: 565.5540, Val Loss: 298.4358\n",
            "Epoch 7088, Train Loss: 623.1993, Val Loss: 294.0180\n",
            "Epoch 7089, Train Loss: 604.3084, Val Loss: 247.1712\n",
            "Epoch 7090, Train Loss: 622.0451, Val Loss: 205.9888\n",
            "Epoch 7091, Train Loss: 623.7631, Val Loss: 212.1945\n",
            "Epoch 7092, Train Loss: 599.9234, Val Loss: 266.0102\n",
            "Epoch 7093, Train Loss: 582.0388, Val Loss: 275.9909\n",
            "Epoch 7094, Train Loss: 606.0545, Val Loss: 196.0894\n",
            "Epoch 7095, Train Loss: 594.7194, Val Loss: 176.4249\n",
            "Epoch 7096, Train Loss: 621.0454, Val Loss: 169.8271\n",
            "Epoch 7097, Train Loss: 593.7206, Val Loss: 196.5871\n",
            "Epoch 7098, Train Loss: 608.7344, Val Loss: 242.2679\n",
            "Epoch 7099, Train Loss: 632.1974, Val Loss: 197.0977\n",
            "Epoch 7100, Train Loss: 589.2189, Val Loss: 179.8954\n",
            "Epoch 7101, Train Loss: 601.6345, Val Loss: 175.0026\n",
            "Epoch 7102, Train Loss: 623.2004, Val Loss: 177.7707\n",
            "Epoch 7103, Train Loss: 615.3866, Val Loss: 188.4063\n",
            "Epoch 7104, Train Loss: 620.8701, Val Loss: 215.5152\n",
            "Epoch 7105, Train Loss: 628.0285, Val Loss: 191.7509\n",
            "Epoch 7106, Train Loss: 621.8831, Val Loss: 174.1963\n",
            "Epoch 7107, Train Loss: 618.0980, Val Loss: 170.6531\n",
            "Epoch 7108, Train Loss: 571.4871, Val Loss: 183.3032\n",
            "Epoch 7109, Train Loss: 627.2208, Val Loss: 214.7906\n",
            "Epoch 7110, Train Loss: 633.8172, Val Loss: 193.7745\n",
            "Epoch 7111, Train Loss: 618.6371, Val Loss: 171.4837\n",
            "Epoch 7112, Train Loss: 604.1845, Val Loss: 170.3632\n",
            "Epoch 7113, Train Loss: 622.2991, Val Loss: 179.3544\n",
            "Epoch 7114, Train Loss: 614.8838, Val Loss: 235.9950\n",
            "Epoch 7115, Train Loss: 613.4390, Val Loss: 248.2761\n",
            "Epoch 7116, Train Loss: 613.6495, Val Loss: 208.6596\n",
            "Epoch 7117, Train Loss: 625.5240, Val Loss: 187.1509\n",
            "Epoch 7118, Train Loss: 619.0549, Val Loss: 177.2108\n",
            "Epoch 7119, Train Loss: 616.2623, Val Loss: 176.7213\n",
            "Epoch 7120, Train Loss: 612.8597, Val Loss: 186.3735\n",
            "Epoch 7121, Train Loss: 627.7793, Val Loss: 226.4851\n",
            "Epoch 7122, Train Loss: 604.6014, Val Loss: 218.6261\n",
            "Epoch 7123, Train Loss: 632.7144, Val Loss: 198.1221\n",
            "Epoch 7124, Train Loss: 608.0089, Val Loss: 194.5522\n",
            "Epoch 7125, Train Loss: 617.2268, Val Loss: 185.9041\n",
            "Epoch 7126, Train Loss: 607.2100, Val Loss: 205.8857\n",
            "Epoch 7127, Train Loss: 605.3424, Val Loss: 240.7461\n",
            "Epoch 7128, Train Loss: 601.0179, Val Loss: 241.8465\n",
            "Epoch 7129, Train Loss: 618.8242, Val Loss: 243.3951\n",
            "Epoch 7130, Train Loss: 573.4946, Val Loss: 219.0473\n",
            "Epoch 7131, Train Loss: 618.7680, Val Loss: 219.8664\n",
            "Epoch 7132, Train Loss: 621.5975, Val Loss: 213.5248\n",
            "Epoch 7133, Train Loss: 625.3167, Val Loss: 230.7194\n",
            "Epoch 7134, Train Loss: 622.3788, Val Loss: 231.4037\n",
            "Epoch 7135, Train Loss: 582.1322, Val Loss: 190.6605\n",
            "Epoch 7136, Train Loss: 615.9716, Val Loss: 182.9355\n",
            "Epoch 7137, Train Loss: 612.9520, Val Loss: 176.4912\n",
            "Epoch 7138, Train Loss: 572.4676, Val Loss: 179.8333\n",
            "Epoch 7139, Train Loss: 627.8396, Val Loss: 199.6957\n",
            "Epoch 7140, Train Loss: 620.9211, Val Loss: 212.6375\n",
            "Epoch 7141, Train Loss: 587.5295, Val Loss: 214.5949\n",
            "Epoch 7142, Train Loss: 597.0362, Val Loss: 231.3711\n",
            "Epoch 7143, Train Loss: 591.3724, Val Loss: 216.4275\n",
            "Epoch 7144, Train Loss: 619.4208, Val Loss: 228.6551\n",
            "Epoch 7145, Train Loss: 607.0897, Val Loss: 348.6536\n",
            "Epoch 7146, Train Loss: 620.8577, Val Loss: 368.4526\n",
            "Epoch 7147, Train Loss: 624.8820, Val Loss: 243.1507\n",
            "Epoch 7148, Train Loss: 566.8019, Val Loss: 225.1730\n",
            "Epoch 7149, Train Loss: 609.6991, Val Loss: 277.5089\n",
            "Epoch 7150, Train Loss: 621.7098, Val Loss: 202.5503\n",
            "Epoch 7151, Train Loss: 617.1863, Val Loss: 203.3231\n",
            "Epoch 7152, Train Loss: 625.4606, Val Loss: 203.7886\n",
            "Epoch 7153, Train Loss: 623.8703, Val Loss: 186.4584\n",
            "Epoch 7154, Train Loss: 597.6057, Val Loss: 176.7132\n",
            "Epoch 7155, Train Loss: 600.4825, Val Loss: 194.6414\n",
            "Epoch 7156, Train Loss: 591.8326, Val Loss: 239.4726\n",
            "Epoch 7157, Train Loss: 566.0749, Val Loss: 178.8764\n",
            "Epoch 7158, Train Loss: 626.6928, Val Loss: 171.7621\n",
            "Epoch 7159, Train Loss: 619.7358, Val Loss: 171.8403\n",
            "Epoch 7160, Train Loss: 624.3059, Val Loss: 178.3575\n",
            "Epoch 7161, Train Loss: 622.5632, Val Loss: 180.8397\n",
            "Epoch 7162, Train Loss: 581.2064, Val Loss: 183.3818\n",
            "Epoch 7163, Train Loss: 596.8339, Val Loss: 184.7600\n",
            "Epoch 7164, Train Loss: 614.8232, Val Loss: 180.7475\n",
            "Epoch 7165, Train Loss: 573.2079, Val Loss: 199.0580\n",
            "Epoch 7166, Train Loss: 622.8220, Val Loss: 222.7100\n",
            "Epoch 7167, Train Loss: 627.7089, Val Loss: 199.4146\n",
            "Epoch 7168, Train Loss: 624.9970, Val Loss: 181.3844\n",
            "Epoch 7169, Train Loss: 556.7468, Val Loss: 190.8845\n",
            "Epoch 7170, Train Loss: 593.4762, Val Loss: 263.6241\n",
            "Epoch 7171, Train Loss: 614.3923, Val Loss: 221.5048\n",
            "Epoch 7172, Train Loss: 621.4871, Val Loss: 177.1966\n",
            "Epoch 7173, Train Loss: 622.0120, Val Loss: 182.3755\n",
            "Epoch 7174, Train Loss: 611.4372, Val Loss: 175.3847\n",
            "Epoch 7175, Train Loss: 617.8131, Val Loss: 171.3873\n",
            "Epoch 7176, Train Loss: 619.9331, Val Loss: 182.4316\n",
            "Epoch 7177, Train Loss: 625.1574, Val Loss: 203.4451\n",
            "Epoch 7178, Train Loss: 609.2432, Val Loss: 187.1564\n",
            "Epoch 7179, Train Loss: 624.8737, Val Loss: 184.0083\n",
            "Epoch 7180, Train Loss: 563.5261, Val Loss: 176.0334\n",
            "Epoch 7181, Train Loss: 626.8993, Val Loss: 184.5141\n",
            "Epoch 7182, Train Loss: 589.4719, Val Loss: 412.4970\n",
            "Epoch 7183, Train Loss: 608.4313, Val Loss: 481.7593\n",
            "Epoch 7184, Train Loss: 616.1865, Val Loss: 391.0022\n",
            "Epoch 7185, Train Loss: 615.5275, Val Loss: 338.8134\n",
            "Epoch 7186, Train Loss: 602.9663, Val Loss: 259.0803\n",
            "Epoch 7187, Train Loss: 619.8048, Val Loss: 173.3105\n",
            "Epoch 7188, Train Loss: 606.8272, Val Loss: 182.0965\n",
            "Epoch 7189, Train Loss: 619.5581, Val Loss: 180.8941\n",
            "Epoch 7190, Train Loss: 620.5177, Val Loss: 222.8524\n",
            "Epoch 7191, Train Loss: 618.7004, Val Loss: 226.3376\n",
            "Epoch 7192, Train Loss: 620.4273, Val Loss: 200.3139\n",
            "Epoch 7193, Train Loss: 612.8594, Val Loss: 187.5740\n",
            "Epoch 7194, Train Loss: 625.7819, Val Loss: 177.7604\n",
            "Epoch 7195, Train Loss: 623.7509, Val Loss: 184.2691\n",
            "Epoch 7196, Train Loss: 566.9128, Val Loss: 216.7965\n",
            "Epoch 7197, Train Loss: 602.0872, Val Loss: 210.7253\n",
            "Epoch 7198, Train Loss: 623.4484, Val Loss: 198.2613\n",
            "Epoch 7199, Train Loss: 622.8290, Val Loss: 199.2447\n",
            "Epoch 7200, Train Loss: 615.1821, Val Loss: 214.5683\n",
            "Epoch 7201, Train Loss: 621.1777, Val Loss: 213.0580\n",
            "Epoch 7202, Train Loss: 578.6399, Val Loss: 203.0813\n",
            "Epoch 7203, Train Loss: 619.9308, Val Loss: 178.7229\n",
            "Epoch 7204, Train Loss: 613.9628, Val Loss: 170.3340\n",
            "Epoch 7205, Train Loss: 576.0269, Val Loss: 187.6916\n",
            "Epoch 7206, Train Loss: 627.7220, Val Loss: 199.2244\n",
            "Epoch 7207, Train Loss: 620.3204, Val Loss: 183.4654\n",
            "Epoch 7208, Train Loss: 598.4305, Val Loss: 181.8677\n",
            "Epoch 7209, Train Loss: 562.1249, Val Loss: 207.3238\n",
            "Epoch 7210, Train Loss: 625.4512, Val Loss: 269.1355\n",
            "Epoch 7211, Train Loss: 614.6560, Val Loss: 286.2510\n",
            "Epoch 7212, Train Loss: 573.4389, Val Loss: 249.0900\n",
            "Epoch 7213, Train Loss: 624.0810, Val Loss: 211.1793\n",
            "Epoch 7214, Train Loss: 620.0572, Val Loss: 214.8654\n",
            "Epoch 7215, Train Loss: 618.9245, Val Loss: 213.1182\n",
            "Epoch 7216, Train Loss: 580.1165, Val Loss: 219.6431\n",
            "Epoch 7217, Train Loss: 618.3293, Val Loss: 222.7517\n",
            "Epoch 7218, Train Loss: 612.5405, Val Loss: 214.6754\n",
            "Epoch 7219, Train Loss: 615.0970, Val Loss: 208.4427\n",
            "Epoch 7220, Train Loss: 615.8728, Val Loss: 196.1195\n",
            "Epoch 7221, Train Loss: 619.9427, Val Loss: 174.0999\n",
            "Epoch 7222, Train Loss: 614.8674, Val Loss: 176.9604\n",
            "Epoch 7223, Train Loss: 602.3800, Val Loss: 173.7726\n",
            "Epoch 7224, Train Loss: 583.1009, Val Loss: 196.6281\n",
            "Epoch 7225, Train Loss: 591.1612, Val Loss: 220.0196\n",
            "Epoch 7226, Train Loss: 623.1168, Val Loss: 200.9151\n",
            "Epoch 7227, Train Loss: 610.6615, Val Loss: 199.7459\n",
            "Epoch 7228, Train Loss: 630.1754, Val Loss: 203.1370\n",
            "Epoch 7229, Train Loss: 629.1398, Val Loss: 212.3440\n",
            "Epoch 7230, Train Loss: 607.7903, Val Loss: 191.2092\n",
            "Epoch 7231, Train Loss: 607.0980, Val Loss: 188.3389\n",
            "Epoch 7232, Train Loss: 615.9544, Val Loss: 191.8049\n",
            "Epoch 7233, Train Loss: 556.9074, Val Loss: 214.9347\n",
            "Epoch 7234, Train Loss: 609.7913, Val Loss: 247.7222\n",
            "Epoch 7235, Train Loss: 614.5047, Val Loss: 292.9615\n",
            "Epoch 7236, Train Loss: 551.7302, Val Loss: 275.7180\n",
            "Epoch 7237, Train Loss: 607.6166, Val Loss: 242.6310\n",
            "Epoch 7238, Train Loss: 609.6366, Val Loss: 208.0891\n",
            "Epoch 7239, Train Loss: 553.9922, Val Loss: 186.3231\n",
            "Epoch 7240, Train Loss: 622.0638, Val Loss: 182.0111\n",
            "Epoch 7241, Train Loss: 588.1743, Val Loss: 181.8609\n",
            "Epoch 7242, Train Loss: 630.2651, Val Loss: 206.8420\n",
            "Epoch 7243, Train Loss: 603.2111, Val Loss: 196.3077\n",
            "Epoch 7244, Train Loss: 608.8684, Val Loss: 191.2284\n",
            "Epoch 7245, Train Loss: 588.4779, Val Loss: 190.5433\n",
            "Epoch 7246, Train Loss: 632.1368, Val Loss: 187.5178\n",
            "Epoch 7247, Train Loss: 624.2865, Val Loss: 185.6784\n",
            "Epoch 7248, Train Loss: 605.4860, Val Loss: 191.6546\n",
            "Epoch 7249, Train Loss: 597.0302, Val Loss: 215.1500\n",
            "Epoch 7250, Train Loss: 623.1746, Val Loss: 209.1585\n",
            "Epoch 7251, Train Loss: 622.1093, Val Loss: 179.8836\n",
            "Epoch 7252, Train Loss: 593.7913, Val Loss: 185.8480\n",
            "Epoch 7253, Train Loss: 619.0692, Val Loss: 236.8451\n",
            "Epoch 7254, Train Loss: 578.1211, Val Loss: 344.3078\n",
            "Epoch 7255, Train Loss: 641.2721, Val Loss: 330.6285\n",
            "Epoch 7256, Train Loss: 587.1481, Val Loss: 261.2151\n",
            "Epoch 7257, Train Loss: 628.3365, Val Loss: 343.5966\n",
            "Epoch 7258, Train Loss: 624.5769, Val Loss: 198.4730\n",
            "Epoch 7259, Train Loss: 624.1665, Val Loss: 510.4835\n",
            "Epoch 7260, Train Loss: 602.3357, Val Loss: 485.1376\n",
            "Epoch 7261, Train Loss: 624.0310, Val Loss: 314.0497\n",
            "Epoch 7262, Train Loss: 622.4408, Val Loss: 198.5430\n",
            "Epoch 7263, Train Loss: 601.1982, Val Loss: 191.4007\n",
            "Epoch 7264, Train Loss: 610.5305, Val Loss: 194.0678\n",
            "Epoch 7265, Train Loss: 605.3526, Val Loss: 227.3005\n",
            "Epoch 7266, Train Loss: 589.1639, Val Loss: 293.7535\n",
            "Epoch 7267, Train Loss: 604.9455, Val Loss: 273.8918\n",
            "Epoch 7268, Train Loss: 639.3678, Val Loss: 277.9174\n",
            "Epoch 7269, Train Loss: 602.8232, Val Loss: 317.1511\n",
            "Epoch 7270, Train Loss: 624.6124, Val Loss: 312.8142\n",
            "Epoch 7271, Train Loss: 580.4223, Val Loss: 289.7342\n",
            "Epoch 7272, Train Loss: 602.6859, Val Loss: 212.5981\n",
            "Epoch 7273, Train Loss: 582.4570, Val Loss: 197.2532\n",
            "Epoch 7274, Train Loss: 619.8401, Val Loss: 202.7915\n",
            "Epoch 7275, Train Loss: 619.4312, Val Loss: 181.8347\n",
            "Epoch 7276, Train Loss: 607.3820, Val Loss: 169.3511\n",
            "Epoch 7277, Train Loss: 574.3765, Val Loss: 170.1179\n",
            "Epoch 7278, Train Loss: 607.5306, Val Loss: 183.3615\n",
            "Epoch 7279, Train Loss: 625.8271, Val Loss: 198.8044\n",
            "Epoch 7280, Train Loss: 628.5376, Val Loss: 202.4286\n",
            "Epoch 7281, Train Loss: 578.2875, Val Loss: 209.1020\n",
            "Epoch 7282, Train Loss: 604.3194, Val Loss: 203.4637\n",
            "Epoch 7283, Train Loss: 616.8679, Val Loss: 188.5492\n",
            "Epoch 7284, Train Loss: 590.9936, Val Loss: 180.4408\n",
            "Epoch 7285, Train Loss: 623.8037, Val Loss: 179.1374\n",
            "Epoch 7286, Train Loss: 573.0621, Val Loss: 202.1662\n",
            "Epoch 7287, Train Loss: 628.7921, Val Loss: 216.6355\n",
            "Epoch 7288, Train Loss: 609.8075, Val Loss: 197.3533\n",
            "Epoch 7289, Train Loss: 620.1558, Val Loss: 181.3520\n",
            "Epoch 7290, Train Loss: 620.7258, Val Loss: 178.4747\n",
            "Epoch 7291, Train Loss: 614.7066, Val Loss: 179.5188\n",
            "Epoch 7292, Train Loss: 624.8830, Val Loss: 176.5085\n",
            "Epoch 7293, Train Loss: 606.7280, Val Loss: 188.0428\n",
            "Epoch 7294, Train Loss: 620.5312, Val Loss: 234.7869\n",
            "Epoch 7295, Train Loss: 624.5980, Val Loss: 243.9570\n",
            "Epoch 7296, Train Loss: 623.4508, Val Loss: 229.9415\n",
            "Epoch 7297, Train Loss: 588.2506, Val Loss: 244.6225\n",
            "Epoch 7298, Train Loss: 627.4409, Val Loss: 255.0436\n",
            "Epoch 7299, Train Loss: 596.4723, Val Loss: 220.0351\n",
            "Epoch 7300, Train Loss: 607.4821, Val Loss: 199.6434\n",
            "Epoch 7301, Train Loss: 611.8906, Val Loss: 223.1224\n",
            "Epoch 7302, Train Loss: 611.6580, Val Loss: 245.7114\n",
            "Epoch 7303, Train Loss: 615.3496, Val Loss: 283.4976\n",
            "Epoch 7304, Train Loss: 628.4006, Val Loss: 238.6241\n",
            "Epoch 7305, Train Loss: 627.8564, Val Loss: 172.7935\n",
            "Epoch 7306, Train Loss: 599.8791, Val Loss: 173.8805\n",
            "Epoch 7307, Train Loss: 624.3724, Val Loss: 175.0712\n",
            "Epoch 7308, Train Loss: 617.1062, Val Loss: 188.2518\n",
            "Epoch 7309, Train Loss: 612.7540, Val Loss: 196.4053\n",
            "Epoch 7310, Train Loss: 588.3332, Val Loss: 186.8410\n",
            "Epoch 7311, Train Loss: 587.6900, Val Loss: 190.7997\n",
            "Epoch 7312, Train Loss: 625.4226, Val Loss: 199.0226\n",
            "Epoch 7313, Train Loss: 586.6959, Val Loss: 190.0773\n",
            "Epoch 7314, Train Loss: 595.7564, Val Loss: 179.1792\n",
            "Epoch 7315, Train Loss: 604.6809, Val Loss: 177.5587\n",
            "Epoch 7316, Train Loss: 591.1213, Val Loss: 201.8936\n",
            "Epoch 7317, Train Loss: 610.8630, Val Loss: 215.5233\n",
            "Epoch 7318, Train Loss: 621.7449, Val Loss: 209.0931\n",
            "Epoch 7319, Train Loss: 618.0255, Val Loss: 199.7040\n",
            "Epoch 7320, Train Loss: 621.5622, Val Loss: 188.3235\n",
            "Epoch 7321, Train Loss: 606.5262, Val Loss: 175.8117\n",
            "Epoch 7322, Train Loss: 575.1101, Val Loss: 174.6379\n",
            "Epoch 7323, Train Loss: 602.9099, Val Loss: 206.0531\n",
            "Epoch 7324, Train Loss: 624.9541, Val Loss: 228.2744\n",
            "Epoch 7325, Train Loss: 607.6082, Val Loss: 216.1963\n",
            "Epoch 7326, Train Loss: 588.2911, Val Loss: 191.8692\n",
            "Epoch 7327, Train Loss: 597.1479, Val Loss: 180.4094\n",
            "Epoch 7328, Train Loss: 590.1567, Val Loss: 174.9372\n",
            "Epoch 7329, Train Loss: 619.3273, Val Loss: 174.8120\n",
            "Epoch 7330, Train Loss: 625.4765, Val Loss: 196.1848\n",
            "Epoch 7331, Train Loss: 612.5793, Val Loss: 218.9844\n",
            "Epoch 7332, Train Loss: 523.6364, Val Loss: 268.9074\n",
            "Epoch 7333, Train Loss: 607.1858, Val Loss: 215.5286\n",
            "Epoch 7334, Train Loss: 623.5608, Val Loss: 181.3159\n",
            "Epoch 7335, Train Loss: 553.9314, Val Loss: 202.7781\n",
            "Epoch 7336, Train Loss: 624.4800, Val Loss: 229.8390\n",
            "Epoch 7337, Train Loss: 576.6522, Val Loss: 234.7253\n",
            "Epoch 7338, Train Loss: 619.8217, Val Loss: 200.7045\n",
            "Epoch 7339, Train Loss: 620.9456, Val Loss: 173.5364\n",
            "Epoch 7340, Train Loss: 609.5490, Val Loss: 169.0285\n",
            "Epoch 7341, Train Loss: 612.4077, Val Loss: 178.2811\n",
            "Epoch 7342, Train Loss: 612.0686, Val Loss: 195.3717\n",
            "Epoch 7343, Train Loss: 610.1796, Val Loss: 250.1427\n",
            "Epoch 7344, Train Loss: 600.6500, Val Loss: 259.2684\n",
            "Epoch 7345, Train Loss: 596.9702, Val Loss: 233.9876\n",
            "Epoch 7346, Train Loss: 583.7133, Val Loss: 244.8099\n",
            "Epoch 7347, Train Loss: 593.5214, Val Loss: 275.9389\n",
            "Epoch 7348, Train Loss: 593.0511, Val Loss: 220.1065\n",
            "Epoch 7349, Train Loss: 595.6312, Val Loss: 209.2543\n",
            "Epoch 7350, Train Loss: 629.0899, Val Loss: 226.9002\n",
            "Epoch 7351, Train Loss: 622.7128, Val Loss: 304.0130\n",
            "Epoch 7352, Train Loss: 627.2109, Val Loss: 248.6630\n",
            "Epoch 7353, Train Loss: 623.7130, Val Loss: 204.2147\n",
            "Epoch 7354, Train Loss: 612.7114, Val Loss: 177.8838\n",
            "Epoch 7355, Train Loss: 624.0162, Val Loss: 172.3238\n",
            "Epoch 7356, Train Loss: 588.9879, Val Loss: 165.8137\n",
            "Epoch 7357, Train Loss: 611.0903, Val Loss: 171.6401\n",
            "Epoch 7358, Train Loss: 562.8910, Val Loss: 222.3609\n",
            "Epoch 7359, Train Loss: 610.6633, Val Loss: 296.7662\n",
            "Epoch 7360, Train Loss: 597.3352, Val Loss: 292.0724\n",
            "Epoch 7361, Train Loss: 583.4929, Val Loss: 305.0270\n",
            "Epoch 7362, Train Loss: 583.8441, Val Loss: 259.1659\n",
            "Epoch 7363, Train Loss: 579.0242, Val Loss: 178.0879\n",
            "Epoch 7364, Train Loss: 590.1931, Val Loss: 176.0216\n",
            "Epoch 7365, Train Loss: 597.5113, Val Loss: 221.3226\n",
            "Epoch 7366, Train Loss: 620.4863, Val Loss: 341.2935\n",
            "Epoch 7367, Train Loss: 619.3487, Val Loss: 340.7901\n",
            "Epoch 7368, Train Loss: 613.0829, Val Loss: 238.2947\n",
            "Epoch 7369, Train Loss: 604.8842, Val Loss: 199.5289\n",
            "Epoch 7370, Train Loss: 620.1636, Val Loss: 227.8686\n",
            "Epoch 7371, Train Loss: 556.8929, Val Loss: 246.5537\n",
            "Epoch 7372, Train Loss: 599.6045, Val Loss: 182.6261\n",
            "Epoch 7373, Train Loss: 618.6363, Val Loss: 172.7694\n",
            "Epoch 7374, Train Loss: 568.6179, Val Loss: 183.3407\n",
            "Epoch 7375, Train Loss: 592.6070, Val Loss: 213.2859\n",
            "Epoch 7376, Train Loss: 622.7559, Val Loss: 191.8904\n",
            "Epoch 7377, Train Loss: 624.6415, Val Loss: 181.7058\n",
            "Epoch 7378, Train Loss: 592.1700, Val Loss: 186.3569\n",
            "Epoch 7379, Train Loss: 617.8047, Val Loss: 192.3345\n",
            "Epoch 7380, Train Loss: 625.1511, Val Loss: 176.8053\n",
            "Epoch 7381, Train Loss: 605.0496, Val Loss: 174.0244\n",
            "Epoch 7382, Train Loss: 608.6986, Val Loss: 176.4939\n",
            "Epoch 7383, Train Loss: 564.3543, Val Loss: 200.5590\n",
            "Epoch 7384, Train Loss: 589.1980, Val Loss: 260.0873\n",
            "Epoch 7385, Train Loss: 623.9750, Val Loss: 239.2160\n",
            "Epoch 7386, Train Loss: 601.2345, Val Loss: 181.5388\n",
            "Epoch 7387, Train Loss: 624.9994, Val Loss: 177.1775\n",
            "Epoch 7388, Train Loss: 570.7121, Val Loss: 179.5973\n",
            "Epoch 7389, Train Loss: 627.5958, Val Loss: 177.0651\n",
            "Epoch 7390, Train Loss: 587.7644, Val Loss: 201.4323\n",
            "Epoch 7391, Train Loss: 599.1457, Val Loss: 303.9832\n",
            "Epoch 7392, Train Loss: 626.3050, Val Loss: 300.9403\n",
            "Epoch 7393, Train Loss: 606.2514, Val Loss: 251.4195\n",
            "Epoch 7394, Train Loss: 560.3394, Val Loss: 206.1956\n",
            "Epoch 7395, Train Loss: 635.9617, Val Loss: 183.5725\n",
            "Epoch 7396, Train Loss: 591.5593, Val Loss: 186.6369\n",
            "Epoch 7397, Train Loss: 624.3273, Val Loss: 201.2542\n",
            "Epoch 7398, Train Loss: 605.4823, Val Loss: 203.8586\n",
            "Epoch 7399, Train Loss: 619.4391, Val Loss: 201.4639\n",
            "Epoch 7400, Train Loss: 606.8609, Val Loss: 237.8176\n",
            "Epoch 7401, Train Loss: 588.1854, Val Loss: 360.1943\n",
            "Epoch 7402, Train Loss: 589.8950, Val Loss: 379.8681\n",
            "Epoch 7403, Train Loss: 625.0286, Val Loss: 291.4851\n",
            "Epoch 7404, Train Loss: 585.0305, Val Loss: 236.8653\n",
            "Epoch 7405, Train Loss: 587.4399, Val Loss: 291.1605\n",
            "Epoch 7406, Train Loss: 625.8470, Val Loss: 366.1285\n",
            "Epoch 7407, Train Loss: 594.3160, Val Loss: 244.7616\n",
            "Epoch 7408, Train Loss: 613.2005, Val Loss: 178.9681\n",
            "Epoch 7409, Train Loss: 607.9971, Val Loss: 169.9549\n",
            "Epoch 7410, Train Loss: 615.1509, Val Loss: 170.7816\n",
            "Epoch 7411, Train Loss: 628.8795, Val Loss: 167.7806\n",
            "Epoch 7412, Train Loss: 584.7323, Val Loss: 168.4462\n",
            "Epoch 7413, Train Loss: 622.3529, Val Loss: 171.4009\n",
            "Epoch 7414, Train Loss: 588.2127, Val Loss: 175.6207\n",
            "Epoch 7415, Train Loss: 587.9867, Val Loss: 355.8505\n",
            "Epoch 7416, Train Loss: 624.5751, Val Loss: 455.4231\n",
            "Epoch 7417, Train Loss: 612.1573, Val Loss: 375.8209\n",
            "Epoch 7418, Train Loss: 618.7094, Val Loss: 197.9706\n",
            "Epoch 7419, Train Loss: 622.0444, Val Loss: 195.4112\n",
            "Epoch 7420, Train Loss: 625.3453, Val Loss: 185.5241\n",
            "Epoch 7421, Train Loss: 624.7533, Val Loss: 186.3076\n",
            "Epoch 7422, Train Loss: 627.0668, Val Loss: 179.1589\n",
            "Epoch 7423, Train Loss: 598.2582, Val Loss: 197.8983\n",
            "Epoch 7424, Train Loss: 624.9250, Val Loss: 233.1213\n",
            "Epoch 7425, Train Loss: 627.6863, Val Loss: 275.9868\n",
            "Epoch 7426, Train Loss: 625.2380, Val Loss: 254.3859\n",
            "Epoch 7427, Train Loss: 625.2224, Val Loss: 193.6918\n",
            "Epoch 7428, Train Loss: 611.9712, Val Loss: 183.2506\n",
            "Epoch 7429, Train Loss: 623.6249, Val Loss: 184.1234\n",
            "Epoch 7430, Train Loss: 623.2642, Val Loss: 184.3322\n",
            "Epoch 7431, Train Loss: 597.8410, Val Loss: 201.1337\n",
            "Epoch 7432, Train Loss: 584.5474, Val Loss: 191.7629\n",
            "Epoch 7433, Train Loss: 613.6324, Val Loss: 184.1404\n",
            "Epoch 7434, Train Loss: 561.2766, Val Loss: 201.9397\n",
            "Epoch 7435, Train Loss: 584.6171, Val Loss: 214.3111\n",
            "Epoch 7436, Train Loss: 610.1263, Val Loss: 188.7240\n",
            "Epoch 7437, Train Loss: 616.0431, Val Loss: 184.7553\n",
            "Epoch 7438, Train Loss: 623.0249, Val Loss: 189.1010\n",
            "Epoch 7439, Train Loss: 623.9038, Val Loss: 205.0480\n",
            "Epoch 7440, Train Loss: 570.7827, Val Loss: 193.8738\n",
            "Epoch 7441, Train Loss: 598.2566, Val Loss: 176.7477\n",
            "Epoch 7442, Train Loss: 613.2693, Val Loss: 173.4688\n",
            "Epoch 7443, Train Loss: 621.9527, Val Loss: 172.2027\n",
            "Epoch 7444, Train Loss: 622.6390, Val Loss: 176.4553\n",
            "Epoch 7445, Train Loss: 615.5142, Val Loss: 208.5377\n",
            "Epoch 7446, Train Loss: 625.0446, Val Loss: 230.7316\n",
            "Epoch 7447, Train Loss: 574.5346, Val Loss: 270.2785\n",
            "Epoch 7448, Train Loss: 577.5008, Val Loss: 299.0346\n",
            "Epoch 7449, Train Loss: 617.6779, Val Loss: 257.6863\n",
            "Epoch 7450, Train Loss: 625.6855, Val Loss: 278.6782\n",
            "Epoch 7451, Train Loss: 623.2332, Val Loss: 306.5740\n",
            "Epoch 7452, Train Loss: 596.6438, Val Loss: 214.8401\n",
            "Epoch 7453, Train Loss: 613.2746, Val Loss: 177.6787\n",
            "Epoch 7454, Train Loss: 592.0733, Val Loss: 176.1264\n",
            "Epoch 7455, Train Loss: 607.1192, Val Loss: 202.9641\n",
            "Epoch 7456, Train Loss: 607.7977, Val Loss: 185.4413\n",
            "Epoch 7457, Train Loss: 624.2602, Val Loss: 172.6857\n",
            "Epoch 7458, Train Loss: 623.0179, Val Loss: 174.5547\n",
            "Epoch 7459, Train Loss: 617.2385, Val Loss: 177.6339\n",
            "Epoch 7460, Train Loss: 611.5262, Val Loss: 183.3031\n",
            "Epoch 7461, Train Loss: 601.2197, Val Loss: 209.4896\n",
            "Epoch 7462, Train Loss: 602.0016, Val Loss: 233.5299\n",
            "Epoch 7463, Train Loss: 624.6613, Val Loss: 173.0942\n",
            "Epoch 7464, Train Loss: 616.1156, Val Loss: 176.8488\n",
            "Epoch 7465, Train Loss: 614.6739, Val Loss: 176.6426\n",
            "Epoch 7466, Train Loss: 597.6885, Val Loss: 194.0236\n",
            "Epoch 7467, Train Loss: 607.0799, Val Loss: 244.8571\n",
            "Epoch 7468, Train Loss: 622.6249, Val Loss: 272.3040\n",
            "Epoch 7469, Train Loss: 608.5889, Val Loss: 245.2031\n",
            "Epoch 7470, Train Loss: 606.6155, Val Loss: 226.7811\n",
            "Epoch 7471, Train Loss: 578.9783, Val Loss: 189.4248\n",
            "Epoch 7472, Train Loss: 602.6602, Val Loss: 203.2058\n",
            "Epoch 7473, Train Loss: 613.9113, Val Loss: 248.2425\n",
            "Epoch 7474, Train Loss: 593.4983, Val Loss: 320.3113\n",
            "Epoch 7475, Train Loss: 617.9043, Val Loss: 246.8515\n",
            "Epoch 7476, Train Loss: 568.2447, Val Loss: 179.6602\n",
            "Epoch 7477, Train Loss: 619.6947, Val Loss: 181.2253\n",
            "Epoch 7478, Train Loss: 581.3867, Val Loss: 212.7282\n",
            "Epoch 7479, Train Loss: 615.3023, Val Loss: 193.8322\n",
            "Epoch 7480, Train Loss: 613.9783, Val Loss: 178.4597\n",
            "Epoch 7481, Train Loss: 611.0587, Val Loss: 181.1218\n",
            "Epoch 7482, Train Loss: 616.8543, Val Loss: 183.0118\n",
            "Epoch 7483, Train Loss: 619.2216, Val Loss: 180.4311\n",
            "Epoch 7484, Train Loss: 619.3438, Val Loss: 178.5056\n",
            "Epoch 7485, Train Loss: 608.2305, Val Loss: 173.2709\n",
            "Epoch 7486, Train Loss: 591.0787, Val Loss: 213.4194\n",
            "Epoch 7487, Train Loss: 614.6211, Val Loss: 241.7611\n",
            "Epoch 7488, Train Loss: 547.8874, Val Loss: 266.9628\n",
            "Epoch 7489, Train Loss: 617.3233, Val Loss: 252.7910\n",
            "Epoch 7490, Train Loss: 619.9047, Val Loss: 179.7992\n",
            "Epoch 7491, Train Loss: 615.3888, Val Loss: 175.6822\n",
            "Epoch 7492, Train Loss: 560.0775, Val Loss: 180.4188\n",
            "Epoch 7493, Train Loss: 625.5631, Val Loss: 196.5753\n",
            "Epoch 7494, Train Loss: 610.4444, Val Loss: 240.4127\n",
            "Epoch 7495, Train Loss: 627.5095, Val Loss: 239.8451\n",
            "Epoch 7496, Train Loss: 614.4411, Val Loss: 214.7728\n",
            "Epoch 7497, Train Loss: 618.9685, Val Loss: 210.1098\n",
            "Epoch 7498, Train Loss: 622.9930, Val Loss: 211.3470\n",
            "Epoch 7499, Train Loss: 612.3826, Val Loss: 206.7628\n",
            "Epoch 7500, Train Loss: 599.1004, Val Loss: 178.1002\n",
            "Epoch 7501, Train Loss: 562.7570, Val Loss: 187.3715\n",
            "Epoch 7502, Train Loss: 595.9555, Val Loss: 270.5916\n",
            "Epoch 7503, Train Loss: 603.8741, Val Loss: 278.4810\n",
            "Epoch 7504, Train Loss: 605.5661, Val Loss: 226.6880\n",
            "Epoch 7505, Train Loss: 608.3706, Val Loss: 203.2739\n",
            "Epoch 7506, Train Loss: 615.8927, Val Loss: 183.7900\n",
            "Epoch 7507, Train Loss: 611.9165, Val Loss: 185.6236\n",
            "Epoch 7508, Train Loss: 615.0874, Val Loss: 213.6446\n",
            "Epoch 7509, Train Loss: 625.6701, Val Loss: 251.0650\n",
            "Epoch 7510, Train Loss: 619.2316, Val Loss: 229.1085\n",
            "Epoch 7511, Train Loss: 583.4249, Val Loss: 173.2530\n",
            "Epoch 7512, Train Loss: 625.9367, Val Loss: 169.9520\n",
            "Epoch 7513, Train Loss: 618.5031, Val Loss: 173.7929\n",
            "Epoch 7514, Train Loss: 614.9417, Val Loss: 188.8493\n",
            "Epoch 7515, Train Loss: 551.8694, Val Loss: 195.3034\n",
            "Epoch 7516, Train Loss: 596.3759, Val Loss: 177.4463\n",
            "Epoch 7517, Train Loss: 573.8154, Val Loss: 188.2645\n",
            "Epoch 7518, Train Loss: 610.3227, Val Loss: 209.5184\n",
            "Epoch 7519, Train Loss: 618.1414, Val Loss: 293.5124\n",
            "Epoch 7520, Train Loss: 617.7141, Val Loss: 278.7140\n",
            "Epoch 7521, Train Loss: 607.8542, Val Loss: 235.5733\n",
            "Epoch 7522, Train Loss: 595.8981, Val Loss: 232.8426\n",
            "Epoch 7523, Train Loss: 624.6895, Val Loss: 243.3188\n",
            "Epoch 7524, Train Loss: 628.9765, Val Loss: 255.2922\n",
            "Epoch 7525, Train Loss: 620.7326, Val Loss: 227.6602\n",
            "Epoch 7526, Train Loss: 612.7187, Val Loss: 176.9277\n",
            "Epoch 7527, Train Loss: 608.2218, Val Loss: 169.2383\n",
            "Epoch 7528, Train Loss: 592.0880, Val Loss: 170.0644\n",
            "Epoch 7529, Train Loss: 621.3363, Val Loss: 176.1208\n",
            "Epoch 7530, Train Loss: 549.6313, Val Loss: 210.9159\n",
            "Epoch 7531, Train Loss: 605.7648, Val Loss: 228.2318\n",
            "Epoch 7532, Train Loss: 603.3020, Val Loss: 209.7088\n",
            "Epoch 7533, Train Loss: 609.0633, Val Loss: 174.9281\n",
            "Epoch 7534, Train Loss: 621.4938, Val Loss: 185.9017\n",
            "Epoch 7535, Train Loss: 576.0777, Val Loss: 254.5814\n",
            "Epoch 7536, Train Loss: 596.0361, Val Loss: 394.4854\n",
            "Epoch 7537, Train Loss: 619.6369, Val Loss: 372.2267\n",
            "Epoch 7538, Train Loss: 603.2517, Val Loss: 273.5164\n",
            "Epoch 7539, Train Loss: 588.5225, Val Loss: 192.5072\n",
            "Epoch 7540, Train Loss: 630.3091, Val Loss: 187.0875\n",
            "Epoch 7541, Train Loss: 618.2280, Val Loss: 253.1818\n",
            "Epoch 7542, Train Loss: 617.4073, Val Loss: 257.7061\n",
            "Epoch 7543, Train Loss: 623.9914, Val Loss: 237.9685\n",
            "Epoch 7544, Train Loss: 583.6426, Val Loss: 220.3631\n",
            "Epoch 7545, Train Loss: 595.8403, Val Loss: 212.7436\n",
            "Epoch 7546, Train Loss: 614.0506, Val Loss: 215.4581\n",
            "Epoch 7547, Train Loss: 597.2564, Val Loss: 240.0453\n",
            "Epoch 7548, Train Loss: 605.4434, Val Loss: 189.5249\n",
            "Epoch 7549, Train Loss: 618.1853, Val Loss: 210.5277\n",
            "Epoch 7550, Train Loss: 622.1649, Val Loss: 186.4512\n",
            "Epoch 7551, Train Loss: 613.8794, Val Loss: 170.9843\n",
            "Epoch 7552, Train Loss: 596.6909, Val Loss: 211.3521\n",
            "Epoch 7553, Train Loss: 621.3210, Val Loss: 226.2126\n",
            "Epoch 7554, Train Loss: 613.2126, Val Loss: 202.7964\n",
            "Epoch 7555, Train Loss: 606.4551, Val Loss: 191.1809\n",
            "Epoch 7556, Train Loss: 606.5143, Val Loss: 196.6521\n",
            "Epoch 7557, Train Loss: 625.8005, Val Loss: 220.5426\n",
            "Epoch 7558, Train Loss: 580.4864, Val Loss: 274.4900\n",
            "Epoch 7559, Train Loss: 598.0742, Val Loss: 247.2951\n",
            "Epoch 7560, Train Loss: 604.6674, Val Loss: 195.1180\n",
            "Epoch 7561, Train Loss: 593.0698, Val Loss: 186.4943\n",
            "Epoch 7562, Train Loss: 599.4666, Val Loss: 218.6637\n",
            "Epoch 7563, Train Loss: 617.7577, Val Loss: 229.3019\n",
            "Epoch 7564, Train Loss: 600.3578, Val Loss: 213.3155\n",
            "Epoch 7565, Train Loss: 595.5479, Val Loss: 210.1297\n",
            "Epoch 7566, Train Loss: 613.5733, Val Loss: 206.9022\n",
            "Epoch 7567, Train Loss: 585.2803, Val Loss: 192.9765\n",
            "Epoch 7568, Train Loss: 623.8990, Val Loss: 176.0292\n",
            "Epoch 7569, Train Loss: 610.8336, Val Loss: 174.9951\n",
            "Epoch 7570, Train Loss: 623.5438, Val Loss: 196.8746\n",
            "Epoch 7571, Train Loss: 601.1694, Val Loss: 202.0121\n",
            "Epoch 7572, Train Loss: 623.9415, Val Loss: 187.7066\n",
            "Epoch 7573, Train Loss: 616.7688, Val Loss: 180.6002\n",
            "Epoch 7574, Train Loss: 623.3099, Val Loss: 194.4277\n",
            "Epoch 7575, Train Loss: 620.5833, Val Loss: 215.8571\n",
            "Epoch 7576, Train Loss: 604.6197, Val Loss: 194.5363\n",
            "Epoch 7577, Train Loss: 610.6417, Val Loss: 173.3552\n",
            "Epoch 7578, Train Loss: 607.1280, Val Loss: 175.3544\n",
            "Epoch 7579, Train Loss: 587.7214, Val Loss: 268.2144\n",
            "Epoch 7580, Train Loss: 624.1688, Val Loss: 339.7471\n",
            "Epoch 7581, Train Loss: 612.4522, Val Loss: 339.7085\n",
            "Epoch 7582, Train Loss: 617.3488, Val Loss: 306.7060\n",
            "Epoch 7583, Train Loss: 611.9901, Val Loss: 216.8293\n",
            "Epoch 7584, Train Loss: 594.9989, Val Loss: 185.0240\n",
            "Epoch 7585, Train Loss: 592.2528, Val Loss: 177.6911\n",
            "Epoch 7586, Train Loss: 624.7111, Val Loss: 203.6155\n",
            "Epoch 7587, Train Loss: 617.9675, Val Loss: 258.7967\n",
            "Epoch 7588, Train Loss: 587.4158, Val Loss: 194.5297\n",
            "Epoch 7589, Train Loss: 617.9600, Val Loss: 179.3645\n",
            "Epoch 7590, Train Loss: 586.3367, Val Loss: 176.7912\n",
            "Epoch 7591, Train Loss: 585.7925, Val Loss: 185.9404\n",
            "Epoch 7592, Train Loss: 627.5924, Val Loss: 185.8632\n",
            "Epoch 7593, Train Loss: 622.2404, Val Loss: 182.8701\n",
            "Epoch 7594, Train Loss: 620.4845, Val Loss: 181.2783\n",
            "Epoch 7595, Train Loss: 618.7197, Val Loss: 180.7925\n",
            "Epoch 7596, Train Loss: 622.2085, Val Loss: 188.2710\n",
            "Epoch 7597, Train Loss: 612.2069, Val Loss: 178.9217\n",
            "Epoch 7598, Train Loss: 587.4265, Val Loss: 169.4180\n",
            "Epoch 7599, Train Loss: 613.4947, Val Loss: 179.7448\n",
            "Epoch 7600, Train Loss: 624.1692, Val Loss: 218.2941\n",
            "Epoch 7601, Train Loss: 617.6610, Val Loss: 268.4218\n",
            "Epoch 7602, Train Loss: 558.4087, Val Loss: 270.3810\n",
            "Epoch 7603, Train Loss: 631.7319, Val Loss: 190.4668\n",
            "Epoch 7604, Train Loss: 620.3243, Val Loss: 172.8629\n",
            "Epoch 7605, Train Loss: 625.6753, Val Loss: 178.4186\n",
            "Epoch 7606, Train Loss: 612.4661, Val Loss: 166.7165\n",
            "Epoch 7607, Train Loss: 607.9782, Val Loss: 174.0087\n",
            "Epoch 7608, Train Loss: 589.5956, Val Loss: 213.4169\n",
            "Epoch 7609, Train Loss: 614.4703, Val Loss: 267.6799\n",
            "Epoch 7610, Train Loss: 588.2864, Val Loss: 271.0474\n",
            "Epoch 7611, Train Loss: 626.5369, Val Loss: 267.5378\n",
            "Epoch 7612, Train Loss: 606.7499, Val Loss: 267.7587\n",
            "Epoch 7613, Train Loss: 618.0496, Val Loss: 221.3244\n",
            "Epoch 7614, Train Loss: 604.8174, Val Loss: 189.2918\n",
            "Epoch 7615, Train Loss: 570.1664, Val Loss: 199.1915\n",
            "Epoch 7616, Train Loss: 590.1209, Val Loss: 222.5003\n",
            "Epoch 7617, Train Loss: 602.3611, Val Loss: 265.6090\n",
            "Epoch 7618, Train Loss: 601.0360, Val Loss: 249.6249\n",
            "Epoch 7619, Train Loss: 607.3796, Val Loss: 199.7406\n",
            "Epoch 7620, Train Loss: 607.4194, Val Loss: 179.9728\n",
            "Epoch 7621, Train Loss: 619.3224, Val Loss: 193.0960\n",
            "Epoch 7622, Train Loss: 601.8346, Val Loss: 206.9435\n",
            "Epoch 7623, Train Loss: 625.7312, Val Loss: 180.0977\n",
            "Epoch 7624, Train Loss: 573.8269, Val Loss: 171.4245\n",
            "Epoch 7625, Train Loss: 572.7001, Val Loss: 171.0279\n",
            "Epoch 7626, Train Loss: 625.0138, Val Loss: 184.5796\n",
            "Epoch 7627, Train Loss: 613.8514, Val Loss: 200.6593\n",
            "Epoch 7628, Train Loss: 622.4792, Val Loss: 186.5842\n",
            "Epoch 7629, Train Loss: 621.0898, Val Loss: 180.4224\n",
            "Epoch 7630, Train Loss: 611.7774, Val Loss: 177.5841\n",
            "Epoch 7631, Train Loss: 617.0623, Val Loss: 198.0745\n",
            "Epoch 7632, Train Loss: 614.1715, Val Loss: 241.8014\n",
            "Epoch 7633, Train Loss: 581.3375, Val Loss: 251.5738\n",
            "Epoch 7634, Train Loss: 622.8531, Val Loss: 230.6725\n",
            "Epoch 7635, Train Loss: 614.8544, Val Loss: 192.9961\n",
            "Epoch 7636, Train Loss: 576.0102, Val Loss: 179.4225\n",
            "Epoch 7637, Train Loss: 615.7784, Val Loss: 184.3360\n",
            "Epoch 7638, Train Loss: 597.9315, Val Loss: 216.0171\n",
            "Epoch 7639, Train Loss: 614.8088, Val Loss: 226.5600\n",
            "Epoch 7640, Train Loss: 618.6348, Val Loss: 222.6363\n",
            "Epoch 7641, Train Loss: 619.3956, Val Loss: 200.4931\n",
            "Epoch 7642, Train Loss: 602.5982, Val Loss: 190.1612\n",
            "Epoch 7643, Train Loss: 604.4319, Val Loss: 193.6545\n",
            "Epoch 7644, Train Loss: 615.1046, Val Loss: 195.5016\n",
            "Epoch 7645, Train Loss: 613.1529, Val Loss: 187.4770\n",
            "Epoch 7646, Train Loss: 572.8306, Val Loss: 187.9280\n",
            "Epoch 7647, Train Loss: 614.4776, Val Loss: 208.8061\n",
            "Epoch 7648, Train Loss: 587.8650, Val Loss: 277.1362\n",
            "Epoch 7649, Train Loss: 599.8580, Val Loss: 270.3657\n",
            "Epoch 7650, Train Loss: 602.2111, Val Loss: 196.4008\n",
            "Epoch 7651, Train Loss: 605.9204, Val Loss: 194.7777\n",
            "Epoch 7652, Train Loss: 574.5290, Val Loss: 199.7993\n",
            "Epoch 7653, Train Loss: 600.5561, Val Loss: 205.3166\n",
            "Epoch 7654, Train Loss: 616.3728, Val Loss: 175.3733\n",
            "Epoch 7655, Train Loss: 575.1997, Val Loss: 174.0224\n",
            "Epoch 7656, Train Loss: 593.1494, Val Loss: 174.4884\n",
            "Epoch 7657, Train Loss: 581.6768, Val Loss: 235.5847\n",
            "Epoch 7658, Train Loss: 618.5609, Val Loss: 255.0880\n",
            "Epoch 7659, Train Loss: 604.6405, Val Loss: 226.6054\n",
            "Epoch 7660, Train Loss: 608.5543, Val Loss: 200.7055\n",
            "Epoch 7661, Train Loss: 540.5090, Val Loss: 210.0717\n",
            "Epoch 7662, Train Loss: 605.3499, Val Loss: 224.1807\n",
            "Epoch 7663, Train Loss: 613.7975, Val Loss: 208.5338\n",
            "Epoch 7664, Train Loss: 614.1246, Val Loss: 177.7804\n",
            "Epoch 7665, Train Loss: 622.5017, Val Loss: 200.6790\n",
            "Epoch 7666, Train Loss: 620.9752, Val Loss: 265.6602\n",
            "Epoch 7667, Train Loss: 610.0712, Val Loss: 209.9481\n",
            "Epoch 7668, Train Loss: 575.4682, Val Loss: 175.4235\n",
            "Epoch 7669, Train Loss: 606.8081, Val Loss: 178.1691\n",
            "Epoch 7670, Train Loss: 612.2594, Val Loss: 176.4568\n",
            "Epoch 7671, Train Loss: 618.9655, Val Loss: 224.2712\n",
            "Epoch 7672, Train Loss: 617.2777, Val Loss: 282.9099\n",
            "Epoch 7673, Train Loss: 609.5951, Val Loss: 270.8466\n",
            "Epoch 7674, Train Loss: 577.5520, Val Loss: 276.4293\n",
            "Epoch 7675, Train Loss: 616.7009, Val Loss: 239.6277\n",
            "Epoch 7676, Train Loss: 630.6119, Val Loss: 194.1669\n",
            "Epoch 7677, Train Loss: 622.9811, Val Loss: 181.5657\n",
            "Epoch 7678, Train Loss: 603.7874, Val Loss: 179.6491\n",
            "Epoch 7679, Train Loss: 580.6854, Val Loss: 188.3002\n",
            "Epoch 7680, Train Loss: 621.4144, Val Loss: 212.4008\n",
            "Epoch 7681, Train Loss: 620.6406, Val Loss: 238.3167\n",
            "Epoch 7682, Train Loss: 589.9982, Val Loss: 244.8629\n",
            "Epoch 7683, Train Loss: 624.2671, Val Loss: 248.5849\n",
            "Epoch 7684, Train Loss: 612.9262, Val Loss: 217.6188\n",
            "Epoch 7685, Train Loss: 607.6549, Val Loss: 188.3194\n",
            "Epoch 7686, Train Loss: 601.7387, Val Loss: 188.4259\n",
            "Epoch 7687, Train Loss: 574.9666, Val Loss: 192.5106\n",
            "Epoch 7688, Train Loss: 594.8902, Val Loss: 213.6318\n",
            "Epoch 7689, Train Loss: 587.4902, Val Loss: 189.6856\n",
            "Epoch 7690, Train Loss: 607.8769, Val Loss: 176.2766\n",
            "Epoch 7691, Train Loss: 599.9981, Val Loss: 177.8481\n",
            "Epoch 7692, Train Loss: 621.7756, Val Loss: 197.5703\n",
            "Epoch 7693, Train Loss: 579.1864, Val Loss: 223.9975\n",
            "Epoch 7694, Train Loss: 599.5313, Val Loss: 219.8747\n",
            "Epoch 7695, Train Loss: 618.7160, Val Loss: 188.6543\n",
            "Epoch 7696, Train Loss: 612.2867, Val Loss: 177.4236\n",
            "Epoch 7697, Train Loss: 606.7578, Val Loss: 186.7446\n",
            "Epoch 7698, Train Loss: 606.8182, Val Loss: 196.0454\n",
            "Epoch 7699, Train Loss: 622.6611, Val Loss: 193.8933\n",
            "Epoch 7700, Train Loss: 624.4480, Val Loss: 189.2855\n",
            "Epoch 7701, Train Loss: 608.7970, Val Loss: 183.4894\n",
            "Epoch 7702, Train Loss: 627.3017, Val Loss: 185.2954\n",
            "Epoch 7703, Train Loss: 610.7034, Val Loss: 187.2582\n",
            "Epoch 7704, Train Loss: 616.5373, Val Loss: 183.5490\n",
            "Epoch 7705, Train Loss: 625.6619, Val Loss: 174.1103\n",
            "Epoch 7706, Train Loss: 577.9297, Val Loss: 191.2452\n",
            "Epoch 7707, Train Loss: 627.8158, Val Loss: 228.7022\n",
            "Epoch 7708, Train Loss: 591.9983, Val Loss: 273.7687\n",
            "Epoch 7709, Train Loss: 619.5461, Val Loss: 305.5299\n",
            "Epoch 7710, Train Loss: 600.8416, Val Loss: 217.8025\n",
            "Epoch 7711, Train Loss: 557.0094, Val Loss: 175.6230\n",
            "Epoch 7712, Train Loss: 618.2032, Val Loss: 177.9021\n",
            "Epoch 7713, Train Loss: 616.8342, Val Loss: 182.6849\n",
            "Epoch 7714, Train Loss: 625.3220, Val Loss: 182.3155\n",
            "Epoch 7715, Train Loss: 622.7316, Val Loss: 185.9218\n",
            "Epoch 7716, Train Loss: 585.0622, Val Loss: 179.4927\n",
            "Epoch 7717, Train Loss: 592.2572, Val Loss: 190.4835\n",
            "Epoch 7718, Train Loss: 588.9039, Val Loss: 199.6913\n",
            "Epoch 7719, Train Loss: 613.2922, Val Loss: 197.7904\n",
            "Epoch 7720, Train Loss: 624.1652, Val Loss: 236.8588\n",
            "Epoch 7721, Train Loss: 600.9169, Val Loss: 259.3802\n",
            "Epoch 7722, Train Loss: 624.8712, Val Loss: 260.8222\n",
            "Epoch 7723, Train Loss: 588.9852, Val Loss: 255.2696\n",
            "Epoch 7724, Train Loss: 597.1253, Val Loss: 202.4268\n",
            "Epoch 7725, Train Loss: 614.7291, Val Loss: 184.6200\n",
            "Epoch 7726, Train Loss: 608.7956, Val Loss: 179.4495\n",
            "Epoch 7727, Train Loss: 606.8578, Val Loss: 184.7262\n",
            "Epoch 7728, Train Loss: 606.7985, Val Loss: 210.4012\n",
            "Epoch 7729, Train Loss: 596.1420, Val Loss: 195.9317\n",
            "Epoch 7730, Train Loss: 618.9999, Val Loss: 172.5531\n",
            "Epoch 7731, Train Loss: 599.4494, Val Loss: 172.6247\n",
            "Epoch 7732, Train Loss: 626.5091, Val Loss: 183.4690\n",
            "Epoch 7733, Train Loss: 623.0750, Val Loss: 219.0170\n",
            "Epoch 7734, Train Loss: 594.4711, Val Loss: 214.3748\n",
            "Epoch 7735, Train Loss: 544.2524, Val Loss: 277.5617\n",
            "Epoch 7736, Train Loss: 618.1434, Val Loss: 315.1949\n",
            "Epoch 7737, Train Loss: 620.9104, Val Loss: 245.7492\n",
            "Epoch 7738, Train Loss: 579.1852, Val Loss: 213.9118\n",
            "Epoch 7739, Train Loss: 600.8272, Val Loss: 229.5566\n",
            "Epoch 7740, Train Loss: 536.9227, Val Loss: 227.4861\n",
            "Epoch 7741, Train Loss: 575.6361, Val Loss: 284.8765\n",
            "Epoch 7742, Train Loss: 612.4205, Val Loss: 257.1134\n",
            "Epoch 7743, Train Loss: 626.2060, Val Loss: 224.4554\n",
            "Epoch 7744, Train Loss: 608.6297, Val Loss: 224.8519\n",
            "Epoch 7745, Train Loss: 557.2745, Val Loss: 273.5681\n",
            "Epoch 7746, Train Loss: 624.4241, Val Loss: 315.0892\n",
            "Epoch 7747, Train Loss: 575.7239, Val Loss: 224.0152\n",
            "Epoch 7748, Train Loss: 614.4970, Val Loss: 171.3961\n",
            "Epoch 7749, Train Loss: 623.1742, Val Loss: 172.5805\n",
            "Epoch 7750, Train Loss: 620.8296, Val Loss: 172.8065\n",
            "Epoch 7751, Train Loss: 603.6661, Val Loss: 186.7464\n",
            "Epoch 7752, Train Loss: 576.2003, Val Loss: 207.7789\n",
            "Epoch 7753, Train Loss: 625.7965, Val Loss: 195.9859\n",
            "Epoch 7754, Train Loss: 597.6764, Val Loss: 179.6021\n",
            "Epoch 7755, Train Loss: 600.9726, Val Loss: 180.3163\n",
            "Epoch 7756, Train Loss: 574.1854, Val Loss: 190.5846\n",
            "Epoch 7757, Train Loss: 626.8484, Val Loss: 202.7116\n",
            "Epoch 7758, Train Loss: 607.6656, Val Loss: 192.8179\n",
            "Epoch 7759, Train Loss: 620.3021, Val Loss: 186.8848\n",
            "Epoch 7760, Train Loss: 595.6162, Val Loss: 241.0879\n",
            "Epoch 7761, Train Loss: 624.4583, Val Loss: 314.2202\n",
            "Epoch 7762, Train Loss: 607.3461, Val Loss: 260.9187\n",
            "Epoch 7763, Train Loss: 613.9576, Val Loss: 192.6467\n",
            "Epoch 7764, Train Loss: 526.1150, Val Loss: 182.3970\n",
            "Epoch 7765, Train Loss: 618.9397, Val Loss: 183.4410\n",
            "Epoch 7766, Train Loss: 608.0840, Val Loss: 199.9477\n",
            "Epoch 7767, Train Loss: 589.4930, Val Loss: 226.4087\n",
            "Epoch 7768, Train Loss: 600.9111, Val Loss: 257.8973\n",
            "Epoch 7769, Train Loss: 607.4400, Val Loss: 234.5165\n",
            "Epoch 7770, Train Loss: 605.3388, Val Loss: 231.1542\n",
            "Epoch 7771, Train Loss: 630.6610, Val Loss: 217.3445\n",
            "Epoch 7772, Train Loss: 616.0181, Val Loss: 199.2910\n",
            "Epoch 7773, Train Loss: 563.7377, Val Loss: 192.5470\n",
            "Epoch 7774, Train Loss: 628.8317, Val Loss: 178.3186\n",
            "Epoch 7775, Train Loss: 617.2685, Val Loss: 174.7099\n",
            "Epoch 7776, Train Loss: 621.2640, Val Loss: 192.0938\n",
            "Epoch 7777, Train Loss: 566.3729, Val Loss: 206.6357\n",
            "Epoch 7778, Train Loss: 618.6862, Val Loss: 199.9860\n",
            "Epoch 7779, Train Loss: 529.5366, Val Loss: 185.0725\n",
            "Epoch 7780, Train Loss: 619.0594, Val Loss: 179.5233\n",
            "Epoch 7781, Train Loss: 604.6515, Val Loss: 192.8781\n",
            "Epoch 7782, Train Loss: 617.6144, Val Loss: 217.5641\n",
            "Epoch 7783, Train Loss: 617.8053, Val Loss: 192.4026\n",
            "Epoch 7784, Train Loss: 578.6943, Val Loss: 172.0461\n",
            "Epoch 7785, Train Loss: 617.3674, Val Loss: 167.4709\n",
            "Epoch 7786, Train Loss: 618.5445, Val Loss: 167.5198\n",
            "Epoch 7787, Train Loss: 616.9288, Val Loss: 215.2675\n",
            "Epoch 7788, Train Loss: 607.4004, Val Loss: 264.4498\n",
            "Epoch 7789, Train Loss: 560.3485, Val Loss: 256.7827\n",
            "Epoch 7790, Train Loss: 619.2419, Val Loss: 246.3878\n",
            "Epoch 7791, Train Loss: 605.8533, Val Loss: 215.1037\n",
            "Epoch 7792, Train Loss: 628.1170, Val Loss: 176.8141\n",
            "Epoch 7793, Train Loss: 617.6808, Val Loss: 178.8618\n",
            "Epoch 7794, Train Loss: 629.1436, Val Loss: 171.6498\n",
            "Epoch 7795, Train Loss: 599.4553, Val Loss: 191.3779\n",
            "Epoch 7796, Train Loss: 614.2626, Val Loss: 225.7035\n",
            "Epoch 7797, Train Loss: 620.7400, Val Loss: 211.3186\n",
            "Epoch 7798, Train Loss: 625.9872, Val Loss: 177.9074\n",
            "Epoch 7799, Train Loss: 622.1454, Val Loss: 171.3101\n",
            "Epoch 7800, Train Loss: 611.3701, Val Loss: 182.5541\n",
            "Epoch 7801, Train Loss: 614.2112, Val Loss: 202.1783\n",
            "Epoch 7802, Train Loss: 579.9209, Val Loss: 222.7435\n",
            "Epoch 7803, Train Loss: 618.3022, Val Loss: 205.0149\n",
            "Epoch 7804, Train Loss: 617.8508, Val Loss: 188.4863\n",
            "Epoch 7805, Train Loss: 585.5154, Val Loss: 202.6652\n",
            "Epoch 7806, Train Loss: 578.0734, Val Loss: 237.5082\n",
            "Epoch 7807, Train Loss: 615.7421, Val Loss: 267.5040\n",
            "Epoch 7808, Train Loss: 622.1410, Val Loss: 286.2777\n",
            "Epoch 7809, Train Loss: 605.9783, Val Loss: 240.4305\n",
            "Epoch 7810, Train Loss: 623.5790, Val Loss: 188.8712\n",
            "Epoch 7811, Train Loss: 615.9866, Val Loss: 171.7188\n",
            "Epoch 7812, Train Loss: 608.0625, Val Loss: 174.8025\n",
            "Epoch 7813, Train Loss: 627.0516, Val Loss: 182.0664\n",
            "Epoch 7814, Train Loss: 617.2609, Val Loss: 192.1145\n",
            "Epoch 7815, Train Loss: 615.9951, Val Loss: 188.4521\n",
            "Epoch 7816, Train Loss: 599.8706, Val Loss: 215.1850\n",
            "Epoch 7817, Train Loss: 628.8459, Val Loss: 267.4190\n",
            "Epoch 7818, Train Loss: 617.9835, Val Loss: 242.5428\n",
            "Epoch 7819, Train Loss: 619.9145, Val Loss: 206.6832\n",
            "Epoch 7820, Train Loss: 578.9652, Val Loss: 182.7817\n",
            "Epoch 7821, Train Loss: 616.7342, Val Loss: 174.2096\n",
            "Epoch 7822, Train Loss: 581.4739, Val Loss: 187.5796\n",
            "Epoch 7823, Train Loss: 614.1056, Val Loss: 271.4740\n",
            "Epoch 7824, Train Loss: 631.0408, Val Loss: 250.6419\n",
            "Epoch 7825, Train Loss: 607.9764, Val Loss: 172.1360\n",
            "Epoch 7826, Train Loss: 627.2276, Val Loss: 168.8348\n",
            "Epoch 7827, Train Loss: 607.7691, Val Loss: 180.8490\n",
            "Epoch 7828, Train Loss: 626.7963, Val Loss: 216.5009\n",
            "Epoch 7829, Train Loss: 612.7517, Val Loss: 265.0124\n",
            "Epoch 7830, Train Loss: 606.5957, Val Loss: 254.3580\n",
            "Epoch 7831, Train Loss: 560.0796, Val Loss: 282.8287\n",
            "Epoch 7832, Train Loss: 598.7982, Val Loss: 236.6122\n",
            "Epoch 7833, Train Loss: 621.3110, Val Loss: 184.9695\n",
            "Epoch 7834, Train Loss: 633.2766, Val Loss: 232.4390\n",
            "Epoch 7835, Train Loss: 609.7119, Val Loss: 207.6287\n",
            "Epoch 7836, Train Loss: 621.7457, Val Loss: 450.9997\n",
            "Epoch 7837, Train Loss: 626.0987, Val Loss: 396.3428\n",
            "Epoch 7838, Train Loss: 618.5429, Val Loss: 205.5565\n",
            "Epoch 7839, Train Loss: 615.1035, Val Loss: 207.2979\n",
            "Epoch 7840, Train Loss: 606.1742, Val Loss: 179.8699\n",
            "Epoch 7841, Train Loss: 565.3338, Val Loss: 320.8519\n",
            "Epoch 7842, Train Loss: 602.0083, Val Loss: 404.7269\n",
            "Epoch 7843, Train Loss: 611.0329, Val Loss: 348.2933\n",
            "Epoch 7844, Train Loss: 603.6239, Val Loss: 287.2428\n",
            "Epoch 7845, Train Loss: 624.4346, Val Loss: 271.4685\n",
            "Epoch 7846, Train Loss: 626.7259, Val Loss: 261.5794\n",
            "Epoch 7847, Train Loss: 583.0088, Val Loss: 244.8381\n",
            "Epoch 7848, Train Loss: 566.8519, Val Loss: 202.1097\n",
            "Epoch 7849, Train Loss: 620.5994, Val Loss: 180.3241\n",
            "Epoch 7850, Train Loss: 570.3575, Val Loss: 178.9717\n",
            "Epoch 7851, Train Loss: 630.7968, Val Loss: 179.4540\n",
            "Epoch 7852, Train Loss: 619.0466, Val Loss: 184.8574\n",
            "Epoch 7853, Train Loss: 601.0013, Val Loss: 203.7158\n",
            "Epoch 7854, Train Loss: 574.7124, Val Loss: 264.9515\n",
            "Epoch 7855, Train Loss: 593.9911, Val Loss: 292.2509\n",
            "Epoch 7856, Train Loss: 574.1046, Val Loss: 218.1139\n",
            "Epoch 7857, Train Loss: 621.7775, Val Loss: 196.2116\n",
            "Epoch 7858, Train Loss: 604.8723, Val Loss: 207.0905\n",
            "Epoch 7859, Train Loss: 610.6929, Val Loss: 200.6723\n",
            "Epoch 7860, Train Loss: 600.7767, Val Loss: 194.0777\n",
            "Epoch 7861, Train Loss: 620.6540, Val Loss: 205.3048\n",
            "Epoch 7862, Train Loss: 609.8336, Val Loss: 193.1869\n",
            "Epoch 7863, Train Loss: 606.9794, Val Loss: 185.5813\n",
            "Epoch 7864, Train Loss: 616.8491, Val Loss: 177.0298\n",
            "Epoch 7865, Train Loss: 595.5755, Val Loss: 191.5171\n",
            "Epoch 7866, Train Loss: 592.2974, Val Loss: 294.5779\n",
            "Epoch 7867, Train Loss: 614.0199, Val Loss: 383.1824\n",
            "Epoch 7868, Train Loss: 623.5397, Val Loss: 331.6918\n",
            "Epoch 7869, Train Loss: 582.8169, Val Loss: 207.3116\n",
            "Epoch 7870, Train Loss: 615.0935, Val Loss: 171.7617\n",
            "Epoch 7871, Train Loss: 612.4797, Val Loss: 175.4561\n",
            "Epoch 7872, Train Loss: 616.0742, Val Loss: 224.6892\n",
            "Epoch 7873, Train Loss: 542.9571, Val Loss: 208.6794\n",
            "Epoch 7874, Train Loss: 602.1493, Val Loss: 202.0736\n",
            "Epoch 7875, Train Loss: 585.3282, Val Loss: 194.2234\n",
            "Epoch 7876, Train Loss: 622.1937, Val Loss: 216.3292\n",
            "Epoch 7877, Train Loss: 613.9852, Val Loss: 201.3616\n",
            "Epoch 7878, Train Loss: 593.5746, Val Loss: 180.6945\n",
            "Epoch 7879, Train Loss: 620.5850, Val Loss: 179.2917\n",
            "Epoch 7880, Train Loss: 600.2134, Val Loss: 193.2238\n",
            "Epoch 7881, Train Loss: 593.9776, Val Loss: 267.4048\n",
            "Epoch 7882, Train Loss: 629.6693, Val Loss: 261.9267\n",
            "Epoch 7883, Train Loss: 617.1029, Val Loss: 177.8491\n",
            "Epoch 7884, Train Loss: 602.4893, Val Loss: 170.5983\n",
            "Epoch 7885, Train Loss: 606.1825, Val Loss: 177.0579\n",
            "Epoch 7886, Train Loss: 573.6228, Val Loss: 243.8008\n",
            "Epoch 7887, Train Loss: 607.2667, Val Loss: 267.7152\n",
            "Epoch 7888, Train Loss: 597.6404, Val Loss: 295.6105\n",
            "Epoch 7889, Train Loss: 620.2360, Val Loss: 328.3671\n",
            "Epoch 7890, Train Loss: 616.4545, Val Loss: 259.1446\n",
            "Epoch 7891, Train Loss: 625.1053, Val Loss: 242.4441\n",
            "Epoch 7892, Train Loss: 611.3171, Val Loss: 198.1562\n",
            "Epoch 7893, Train Loss: 608.9222, Val Loss: 181.7740\n",
            "Epoch 7894, Train Loss: 604.6953, Val Loss: 176.0064\n",
            "Epoch 7895, Train Loss: 611.3948, Val Loss: 184.2321\n",
            "Epoch 7896, Train Loss: 590.8223, Val Loss: 198.2788\n",
            "Epoch 7897, Train Loss: 619.6187, Val Loss: 194.5193\n",
            "Epoch 7898, Train Loss: 611.1543, Val Loss: 182.2968\n",
            "Epoch 7899, Train Loss: 597.1608, Val Loss: 175.8012\n",
            "Epoch 7900, Train Loss: 613.4370, Val Loss: 177.4949\n",
            "Epoch 7901, Train Loss: 619.3660, Val Loss: 180.7262\n",
            "Epoch 7902, Train Loss: 597.5746, Val Loss: 199.8303\n",
            "Epoch 7903, Train Loss: 602.3803, Val Loss: 223.2343\n",
            "Epoch 7904, Train Loss: 626.2499, Val Loss: 217.9499\n",
            "Epoch 7905, Train Loss: 624.4231, Val Loss: 194.5440\n",
            "Epoch 7906, Train Loss: 618.0292, Val Loss: 189.0088\n",
            "Epoch 7907, Train Loss: 620.4783, Val Loss: 186.0080\n",
            "Epoch 7908, Train Loss: 579.4866, Val Loss: 195.3137\n",
            "Epoch 7909, Train Loss: 612.5987, Val Loss: 205.6461\n",
            "Epoch 7910, Train Loss: 595.4300, Val Loss: 193.1846\n",
            "Epoch 7911, Train Loss: 610.8034, Val Loss: 193.3035\n",
            "Epoch 7912, Train Loss: 610.8160, Val Loss: 217.8350\n",
            "Epoch 7913, Train Loss: 596.3228, Val Loss: 306.6434\n",
            "Epoch 7914, Train Loss: 615.1506, Val Loss: 319.3756\n",
            "Epoch 7915, Train Loss: 593.2117, Val Loss: 300.8045\n",
            "Epoch 7916, Train Loss: 614.1924, Val Loss: 293.0355\n",
            "Epoch 7917, Train Loss: 616.4132, Val Loss: 235.5633\n",
            "Epoch 7918, Train Loss: 564.6115, Val Loss: 178.1588\n",
            "Epoch 7919, Train Loss: 592.5545, Val Loss: 185.4165\n",
            "Epoch 7920, Train Loss: 623.1656, Val Loss: 201.4489\n",
            "Epoch 7921, Train Loss: 584.7039, Val Loss: 204.2419\n",
            "Epoch 7922, Train Loss: 618.5010, Val Loss: 190.6308\n",
            "Epoch 7923, Train Loss: 613.6111, Val Loss: 171.3273\n",
            "Epoch 7924, Train Loss: 618.2202, Val Loss: 170.6617\n",
            "Epoch 7925, Train Loss: 626.2621, Val Loss: 173.9869\n",
            "Epoch 7926, Train Loss: 594.0540, Val Loss: 176.1446\n",
            "Epoch 7927, Train Loss: 551.9022, Val Loss: 208.2891\n",
            "Epoch 7928, Train Loss: 613.5156, Val Loss: 216.3940\n",
            "Epoch 7929, Train Loss: 602.8493, Val Loss: 183.0061\n",
            "Epoch 7930, Train Loss: 626.1332, Val Loss: 190.6377\n",
            "Epoch 7931, Train Loss: 622.2319, Val Loss: 210.4515\n",
            "Epoch 7932, Train Loss: 615.6178, Val Loss: 222.2999\n",
            "Epoch 7933, Train Loss: 620.6573, Val Loss: 197.1231\n",
            "Epoch 7934, Train Loss: 621.5638, Val Loss: 184.3935\n",
            "Epoch 7935, Train Loss: 614.8000, Val Loss: 179.8478\n",
            "Epoch 7936, Train Loss: 601.8339, Val Loss: 191.9775\n",
            "Epoch 7937, Train Loss: 606.4521, Val Loss: 236.7837\n",
            "Epoch 7938, Train Loss: 610.0025, Val Loss: 241.6996\n",
            "Epoch 7939, Train Loss: 616.3886, Val Loss: 203.9267\n",
            "Epoch 7940, Train Loss: 611.5023, Val Loss: 187.8467\n",
            "Epoch 7941, Train Loss: 608.3382, Val Loss: 183.2854\n",
            "Epoch 7942, Train Loss: 608.7336, Val Loss: 207.2654\n",
            "Epoch 7943, Train Loss: 632.0977, Val Loss: 215.7925\n",
            "Epoch 7944, Train Loss: 596.8465, Val Loss: 170.5448\n",
            "Epoch 7945, Train Loss: 604.7666, Val Loss: 169.4190\n",
            "Epoch 7946, Train Loss: 598.6199, Val Loss: 189.9379\n",
            "Epoch 7947, Train Loss: 562.4067, Val Loss: 226.6258\n",
            "Epoch 7948, Train Loss: 604.9425, Val Loss: 245.8896\n",
            "Epoch 7949, Train Loss: 616.2602, Val Loss: 230.8778\n",
            "Epoch 7950, Train Loss: 625.5545, Val Loss: 196.1124\n",
            "Epoch 7951, Train Loss: 606.4560, Val Loss: 201.8189\n",
            "Epoch 7952, Train Loss: 608.8501, Val Loss: 199.0736\n",
            "Epoch 7953, Train Loss: 624.3311, Val Loss: 222.4090\n",
            "Epoch 7954, Train Loss: 612.2623, Val Loss: 221.3657\n",
            "Epoch 7955, Train Loss: 622.7158, Val Loss: 185.6641\n",
            "Epoch 7956, Train Loss: 620.9283, Val Loss: 177.0826\n",
            "Epoch 7957, Train Loss: 622.2109, Val Loss: 172.8377\n",
            "Epoch 7958, Train Loss: 611.2796, Val Loss: 172.7164\n",
            "Epoch 7959, Train Loss: 568.3979, Val Loss: 177.2535\n",
            "Epoch 7960, Train Loss: 600.2776, Val Loss: 197.9935\n",
            "Epoch 7961, Train Loss: 622.5057, Val Loss: 205.3137\n",
            "Epoch 7962, Train Loss: 603.3880, Val Loss: 198.7162\n",
            "Epoch 7963, Train Loss: 622.0095, Val Loss: 207.5190\n",
            "Epoch 7964, Train Loss: 596.6082, Val Loss: 220.0182\n",
            "Epoch 7965, Train Loss: 614.1795, Val Loss: 251.6070\n",
            "Epoch 7966, Train Loss: 615.6679, Val Loss: 245.6206\n",
            "Epoch 7967, Train Loss: 627.7621, Val Loss: 190.2964\n",
            "Epoch 7968, Train Loss: 622.7812, Val Loss: 178.2906\n",
            "Epoch 7969, Train Loss: 613.2464, Val Loss: 183.3866\n",
            "Epoch 7970, Train Loss: 621.8488, Val Loss: 194.2925\n",
            "Epoch 7971, Train Loss: 569.9891, Val Loss: 258.3379\n",
            "Epoch 7972, Train Loss: 627.7633, Val Loss: 229.2680\n",
            "Epoch 7973, Train Loss: 610.7802, Val Loss: 200.1823\n",
            "Epoch 7974, Train Loss: 608.1971, Val Loss: 178.4225\n",
            "Epoch 7975, Train Loss: 622.3595, Val Loss: 180.3772\n",
            "Epoch 7976, Train Loss: 619.2648, Val Loss: 183.5133\n",
            "Epoch 7977, Train Loss: 575.8414, Val Loss: 200.3465\n",
            "Epoch 7978, Train Loss: 608.2636, Val Loss: 188.0863\n",
            "Epoch 7979, Train Loss: 593.8247, Val Loss: 185.2360\n",
            "Epoch 7980, Train Loss: 622.6283, Val Loss: 189.7840\n",
            "Epoch 7981, Train Loss: 615.1180, Val Loss: 199.1059\n",
            "Epoch 7982, Train Loss: 619.7880, Val Loss: 185.0898\n",
            "Epoch 7983, Train Loss: 579.8683, Val Loss: 178.8762\n",
            "Epoch 7984, Train Loss: 591.4952, Val Loss: 181.9850\n",
            "Epoch 7985, Train Loss: 599.9417, Val Loss: 203.1608\n",
            "Epoch 7986, Train Loss: 596.2341, Val Loss: 247.4216\n",
            "Epoch 7987, Train Loss: 610.6671, Val Loss: 293.3164\n",
            "Epoch 7988, Train Loss: 613.3049, Val Loss: 341.9560\n",
            "Epoch 7989, Train Loss: 601.3239, Val Loss: 282.5292\n",
            "Epoch 7990, Train Loss: 619.6395, Val Loss: 222.9284\n",
            "Epoch 7991, Train Loss: 607.7841, Val Loss: 193.2901\n",
            "Epoch 7992, Train Loss: 574.7355, Val Loss: 208.5364\n",
            "Epoch 7993, Train Loss: 614.4774, Val Loss: 226.4247\n",
            "Epoch 7994, Train Loss: 627.3621, Val Loss: 187.2609\n",
            "Epoch 7995, Train Loss: 587.1663, Val Loss: 188.6233\n",
            "Epoch 7996, Train Loss: 620.8644, Val Loss: 188.5832\n",
            "Epoch 7997, Train Loss: 607.3612, Val Loss: 172.6312\n",
            "Epoch 7998, Train Loss: 613.4223, Val Loss: 168.3391\n",
            "Epoch 7999, Train Loss: 621.8696, Val Loss: 170.2673\n",
            "Epoch 8000, Train Loss: 624.3102, Val Loss: 172.2488\n",
            "Epoch 8001, Train Loss: 596.7018, Val Loss: 177.2016\n",
            "Epoch 8002, Train Loss: 591.6133, Val Loss: 207.5655\n",
            "Epoch 8003, Train Loss: 614.8797, Val Loss: 288.1801\n",
            "Epoch 8004, Train Loss: 626.5857, Val Loss: 330.3039\n",
            "Epoch 8005, Train Loss: 595.7939, Val Loss: 266.4145\n",
            "Epoch 8006, Train Loss: 621.8930, Val Loss: 216.2412\n",
            "Epoch 8007, Train Loss: 620.6950, Val Loss: 181.0427\n",
            "Epoch 8008, Train Loss: 593.9441, Val Loss: 180.1380\n",
            "Epoch 8009, Train Loss: 575.5620, Val Loss: 172.7556\n",
            "Epoch 8010, Train Loss: 588.5788, Val Loss: 203.2583\n",
            "Epoch 8011, Train Loss: 633.2005, Val Loss: 210.1606\n",
            "Epoch 8012, Train Loss: 614.6409, Val Loss: 179.4325\n",
            "Epoch 8013, Train Loss: 603.9610, Val Loss: 174.0249\n",
            "Epoch 8014, Train Loss: 593.1302, Val Loss: 176.1876\n",
            "Epoch 8015, Train Loss: 619.6297, Val Loss: 173.4522\n",
            "Epoch 8016, Train Loss: 616.6788, Val Loss: 179.5284\n",
            "Epoch 8017, Train Loss: 596.3381, Val Loss: 258.1106\n",
            "Epoch 8018, Train Loss: 580.9575, Val Loss: 269.3525\n",
            "Epoch 8019, Train Loss: 575.1155, Val Loss: 296.6956\n",
            "Epoch 8020, Train Loss: 596.5753, Val Loss: 363.4750\n",
            "Epoch 8021, Train Loss: 615.6249, Val Loss: 344.6069\n",
            "Epoch 8022, Train Loss: 620.5888, Val Loss: 235.0499\n",
            "Epoch 8023, Train Loss: 604.2785, Val Loss: 252.2054\n",
            "Epoch 8024, Train Loss: 595.7459, Val Loss: 252.7760\n",
            "Epoch 8025, Train Loss: 589.6353, Val Loss: 218.4653\n",
            "Epoch 8026, Train Loss: 612.5251, Val Loss: 186.4546\n",
            "Epoch 8027, Train Loss: 623.5330, Val Loss: 177.8092\n",
            "Epoch 8028, Train Loss: 627.9956, Val Loss: 176.7778\n",
            "Epoch 8029, Train Loss: 572.3039, Val Loss: 174.7373\n",
            "Epoch 8030, Train Loss: 614.3305, Val Loss: 171.9660\n",
            "Epoch 8031, Train Loss: 580.5823, Val Loss: 175.3091\n",
            "Epoch 8032, Train Loss: 517.9897, Val Loss: 190.8248\n",
            "Epoch 8033, Train Loss: 604.6735, Val Loss: 247.9463\n",
            "Epoch 8034, Train Loss: 607.5226, Val Loss: 316.1122\n",
            "Epoch 8035, Train Loss: 603.3654, Val Loss: 253.8043\n",
            "Epoch 8036, Train Loss: 617.7880, Val Loss: 189.3658\n",
            "Epoch 8037, Train Loss: 605.9205, Val Loss: 177.4126\n",
            "Epoch 8038, Train Loss: 613.1895, Val Loss: 180.5309\n",
            "Epoch 8039, Train Loss: 615.3732, Val Loss: 203.4394\n",
            "Epoch 8040, Train Loss: 613.3782, Val Loss: 199.3779\n",
            "Epoch 8041, Train Loss: 587.3406, Val Loss: 190.2237\n",
            "Epoch 8042, Train Loss: 617.5801, Val Loss: 176.3044\n",
            "Epoch 8043, Train Loss: 601.9213, Val Loss: 173.0059\n",
            "Epoch 8044, Train Loss: 601.3002, Val Loss: 174.5095\n",
            "Epoch 8045, Train Loss: 627.8507, Val Loss: 174.5639\n",
            "Epoch 8046, Train Loss: 582.2885, Val Loss: 177.3365\n",
            "Epoch 8047, Train Loss: 593.1927, Val Loss: 173.7289\n",
            "Epoch 8048, Train Loss: 587.4726, Val Loss: 172.0287\n",
            "Epoch 8049, Train Loss: 601.4152, Val Loss: 199.0659\n",
            "Epoch 8050, Train Loss: 583.2492, Val Loss: 278.6087\n",
            "Epoch 8051, Train Loss: 620.4626, Val Loss: 331.8996\n",
            "Epoch 8052, Train Loss: 623.2028, Val Loss: 297.8146\n",
            "Epoch 8053, Train Loss: 599.4097, Val Loss: 231.8360\n",
            "Epoch 8054, Train Loss: 616.7770, Val Loss: 237.1963\n",
            "Epoch 8055, Train Loss: 601.2085, Val Loss: 246.8014\n",
            "Epoch 8056, Train Loss: 608.4898, Val Loss: 202.6392\n",
            "Epoch 8057, Train Loss: 615.4965, Val Loss: 178.0863\n",
            "Epoch 8058, Train Loss: 609.0176, Val Loss: 173.9529\n",
            "Epoch 8059, Train Loss: 621.3248, Val Loss: 179.1262\n",
            "Epoch 8060, Train Loss: 605.3416, Val Loss: 208.6757\n",
            "Epoch 8061, Train Loss: 614.6260, Val Loss: 225.8057\n",
            "Epoch 8062, Train Loss: 616.4654, Val Loss: 199.2003\n",
            "Epoch 8063, Train Loss: 613.8814, Val Loss: 190.4682\n",
            "Epoch 8064, Train Loss: 620.0526, Val Loss: 205.8653\n",
            "Epoch 8065, Train Loss: 610.7917, Val Loss: 203.2158\n",
            "Epoch 8066, Train Loss: 612.8259, Val Loss: 199.9443\n",
            "Epoch 8067, Train Loss: 625.4934, Val Loss: 190.5301\n",
            "Epoch 8068, Train Loss: 602.6549, Val Loss: 185.2398\n",
            "Epoch 8069, Train Loss: 596.8488, Val Loss: 189.7760\n",
            "Epoch 8070, Train Loss: 587.8919, Val Loss: 205.8070\n",
            "Epoch 8071, Train Loss: 613.5255, Val Loss: 211.4833\n",
            "Epoch 8072, Train Loss: 600.5399, Val Loss: 207.6076\n",
            "Epoch 8073, Train Loss: 616.3974, Val Loss: 186.6875\n",
            "Epoch 8074, Train Loss: 577.9657, Val Loss: 222.1904\n",
            "Epoch 8075, Train Loss: 610.1544, Val Loss: 292.1807\n",
            "Epoch 8076, Train Loss: 576.8405, Val Loss: 253.6813\n",
            "Epoch 8077, Train Loss: 612.8997, Val Loss: 181.5478\n",
            "Epoch 8078, Train Loss: 599.8988, Val Loss: 173.8892\n",
            "Epoch 8079, Train Loss: 555.3555, Val Loss: 239.5048\n",
            "Epoch 8080, Train Loss: 611.8744, Val Loss: 465.4281\n",
            "Epoch 8081, Train Loss: 547.3145, Val Loss: 385.2311\n",
            "Epoch 8082, Train Loss: 612.6484, Val Loss: 227.1791\n",
            "Epoch 8083, Train Loss: 618.4059, Val Loss: 189.0312\n",
            "Epoch 8084, Train Loss: 557.9427, Val Loss: 225.8248\n",
            "Epoch 8085, Train Loss: 621.3460, Val Loss: 313.8217\n",
            "Epoch 8086, Train Loss: 624.6468, Val Loss: 340.3694\n",
            "Epoch 8087, Train Loss: 601.0607, Val Loss: 221.6901\n",
            "Epoch 8088, Train Loss: 619.0181, Val Loss: 190.7835\n",
            "Epoch 8089, Train Loss: 623.6976, Val Loss: 197.5562\n",
            "Epoch 8090, Train Loss: 610.8526, Val Loss: 197.5258\n",
            "Epoch 8091, Train Loss: 597.0527, Val Loss: 182.5595\n",
            "Epoch 8092, Train Loss: 614.2473, Val Loss: 180.1942\n",
            "Epoch 8093, Train Loss: 615.5558, Val Loss: 181.0305\n",
            "Epoch 8094, Train Loss: 558.9626, Val Loss: 175.2940\n",
            "Epoch 8095, Train Loss: 591.1643, Val Loss: 200.8498\n",
            "Epoch 8096, Train Loss: 622.8096, Val Loss: 204.1764\n",
            "Epoch 8097, Train Loss: 623.4340, Val Loss: 199.3310\n",
            "Epoch 8098, Train Loss: 616.4127, Val Loss: 197.9795\n",
            "Epoch 8099, Train Loss: 615.9678, Val Loss: 203.9131\n",
            "Epoch 8100, Train Loss: 561.9359, Val Loss: 179.5721\n",
            "Epoch 8101, Train Loss: 579.9884, Val Loss: 176.0487\n",
            "Epoch 8102, Train Loss: 623.9195, Val Loss: 177.6825\n",
            "Epoch 8103, Train Loss: 624.4654, Val Loss: 209.9944\n",
            "Epoch 8104, Train Loss: 622.1477, Val Loss: 230.4193\n",
            "Epoch 8105, Train Loss: 585.4704, Val Loss: 254.4502\n",
            "Epoch 8106, Train Loss: 594.6774, Val Loss: 282.7188\n",
            "Epoch 8107, Train Loss: 596.1002, Val Loss: 265.0595\n",
            "Epoch 8108, Train Loss: 614.9005, Val Loss: 194.8109\n",
            "Epoch 8109, Train Loss: 625.0989, Val Loss: 187.1876\n",
            "Epoch 8110, Train Loss: 607.3450, Val Loss: 192.4625\n",
            "Epoch 8111, Train Loss: 589.9829, Val Loss: 232.2454\n",
            "Epoch 8112, Train Loss: 581.3881, Val Loss: 219.2410\n",
            "Epoch 8113, Train Loss: 610.9189, Val Loss: 170.2402\n",
            "Epoch 8114, Train Loss: 612.0433, Val Loss: 173.8342\n",
            "Epoch 8115, Train Loss: 608.0933, Val Loss: 182.4576\n",
            "Epoch 8116, Train Loss: 608.8039, Val Loss: 192.6413\n",
            "Epoch 8117, Train Loss: 585.3299, Val Loss: 194.2326\n",
            "Epoch 8118, Train Loss: 613.6589, Val Loss: 192.0522\n",
            "Epoch 8119, Train Loss: 624.7888, Val Loss: 184.2306\n",
            "Epoch 8120, Train Loss: 565.4202, Val Loss: 229.8215\n",
            "Epoch 8121, Train Loss: 623.0296, Val Loss: 270.1826\n",
            "Epoch 8122, Train Loss: 624.6502, Val Loss: 293.4804\n",
            "Epoch 8123, Train Loss: 616.9863, Val Loss: 308.4168\n",
            "Epoch 8124, Train Loss: 623.5494, Val Loss: 300.7261\n",
            "Epoch 8125, Train Loss: 590.9413, Val Loss: 247.3057\n",
            "Epoch 8126, Train Loss: 577.3959, Val Loss: 190.7921\n",
            "Epoch 8127, Train Loss: 601.5098, Val Loss: 175.9277\n",
            "Epoch 8128, Train Loss: 608.9294, Val Loss: 175.9419\n",
            "Epoch 8129, Train Loss: 614.1353, Val Loss: 219.3104\n",
            "Epoch 8130, Train Loss: 602.9267, Val Loss: 219.3594\n",
            "Epoch 8131, Train Loss: 627.0248, Val Loss: 200.6213\n",
            "Epoch 8132, Train Loss: 619.0853, Val Loss: 181.9773\n",
            "Epoch 8133, Train Loss: 589.2873, Val Loss: 180.7259\n",
            "Epoch 8134, Train Loss: 608.0673, Val Loss: 181.6656\n",
            "Epoch 8135, Train Loss: 595.4520, Val Loss: 184.3734\n",
            "Epoch 8136, Train Loss: 626.7683, Val Loss: 177.8698\n",
            "Epoch 8137, Train Loss: 612.1041, Val Loss: 175.4885\n",
            "Epoch 8138, Train Loss: 615.8907, Val Loss: 186.3195\n",
            "Epoch 8139, Train Loss: 616.7716, Val Loss: 204.8213\n",
            "Epoch 8140, Train Loss: 628.2666, Val Loss: 188.1370\n",
            "Epoch 8141, Train Loss: 592.4904, Val Loss: 188.0636\n",
            "Epoch 8142, Train Loss: 623.3606, Val Loss: 203.9408\n",
            "Epoch 8143, Train Loss: 616.6593, Val Loss: 199.7038\n",
            "Epoch 8144, Train Loss: 612.4504, Val Loss: 193.6165\n",
            "Epoch 8145, Train Loss: 593.7598, Val Loss: 223.7361\n",
            "Epoch 8146, Train Loss: 590.2000, Val Loss: 272.5371\n",
            "Epoch 8147, Train Loss: 623.6047, Val Loss: 280.3730\n",
            "Epoch 8148, Train Loss: 614.5970, Val Loss: 272.3189\n",
            "Epoch 8149, Train Loss: 596.0795, Val Loss: 183.2692\n",
            "Epoch 8150, Train Loss: 620.0275, Val Loss: 169.9127\n",
            "Epoch 8151, Train Loss: 598.8920, Val Loss: 171.6319\n",
            "Epoch 8152, Train Loss: 609.3849, Val Loss: 176.8919\n",
            "Epoch 8153, Train Loss: 605.1622, Val Loss: 181.7935\n",
            "Epoch 8154, Train Loss: 611.8412, Val Loss: 195.6100\n",
            "Epoch 8155, Train Loss: 604.6010, Val Loss: 228.6460\n",
            "Epoch 8156, Train Loss: 611.0133, Val Loss: 183.3278\n",
            "Epoch 8157, Train Loss: 589.8122, Val Loss: 176.1175\n",
            "Epoch 8158, Train Loss: 619.7453, Val Loss: 177.1315\n",
            "Epoch 8159, Train Loss: 620.3110, Val Loss: 179.4316\n",
            "Epoch 8160, Train Loss: 623.6120, Val Loss: 179.7830\n",
            "Epoch 8161, Train Loss: 597.1415, Val Loss: 180.7399\n",
            "Epoch 8162, Train Loss: 595.5655, Val Loss: 180.1987\n",
            "Epoch 8163, Train Loss: 611.1246, Val Loss: 212.2225\n",
            "Epoch 8164, Train Loss: 596.3904, Val Loss: 244.2468\n",
            "Epoch 8165, Train Loss: 621.0495, Val Loss: 271.9288\n",
            "Epoch 8166, Train Loss: 622.5180, Val Loss: 240.9252\n",
            "Epoch 8167, Train Loss: 612.7212, Val Loss: 187.1695\n",
            "Epoch 8168, Train Loss: 602.9065, Val Loss: 181.9693\n",
            "Epoch 8169, Train Loss: 607.6023, Val Loss: 209.1410\n",
            "Epoch 8170, Train Loss: 625.6647, Val Loss: 208.7666\n",
            "Epoch 8171, Train Loss: 625.8070, Val Loss: 184.9382\n",
            "Epoch 8172, Train Loss: 596.6081, Val Loss: 179.7868\n",
            "Epoch 8173, Train Loss: 620.7130, Val Loss: 210.4549\n",
            "Epoch 8174, Train Loss: 598.0702, Val Loss: 230.3751\n",
            "Epoch 8175, Train Loss: 594.8065, Val Loss: 200.3308\n",
            "Epoch 8176, Train Loss: 601.9059, Val Loss: 195.9041\n",
            "Epoch 8177, Train Loss: 621.2377, Val Loss: 241.5869\n",
            "Epoch 8178, Train Loss: 586.4810, Val Loss: 299.4576\n",
            "Epoch 8179, Train Loss: 621.9768, Val Loss: 182.7174\n",
            "Epoch 8180, Train Loss: 604.9673, Val Loss: 178.1233\n",
            "Epoch 8181, Train Loss: 602.8112, Val Loss: 224.5180\n",
            "Epoch 8182, Train Loss: 619.9514, Val Loss: 338.1669\n",
            "Epoch 8183, Train Loss: 624.2099, Val Loss: 312.0670\n",
            "Epoch 8184, Train Loss: 613.7688, Val Loss: 247.4585\n",
            "Epoch 8185, Train Loss: 570.8016, Val Loss: 225.4143\n",
            "Epoch 8186, Train Loss: 622.5938, Val Loss: 224.6848\n",
            "Epoch 8187, Train Loss: 628.3443, Val Loss: 213.7974\n",
            "Epoch 8188, Train Loss: 562.7228, Val Loss: 203.4061\n",
            "Epoch 8189, Train Loss: 577.1687, Val Loss: 187.9142\n",
            "Epoch 8190, Train Loss: 627.7300, Val Loss: 179.2970\n",
            "Epoch 8191, Train Loss: 571.3032, Val Loss: 174.2823\n",
            "Epoch 8192, Train Loss: 608.0824, Val Loss: 175.7415\n",
            "Epoch 8193, Train Loss: 586.3196, Val Loss: 174.6340\n",
            "Epoch 8194, Train Loss: 584.5591, Val Loss: 176.3838\n",
            "Epoch 8195, Train Loss: 557.3029, Val Loss: 331.6436\n",
            "Epoch 8196, Train Loss: 606.2330, Val Loss: 345.7658\n",
            "Epoch 8197, Train Loss: 599.5560, Val Loss: 314.4692\n",
            "Epoch 8198, Train Loss: 588.5068, Val Loss: 222.5790\n",
            "Epoch 8199, Train Loss: 620.9193, Val Loss: 173.4821\n",
            "Epoch 8200, Train Loss: 621.3393, Val Loss: 171.6931\n",
            "Epoch 8201, Train Loss: 617.7879, Val Loss: 177.4049\n",
            "Epoch 8202, Train Loss: 627.6383, Val Loss: 193.8706\n",
            "Epoch 8203, Train Loss: 627.1723, Val Loss: 188.2048\n",
            "Epoch 8204, Train Loss: 603.3081, Val Loss: 187.1322\n",
            "Epoch 8205, Train Loss: 617.3315, Val Loss: 191.3574\n",
            "Epoch 8206, Train Loss: 577.4149, Val Loss: 197.7102\n",
            "Epoch 8207, Train Loss: 612.6142, Val Loss: 232.9187\n",
            "Epoch 8208, Train Loss: 616.0778, Val Loss: 254.7757\n",
            "Epoch 8209, Train Loss: 591.1198, Val Loss: 225.3161\n",
            "Epoch 8210, Train Loss: 626.1337, Val Loss: 195.2768\n",
            "Epoch 8211, Train Loss: 616.0161, Val Loss: 189.7006\n",
            "Epoch 8212, Train Loss: 569.8985, Val Loss: 206.4932\n",
            "Epoch 8213, Train Loss: 617.0673, Val Loss: 198.9335\n",
            "Epoch 8214, Train Loss: 614.0876, Val Loss: 199.5778\n",
            "Epoch 8215, Train Loss: 560.9204, Val Loss: 186.0397\n",
            "Epoch 8216, Train Loss: 614.9023, Val Loss: 175.7018\n",
            "Epoch 8217, Train Loss: 619.6867, Val Loss: 178.5311\n",
            "Epoch 8218, Train Loss: 611.6736, Val Loss: 201.3384\n",
            "Epoch 8219, Train Loss: 629.4102, Val Loss: 219.2912\n",
            "Epoch 8220, Train Loss: 604.9504, Val Loss: 201.8499\n",
            "Epoch 8221, Train Loss: 583.6214, Val Loss: 203.4390\n",
            "Epoch 8222, Train Loss: 609.2530, Val Loss: 211.1848\n",
            "Epoch 8223, Train Loss: 630.9056, Val Loss: 210.9804\n",
            "Epoch 8224, Train Loss: 562.3899, Val Loss: 264.8764\n",
            "Epoch 8225, Train Loss: 596.7172, Val Loss: 291.4726\n",
            "Epoch 8226, Train Loss: 626.4714, Val Loss: 171.2538\n",
            "Epoch 8227, Train Loss: 583.8487, Val Loss: 176.7213\n",
            "Epoch 8228, Train Loss: 607.1404, Val Loss: 180.0632\n",
            "Epoch 8229, Train Loss: 605.9003, Val Loss: 314.1202\n",
            "Epoch 8230, Train Loss: 540.1279, Val Loss: 353.5987\n",
            "Epoch 8231, Train Loss: 623.4009, Val Loss: 337.0905\n",
            "Epoch 8232, Train Loss: 596.9513, Val Loss: 205.9774\n",
            "Epoch 8233, Train Loss: 585.2795, Val Loss: 198.7472\n",
            "Epoch 8234, Train Loss: 629.2019, Val Loss: 229.8799\n",
            "Epoch 8235, Train Loss: 615.9011, Val Loss: 242.1985\n",
            "Epoch 8236, Train Loss: 561.6510, Val Loss: 249.2360\n",
            "Epoch 8237, Train Loss: 568.1910, Val Loss: 182.1810\n",
            "Epoch 8238, Train Loss: 626.3721, Val Loss: 178.4413\n",
            "Epoch 8239, Train Loss: 601.6464, Val Loss: 174.3188\n",
            "Epoch 8240, Train Loss: 619.0641, Val Loss: 189.0254\n",
            "Epoch 8241, Train Loss: 611.5810, Val Loss: 205.2077\n",
            "Epoch 8242, Train Loss: 606.6476, Val Loss: 216.5723\n",
            "Epoch 8243, Train Loss: 613.8876, Val Loss: 189.7686\n",
            "Epoch 8244, Train Loss: 612.3080, Val Loss: 181.6847\n",
            "Epoch 8245, Train Loss: 603.2700, Val Loss: 187.6796\n",
            "Epoch 8246, Train Loss: 607.6411, Val Loss: 199.3749\n",
            "Epoch 8247, Train Loss: 613.0714, Val Loss: 207.9241\n",
            "Epoch 8248, Train Loss: 607.9849, Val Loss: 208.6869\n",
            "Epoch 8249, Train Loss: 629.5340, Val Loss: 183.7908\n",
            "Epoch 8250, Train Loss: 615.2789, Val Loss: 174.9105\n",
            "Epoch 8251, Train Loss: 544.4515, Val Loss: 181.0449\n",
            "Epoch 8252, Train Loss: 624.1915, Val Loss: 234.4378\n",
            "Epoch 8253, Train Loss: 627.9695, Val Loss: 241.9872\n",
            "Epoch 8254, Train Loss: 598.8458, Val Loss: 205.8646\n",
            "Epoch 8255, Train Loss: 622.1634, Val Loss: 194.8574\n",
            "Epoch 8256, Train Loss: 582.3481, Val Loss: 236.4982\n",
            "Epoch 8257, Train Loss: 580.0515, Val Loss: 256.3094\n",
            "Epoch 8258, Train Loss: 625.8796, Val Loss: 202.9178\n",
            "Epoch 8259, Train Loss: 609.6146, Val Loss: 179.3158\n",
            "Epoch 8260, Train Loss: 626.0911, Val Loss: 175.3137\n",
            "Epoch 8261, Train Loss: 616.1777, Val Loss: 175.2545\n",
            "Epoch 8262, Train Loss: 607.8965, Val Loss: 182.0172\n",
            "Epoch 8263, Train Loss: 611.5627, Val Loss: 188.6908\n",
            "Epoch 8264, Train Loss: 589.9836, Val Loss: 194.7724\n",
            "Epoch 8265, Train Loss: 618.3694, Val Loss: 185.1974\n",
            "Epoch 8266, Train Loss: 599.9833, Val Loss: 175.1750\n",
            "Epoch 8267, Train Loss: 628.3650, Val Loss: 173.2741\n",
            "Epoch 8268, Train Loss: 603.3105, Val Loss: 196.1595\n",
            "Epoch 8269, Train Loss: 613.8661, Val Loss: 198.7400\n",
            "Epoch 8270, Train Loss: 619.2649, Val Loss: 189.4433\n",
            "Epoch 8271, Train Loss: 580.9808, Val Loss: 221.7804\n",
            "Epoch 8272, Train Loss: 617.5044, Val Loss: 265.8942\n",
            "Epoch 8273, Train Loss: 601.9776, Val Loss: 244.8363\n",
            "Epoch 8274, Train Loss: 623.2529, Val Loss: 185.8414\n",
            "Epoch 8275, Train Loss: 627.8348, Val Loss: 175.7454\n",
            "Epoch 8276, Train Loss: 612.9872, Val Loss: 181.8957\n",
            "Epoch 8277, Train Loss: 606.0968, Val Loss: 180.3997\n",
            "Epoch 8278, Train Loss: 597.6853, Val Loss: 286.9628\n",
            "Epoch 8279, Train Loss: 614.8334, Val Loss: 295.5762\n",
            "Epoch 8280, Train Loss: 621.1371, Val Loss: 247.9711\n",
            "Epoch 8281, Train Loss: 609.0571, Val Loss: 241.7146\n",
            "Epoch 8282, Train Loss: 598.9381, Val Loss: 184.4192\n",
            "Epoch 8283, Train Loss: 610.4876, Val Loss: 187.9659\n",
            "Epoch 8284, Train Loss: 608.3376, Val Loss: 193.8054\n",
            "Epoch 8285, Train Loss: 615.4465, Val Loss: 171.8414\n",
            "Epoch 8286, Train Loss: 620.0847, Val Loss: 171.6789\n",
            "Epoch 8287, Train Loss: 622.8474, Val Loss: 173.8121\n",
            "Epoch 8288, Train Loss: 593.2899, Val Loss: 174.4611\n",
            "Epoch 8289, Train Loss: 604.2004, Val Loss: 226.5475\n",
            "Epoch 8290, Train Loss: 564.3902, Val Loss: 350.0628\n",
            "Epoch 8291, Train Loss: 584.4213, Val Loss: 390.8794\n",
            "Epoch 8292, Train Loss: 608.3715, Val Loss: 359.3381\n",
            "Epoch 8293, Train Loss: 617.9445, Val Loss: 298.8610\n",
            "Epoch 8294, Train Loss: 625.2560, Val Loss: 196.4907\n",
            "Epoch 8295, Train Loss: 616.9374, Val Loss: 195.4513\n",
            "Epoch 8296, Train Loss: 577.5294, Val Loss: 222.1988\n",
            "Epoch 8297, Train Loss: 586.2133, Val Loss: 244.7615\n",
            "Epoch 8298, Train Loss: 626.1409, Val Loss: 223.9250\n",
            "Epoch 8299, Train Loss: 575.3986, Val Loss: 183.8783\n",
            "Epoch 8300, Train Loss: 620.8193, Val Loss: 183.6644\n",
            "Epoch 8301, Train Loss: 619.0333, Val Loss: 181.5362\n",
            "Epoch 8302, Train Loss: 565.1536, Val Loss: 183.7028\n",
            "Epoch 8303, Train Loss: 628.0383, Val Loss: 185.7427\n",
            "Epoch 8304, Train Loss: 611.7661, Val Loss: 183.3499\n",
            "Epoch 8305, Train Loss: 543.6416, Val Loss: 191.4673\n",
            "Epoch 8306, Train Loss: 602.7121, Val Loss: 248.4847\n",
            "Epoch 8307, Train Loss: 606.8818, Val Loss: 258.0266\n",
            "Epoch 8308, Train Loss: 615.8507, Val Loss: 206.3218\n",
            "Epoch 8309, Train Loss: 624.7955, Val Loss: 193.5897\n",
            "Epoch 8310, Train Loss: 617.2685, Val Loss: 191.1447\n",
            "Epoch 8311, Train Loss: 610.4637, Val Loss: 194.9799\n",
            "Epoch 8312, Train Loss: 588.0506, Val Loss: 186.2720\n",
            "Epoch 8313, Train Loss: 611.8094, Val Loss: 176.9903\n",
            "Epoch 8314, Train Loss: 625.7853, Val Loss: 187.1799\n",
            "Epoch 8315, Train Loss: 610.9006, Val Loss: 210.4800\n",
            "Epoch 8316, Train Loss: 622.3185, Val Loss: 212.7614\n",
            "Epoch 8317, Train Loss: 600.3369, Val Loss: 211.1881\n",
            "Epoch 8318, Train Loss: 620.8586, Val Loss: 207.8217\n",
            "Epoch 8319, Train Loss: 623.9880, Val Loss: 200.4007\n",
            "Epoch 8320, Train Loss: 609.3690, Val Loss: 186.8250\n",
            "Epoch 8321, Train Loss: 594.3970, Val Loss: 182.9540\n",
            "Epoch 8322, Train Loss: 609.7139, Val Loss: 193.9964\n",
            "Epoch 8323, Train Loss: 623.1594, Val Loss: 203.3566\n",
            "Epoch 8324, Train Loss: 597.7169, Val Loss: 201.3003\n",
            "Epoch 8325, Train Loss: 620.9919, Val Loss: 197.3586\n",
            "Epoch 8326, Train Loss: 623.5684, Val Loss: 192.2403\n",
            "Epoch 8327, Train Loss: 600.5698, Val Loss: 180.1510\n",
            "Epoch 8328, Train Loss: 620.2801, Val Loss: 177.4305\n",
            "Epoch 8329, Train Loss: 598.5234, Val Loss: 199.1897\n",
            "Epoch 8330, Train Loss: 618.8199, Val Loss: 218.8682\n",
            "Epoch 8331, Train Loss: 600.7088, Val Loss: 206.3770\n",
            "Epoch 8332, Train Loss: 615.6402, Val Loss: 186.7003\n",
            "Epoch 8333, Train Loss: 604.9803, Val Loss: 177.1304\n",
            "Epoch 8334, Train Loss: 624.2789, Val Loss: 180.0471\n",
            "Epoch 8335, Train Loss: 622.1569, Val Loss: 203.3104\n",
            "Epoch 8336, Train Loss: 607.0079, Val Loss: 241.7744\n",
            "Epoch 8337, Train Loss: 624.0837, Val Loss: 268.7363\n",
            "Epoch 8338, Train Loss: 605.6929, Val Loss: 217.6219\n",
            "Epoch 8339, Train Loss: 600.2082, Val Loss: 203.4422\n",
            "Epoch 8340, Train Loss: 616.0360, Val Loss: 196.9499\n",
            "Epoch 8341, Train Loss: 587.6783, Val Loss: 198.4359\n",
            "Epoch 8342, Train Loss: 585.4674, Val Loss: 238.7917\n",
            "Epoch 8343, Train Loss: 602.8765, Val Loss: 277.8495\n",
            "Epoch 8344, Train Loss: 622.0415, Val Loss: 245.4091\n",
            "Epoch 8345, Train Loss: 615.1984, Val Loss: 225.5716\n",
            "Epoch 8346, Train Loss: 618.2581, Val Loss: 183.0532\n",
            "Epoch 8347, Train Loss: 556.7298, Val Loss: 179.3564\n",
            "Epoch 8348, Train Loss: 611.9157, Val Loss: 207.8534\n",
            "Epoch 8349, Train Loss: 572.8401, Val Loss: 233.9498\n",
            "Epoch 8350, Train Loss: 597.2532, Val Loss: 205.7193\n",
            "Epoch 8351, Train Loss: 566.9846, Val Loss: 184.7766\n",
            "Epoch 8352, Train Loss: 613.9958, Val Loss: 187.8703\n",
            "Epoch 8353, Train Loss: 627.8012, Val Loss: 241.2034\n",
            "Epoch 8354, Train Loss: 612.1606, Val Loss: 327.1235\n",
            "Epoch 8355, Train Loss: 629.3234, Val Loss: 275.1783\n",
            "Epoch 8356, Train Loss: 606.0602, Val Loss: 178.8888\n",
            "Epoch 8357, Train Loss: 619.4313, Val Loss: 180.4126\n",
            "Epoch 8358, Train Loss: 578.3461, Val Loss: 173.4267\n",
            "Epoch 8359, Train Loss: 617.7039, Val Loss: 257.0023\n",
            "Epoch 8360, Train Loss: 616.9330, Val Loss: 258.8710\n",
            "Epoch 8361, Train Loss: 605.5460, Val Loss: 204.6960\n",
            "Epoch 8362, Train Loss: 590.4490, Val Loss: 176.5862\n",
            "Epoch 8363, Train Loss: 554.4722, Val Loss: 188.9149\n",
            "Epoch 8364, Train Loss: 530.5192, Val Loss: 289.4168\n",
            "Epoch 8365, Train Loss: 571.4257, Val Loss: 277.3127\n",
            "Epoch 8366, Train Loss: 616.7742, Val Loss: 228.1996\n",
            "Epoch 8367, Train Loss: 623.5853, Val Loss: 212.4042\n",
            "Epoch 8368, Train Loss: 604.7726, Val Loss: 186.0978\n",
            "Epoch 8369, Train Loss: 616.0772, Val Loss: 172.0730\n",
            "Epoch 8370, Train Loss: 597.0419, Val Loss: 173.2324\n",
            "Epoch 8371, Train Loss: 619.6227, Val Loss: 177.3675\n",
            "Epoch 8372, Train Loss: 623.8833, Val Loss: 174.7523\n",
            "Epoch 8373, Train Loss: 584.9855, Val Loss: 175.7362\n",
            "Epoch 8374, Train Loss: 624.5678, Val Loss: 178.8351\n",
            "Epoch 8375, Train Loss: 613.5618, Val Loss: 185.8467\n",
            "Epoch 8376, Train Loss: 611.0560, Val Loss: 207.8564\n",
            "Epoch 8377, Train Loss: 615.4149, Val Loss: 217.6498\n",
            "Epoch 8378, Train Loss: 628.1462, Val Loss: 212.5730\n",
            "Epoch 8379, Train Loss: 602.8672, Val Loss: 237.0977\n",
            "Epoch 8380, Train Loss: 578.1791, Val Loss: 287.1677\n",
            "Epoch 8381, Train Loss: 573.0432, Val Loss: 233.3116\n",
            "Epoch 8382, Train Loss: 620.2691, Val Loss: 182.5727\n",
            "Epoch 8383, Train Loss: 622.0962, Val Loss: 187.9197\n",
            "Epoch 8384, Train Loss: 569.0484, Val Loss: 169.4418\n",
            "Epoch 8385, Train Loss: 625.1271, Val Loss: 374.9401\n",
            "Epoch 8386, Train Loss: 624.7616, Val Loss: 414.7376\n",
            "Epoch 8387, Train Loss: 615.7872, Val Loss: 320.5629\n",
            "Epoch 8388, Train Loss: 590.2430, Val Loss: 233.7197\n",
            "Epoch 8389, Train Loss: 619.0691, Val Loss: 215.0346\n",
            "Epoch 8390, Train Loss: 595.9719, Val Loss: 199.7421\n",
            "Epoch 8391, Train Loss: 601.8880, Val Loss: 242.7598\n",
            "Epoch 8392, Train Loss: 536.2146, Val Loss: 231.4642\n",
            "Epoch 8393, Train Loss: 605.6360, Val Loss: 188.0480\n",
            "Epoch 8394, Train Loss: 591.0438, Val Loss: 182.7072\n",
            "Epoch 8395, Train Loss: 601.8798, Val Loss: 204.3201\n",
            "Epoch 8396, Train Loss: 630.9264, Val Loss: 209.6838\n",
            "Epoch 8397, Train Loss: 517.2195, Val Loss: 216.5964\n",
            "Epoch 8398, Train Loss: 588.7569, Val Loss: 180.8979\n",
            "Epoch 8399, Train Loss: 583.7998, Val Loss: 178.9172\n",
            "Epoch 8400, Train Loss: 594.7622, Val Loss: 179.3350\n",
            "Epoch 8401, Train Loss: 600.7202, Val Loss: 182.3343\n",
            "Epoch 8402, Train Loss: 584.1175, Val Loss: 220.4748\n",
            "Epoch 8403, Train Loss: 610.5222, Val Loss: 209.1094\n",
            "Epoch 8404, Train Loss: 621.8894, Val Loss: 192.1181\n",
            "Epoch 8405, Train Loss: 601.3395, Val Loss: 179.7350\n",
            "Epoch 8406, Train Loss: 620.4106, Val Loss: 178.4490\n",
            "Epoch 8407, Train Loss: 624.7595, Val Loss: 179.8040\n",
            "Epoch 8408, Train Loss: 605.1799, Val Loss: 189.3865\n",
            "Epoch 8409, Train Loss: 624.3195, Val Loss: 186.3518\n",
            "Epoch 8410, Train Loss: 585.0770, Val Loss: 198.7121\n",
            "Epoch 8411, Train Loss: 622.2857, Val Loss: 200.1648\n",
            "Epoch 8412, Train Loss: 627.9237, Val Loss: 193.1159\n",
            "Epoch 8413, Train Loss: 614.5396, Val Loss: 181.7024\n",
            "Epoch 8414, Train Loss: 569.9067, Val Loss: 180.3072\n",
            "Epoch 8415, Train Loss: 593.2580, Val Loss: 184.2841\n",
            "Epoch 8416, Train Loss: 624.5847, Val Loss: 195.3853\n",
            "Epoch 8417, Train Loss: 605.6301, Val Loss: 202.4976\n",
            "Epoch 8418, Train Loss: 614.2470, Val Loss: 209.2023\n",
            "Epoch 8419, Train Loss: 614.5315, Val Loss: 229.4235\n",
            "Epoch 8420, Train Loss: 595.5861, Val Loss: 243.9271\n",
            "Epoch 8421, Train Loss: 624.1375, Val Loss: 218.8610\n",
            "Epoch 8422, Train Loss: 606.3966, Val Loss: 185.9150\n",
            "Epoch 8423, Train Loss: 611.1029, Val Loss: 182.6101\n",
            "Epoch 8424, Train Loss: 615.8218, Val Loss: 189.9835\n",
            "Epoch 8425, Train Loss: 605.8610, Val Loss: 189.7795\n",
            "Epoch 8426, Train Loss: 599.0808, Val Loss: 178.7884\n",
            "Epoch 8427, Train Loss: 616.7963, Val Loss: 180.8803\n",
            "Epoch 8428, Train Loss: 622.5584, Val Loss: 199.5045\n",
            "Epoch 8429, Train Loss: 620.2887, Val Loss: 207.3105\n",
            "Epoch 8430, Train Loss: 598.7865, Val Loss: 186.5295\n",
            "Epoch 8431, Train Loss: 619.6326, Val Loss: 182.5222\n",
            "Epoch 8432, Train Loss: 614.3738, Val Loss: 189.8025\n",
            "Epoch 8433, Train Loss: 592.3983, Val Loss: 221.9404\n",
            "Epoch 8434, Train Loss: 607.9413, Val Loss: 195.2621\n",
            "Epoch 8435, Train Loss: 614.2304, Val Loss: 176.7077\n",
            "Epoch 8436, Train Loss: 616.3731, Val Loss: 172.7663\n",
            "Epoch 8437, Train Loss: 608.2784, Val Loss: 195.7215\n",
            "Epoch 8438, Train Loss: 545.7008, Val Loss: 500.2333\n",
            "Epoch 8439, Train Loss: 619.2839, Val Loss: 487.2369\n",
            "Epoch 8440, Train Loss: 627.1238, Val Loss: 331.3615\n",
            "Epoch 8441, Train Loss: 600.9461, Val Loss: 224.0892\n",
            "Epoch 8442, Train Loss: 581.6525, Val Loss: 236.6771\n",
            "Epoch 8443, Train Loss: 621.5745, Val Loss: 191.2186\n",
            "Epoch 8444, Train Loss: 613.5379, Val Loss: 176.8211\n",
            "Epoch 8445, Train Loss: 625.4625, Val Loss: 173.9874\n",
            "Epoch 8446, Train Loss: 616.0730, Val Loss: 179.7145\n",
            "Epoch 8447, Train Loss: 610.5350, Val Loss: 189.0070\n",
            "Epoch 8448, Train Loss: 608.3013, Val Loss: 187.3472\n",
            "Epoch 8449, Train Loss: 610.9785, Val Loss: 187.3067\n",
            "Epoch 8450, Train Loss: 615.9499, Val Loss: 178.7817\n",
            "Epoch 8451, Train Loss: 609.9951, Val Loss: 180.6953\n",
            "Epoch 8452, Train Loss: 620.4153, Val Loss: 185.9751\n",
            "Epoch 8453, Train Loss: 599.3953, Val Loss: 190.8887\n",
            "Epoch 8454, Train Loss: 615.1207, Val Loss: 184.5360\n",
            "Epoch 8455, Train Loss: 612.1818, Val Loss: 183.2102\n",
            "Epoch 8456, Train Loss: 619.5808, Val Loss: 193.0022\n",
            "Epoch 8457, Train Loss: 625.6726, Val Loss: 215.5043\n",
            "Epoch 8458, Train Loss: 615.5943, Val Loss: 246.5601\n",
            "Epoch 8459, Train Loss: 623.1027, Val Loss: 225.0861\n",
            "Epoch 8460, Train Loss: 613.6393, Val Loss: 189.0350\n",
            "Epoch 8461, Train Loss: 624.0574, Val Loss: 184.4669\n",
            "Epoch 8462, Train Loss: 619.8397, Val Loss: 191.8468\n",
            "Epoch 8463, Train Loss: 606.5709, Val Loss: 192.2042\n",
            "Epoch 8464, Train Loss: 621.1581, Val Loss: 191.5923\n",
            "Epoch 8465, Train Loss: 602.8758, Val Loss: 186.2588\n",
            "Epoch 8466, Train Loss: 603.8853, Val Loss: 182.8286\n",
            "Epoch 8467, Train Loss: 619.5381, Val Loss: 186.5305\n",
            "Epoch 8468, Train Loss: 610.8242, Val Loss: 190.0005\n",
            "Epoch 8469, Train Loss: 616.6500, Val Loss: 197.8506\n",
            "Epoch 8470, Train Loss: 568.5297, Val Loss: 201.8026\n",
            "Epoch 8471, Train Loss: 579.5374, Val Loss: 183.2534\n",
            "Epoch 8472, Train Loss: 624.1267, Val Loss: 174.1750\n",
            "Epoch 8473, Train Loss: 614.9019, Val Loss: 173.2662\n",
            "Epoch 8474, Train Loss: 570.6032, Val Loss: 180.8847\n",
            "Epoch 8475, Train Loss: 617.3450, Val Loss: 187.5231\n",
            "Epoch 8476, Train Loss: 628.1736, Val Loss: 182.8229\n",
            "Epoch 8477, Train Loss: 600.2660, Val Loss: 206.9510\n",
            "Epoch 8478, Train Loss: 618.1954, Val Loss: 333.1460\n",
            "Epoch 8479, Train Loss: 623.3173, Val Loss: 345.9478\n",
            "Epoch 8480, Train Loss: 568.5050, Val Loss: 260.6807\n",
            "Epoch 8481, Train Loss: 625.5078, Val Loss: 211.6970\n",
            "Epoch 8482, Train Loss: 617.5502, Val Loss: 203.4417\n",
            "Epoch 8483, Train Loss: 602.2665, Val Loss: 185.4222\n",
            "Epoch 8484, Train Loss: 627.5741, Val Loss: 172.4674\n",
            "Epoch 8485, Train Loss: 591.5243, Val Loss: 214.0966\n",
            "Epoch 8486, Train Loss: 599.1853, Val Loss: 282.9246\n",
            "Epoch 8487, Train Loss: 612.4294, Val Loss: 302.2720\n",
            "Epoch 8488, Train Loss: 598.9171, Val Loss: 275.5675\n",
            "Epoch 8489, Train Loss: 586.7658, Val Loss: 309.0392\n",
            "Epoch 8490, Train Loss: 629.5064, Val Loss: 228.3893\n",
            "Epoch 8491, Train Loss: 603.3716, Val Loss: 186.7546\n",
            "Epoch 8492, Train Loss: 584.9014, Val Loss: 189.1065\n",
            "Epoch 8493, Train Loss: 616.8696, Val Loss: 232.2217\n",
            "Epoch 8494, Train Loss: 613.2700, Val Loss: 205.9122\n",
            "Epoch 8495, Train Loss: 612.9200, Val Loss: 193.4062\n",
            "Epoch 8496, Train Loss: 613.3116, Val Loss: 194.2920\n",
            "Epoch 8497, Train Loss: 622.4630, Val Loss: 204.3810\n",
            "Epoch 8498, Train Loss: 604.9622, Val Loss: 197.9172\n",
            "Epoch 8499, Train Loss: 626.1975, Val Loss: 178.7621\n",
            "Epoch 8500, Train Loss: 574.3710, Val Loss: 175.5454\n",
            "Epoch 8501, Train Loss: 597.5548, Val Loss: 179.3246\n",
            "Epoch 8502, Train Loss: 610.9533, Val Loss: 211.4578\n",
            "Epoch 8503, Train Loss: 621.2452, Val Loss: 242.0226\n",
            "Epoch 8504, Train Loss: 563.6103, Val Loss: 303.0686\n",
            "Epoch 8505, Train Loss: 618.0672, Val Loss: 320.7043\n",
            "Epoch 8506, Train Loss: 612.8734, Val Loss: 290.5975\n",
            "Epoch 8507, Train Loss: 610.0036, Val Loss: 218.7303\n",
            "Epoch 8508, Train Loss: 614.0203, Val Loss: 184.2984\n",
            "Epoch 8509, Train Loss: 541.1518, Val Loss: 178.0973\n",
            "Epoch 8510, Train Loss: 593.2222, Val Loss: 188.7275\n",
            "Epoch 8511, Train Loss: 608.8511, Val Loss: 205.1109\n",
            "Epoch 8512, Train Loss: 606.4131, Val Loss: 203.8462\n",
            "Epoch 8513, Train Loss: 626.0923, Val Loss: 205.9650\n",
            "Epoch 8514, Train Loss: 585.2244, Val Loss: 266.5671\n",
            "Epoch 8515, Train Loss: 598.8214, Val Loss: 273.9040\n",
            "Epoch 8516, Train Loss: 613.2287, Val Loss: 195.2282\n",
            "Epoch 8517, Train Loss: 575.8615, Val Loss: 184.8162\n",
            "Epoch 8518, Train Loss: 623.2340, Val Loss: 190.5039\n",
            "Epoch 8519, Train Loss: 621.9625, Val Loss: 170.6546\n",
            "Epoch 8520, Train Loss: 600.9366, Val Loss: 187.0702\n",
            "Epoch 8521, Train Loss: 617.6178, Val Loss: 295.9412\n",
            "Epoch 8522, Train Loss: 609.9657, Val Loss: 308.4737\n",
            "Epoch 8523, Train Loss: 584.2383, Val Loss: 249.5130\n",
            "Epoch 8524, Train Loss: 612.0421, Val Loss: 198.9936\n",
            "Epoch 8525, Train Loss: 606.0429, Val Loss: 193.7112\n",
            "Epoch 8526, Train Loss: 617.2542, Val Loss: 183.6781\n",
            "Epoch 8527, Train Loss: 606.6437, Val Loss: 176.0386\n",
            "Epoch 8528, Train Loss: 562.5041, Val Loss: 191.5597\n",
            "Epoch 8529, Train Loss: 626.2603, Val Loss: 211.7056\n",
            "Epoch 8530, Train Loss: 567.7280, Val Loss: 237.6004\n",
            "Epoch 8531, Train Loss: 606.4886, Val Loss: 240.8102\n",
            "Epoch 8532, Train Loss: 620.6934, Val Loss: 212.5627\n",
            "Epoch 8533, Train Loss: 617.6788, Val Loss: 203.3586\n",
            "Epoch 8534, Train Loss: 554.8286, Val Loss: 191.0514\n",
            "Epoch 8535, Train Loss: 623.9737, Val Loss: 183.4229\n",
            "Epoch 8536, Train Loss: 609.8444, Val Loss: 182.7823\n",
            "Epoch 8537, Train Loss: 574.6737, Val Loss: 189.6413\n",
            "Epoch 8538, Train Loss: 625.8508, Val Loss: 206.1040\n",
            "Epoch 8539, Train Loss: 611.4636, Val Loss: 207.5438\n",
            "Epoch 8540, Train Loss: 618.0903, Val Loss: 177.7054\n",
            "Epoch 8541, Train Loss: 604.8670, Val Loss: 169.4782\n",
            "Epoch 8542, Train Loss: 601.0018, Val Loss: 172.5984\n",
            "Epoch 8543, Train Loss: 620.5981, Val Loss: 181.2039\n",
            "Epoch 8544, Train Loss: 596.6586, Val Loss: 211.1494\n",
            "Epoch 8545, Train Loss: 597.5041, Val Loss: 234.8514\n",
            "Epoch 8546, Train Loss: 584.9303, Val Loss: 242.9609\n",
            "Epoch 8547, Train Loss: 611.1723, Val Loss: 239.7846\n",
            "Epoch 8548, Train Loss: 625.8609, Val Loss: 232.3358\n",
            "Epoch 8549, Train Loss: 619.6164, Val Loss: 201.7935\n",
            "Epoch 8550, Train Loss: 574.8725, Val Loss: 199.3991\n",
            "Epoch 8551, Train Loss: 605.8233, Val Loss: 195.4309\n",
            "Epoch 8552, Train Loss: 606.6009, Val Loss: 185.5248\n",
            "Epoch 8553, Train Loss: 603.2931, Val Loss: 188.0567\n",
            "Epoch 8554, Train Loss: 622.6927, Val Loss: 205.7271\n",
            "Epoch 8555, Train Loss: 594.0748, Val Loss: 218.3643\n",
            "Epoch 8556, Train Loss: 623.2826, Val Loss: 213.3275\n",
            "Epoch 8557, Train Loss: 583.2900, Val Loss: 213.9141\n",
            "Epoch 8558, Train Loss: 540.2033, Val Loss: 228.7796\n",
            "Epoch 8559, Train Loss: 627.7910, Val Loss: 186.2942\n",
            "Epoch 8560, Train Loss: 610.6208, Val Loss: 173.9614\n",
            "Epoch 8561, Train Loss: 592.2171, Val Loss: 172.0198\n",
            "Epoch 8562, Train Loss: 612.6116, Val Loss: 190.5873\n",
            "Epoch 8563, Train Loss: 620.7156, Val Loss: 212.8968\n",
            "Epoch 8564, Train Loss: 591.5613, Val Loss: 213.9882\n",
            "Epoch 8565, Train Loss: 612.6898, Val Loss: 187.0459\n",
            "Epoch 8566, Train Loss: 601.0416, Val Loss: 186.3325\n",
            "Epoch 8567, Train Loss: 620.0331, Val Loss: 190.2304\n",
            "Epoch 8568, Train Loss: 622.7062, Val Loss: 187.1016\n",
            "Epoch 8569, Train Loss: 603.2725, Val Loss: 178.6719\n",
            "Epoch 8570, Train Loss: 622.4346, Val Loss: 174.7924\n",
            "Epoch 8571, Train Loss: 572.6998, Val Loss: 175.5781\n",
            "Epoch 8572, Train Loss: 609.9361, Val Loss: 197.4735\n",
            "Epoch 8573, Train Loss: 613.0378, Val Loss: 218.6642\n",
            "Epoch 8574, Train Loss: 620.3661, Val Loss: 211.5303\n",
            "Epoch 8575, Train Loss: 622.8800, Val Loss: 180.6158\n",
            "Epoch 8576, Train Loss: 613.4986, Val Loss: 172.6773\n",
            "Epoch 8577, Train Loss: 624.6246, Val Loss: 171.9648\n",
            "Epoch 8578, Train Loss: 575.8133, Val Loss: 201.8752\n",
            "Epoch 8579, Train Loss: 625.3817, Val Loss: 204.3896\n",
            "Epoch 8580, Train Loss: 615.5473, Val Loss: 194.3469\n",
            "Epoch 8581, Train Loss: 607.6974, Val Loss: 203.8132\n",
            "Epoch 8582, Train Loss: 617.4171, Val Loss: 216.2899\n",
            "Epoch 8583, Train Loss: 596.9151, Val Loss: 260.2329\n",
            "Epoch 8584, Train Loss: 595.2456, Val Loss: 302.8023\n",
            "Epoch 8585, Train Loss: 620.0431, Val Loss: 237.5522\n",
            "Epoch 8586, Train Loss: 620.8712, Val Loss: 211.8211\n",
            "Epoch 8587, Train Loss: 611.3152, Val Loss: 210.7197\n",
            "Epoch 8588, Train Loss: 611.6448, Val Loss: 222.6148\n",
            "Epoch 8589, Train Loss: 608.0622, Val Loss: 213.6508\n",
            "Epoch 8590, Train Loss: 603.6218, Val Loss: 183.3844\n",
            "Epoch 8591, Train Loss: 612.5151, Val Loss: 195.5294\n",
            "Epoch 8592, Train Loss: 603.3951, Val Loss: 178.6913\n",
            "Epoch 8593, Train Loss: 607.8656, Val Loss: 174.9317\n",
            "Epoch 8594, Train Loss: 600.6453, Val Loss: 178.6394\n",
            "Epoch 8595, Train Loss: 588.8370, Val Loss: 195.8116\n",
            "Epoch 8596, Train Loss: 617.0819, Val Loss: 221.2988\n",
            "Epoch 8597, Train Loss: 606.2470, Val Loss: 280.3474\n",
            "Epoch 8598, Train Loss: 601.0065, Val Loss: 231.5303\n",
            "Epoch 8599, Train Loss: 627.9397, Val Loss: 198.8376\n",
            "Epoch 8600, Train Loss: 544.3780, Val Loss: 193.1207\n",
            "Epoch 8601, Train Loss: 620.8943, Val Loss: 234.5220\n",
            "Epoch 8602, Train Loss: 617.1331, Val Loss: 246.3092\n",
            "Epoch 8603, Train Loss: 598.6327, Val Loss: 205.0856\n",
            "Epoch 8604, Train Loss: 606.8031, Val Loss: 182.4286\n",
            "Epoch 8605, Train Loss: 609.0588, Val Loss: 179.4170\n",
            "Epoch 8606, Train Loss: 613.1751, Val Loss: 196.6274\n",
            "Epoch 8607, Train Loss: 585.9283, Val Loss: 246.7263\n",
            "Epoch 8608, Train Loss: 622.9129, Val Loss: 225.4523\n",
            "Epoch 8609, Train Loss: 607.5130, Val Loss: 193.2006\n",
            "Epoch 8610, Train Loss: 551.2614, Val Loss: 244.2397\n",
            "Epoch 8611, Train Loss: 561.8276, Val Loss: 210.0649\n",
            "Epoch 8612, Train Loss: 589.2786, Val Loss: 179.2067\n",
            "Epoch 8613, Train Loss: 623.9714, Val Loss: 178.5741\n",
            "Epoch 8614, Train Loss: 609.2499, Val Loss: 178.2971\n",
            "Epoch 8615, Train Loss: 554.6169, Val Loss: 190.5053\n",
            "Epoch 8616, Train Loss: 547.8981, Val Loss: 275.4671\n",
            "Epoch 8617, Train Loss: 620.5583, Val Loss: 310.7905\n",
            "Epoch 8618, Train Loss: 611.2844, Val Loss: 262.8013\n",
            "Epoch 8619, Train Loss: 605.5556, Val Loss: 221.0738\n",
            "Epoch 8620, Train Loss: 592.4294, Val Loss: 194.2993\n",
            "Epoch 8621, Train Loss: 630.7281, Val Loss: 189.1570\n",
            "Epoch 8622, Train Loss: 582.1860, Val Loss: 180.8464\n",
            "Epoch 8623, Train Loss: 618.8894, Val Loss: 172.1261\n",
            "Epoch 8624, Train Loss: 614.7029, Val Loss: 176.3205\n",
            "Epoch 8625, Train Loss: 615.5129, Val Loss: 193.4335\n",
            "Epoch 8626, Train Loss: 620.0686, Val Loss: 180.6557\n",
            "Epoch 8627, Train Loss: 617.1014, Val Loss: 175.8202\n",
            "Epoch 8628, Train Loss: 574.1598, Val Loss: 197.7350\n",
            "Epoch 8629, Train Loss: 602.2559, Val Loss: 265.7093\n",
            "Epoch 8630, Train Loss: 591.1611, Val Loss: 286.5441\n",
            "Epoch 8631, Train Loss: 609.7698, Val Loss: 233.5801\n",
            "Epoch 8632, Train Loss: 600.8306, Val Loss: 185.4865\n",
            "Epoch 8633, Train Loss: 606.7952, Val Loss: 182.6158\n",
            "Epoch 8634, Train Loss: 580.2407, Val Loss: 212.6279\n",
            "Epoch 8635, Train Loss: 596.5844, Val Loss: 201.0429\n",
            "Epoch 8636, Train Loss: 593.2584, Val Loss: 209.7687\n",
            "Epoch 8637, Train Loss: 605.2590, Val Loss: 208.7320\n",
            "Epoch 8638, Train Loss: 615.1635, Val Loss: 190.8064\n",
            "Epoch 8639, Train Loss: 615.8536, Val Loss: 184.8137\n",
            "Epoch 8640, Train Loss: 627.8248, Val Loss: 173.6515\n",
            "Epoch 8641, Train Loss: 623.1977, Val Loss: 167.6732\n",
            "Epoch 8642, Train Loss: 618.0872, Val Loss: 187.2522\n",
            "Epoch 8643, Train Loss: 617.4630, Val Loss: 211.2190\n",
            "Epoch 8644, Train Loss: 602.3001, Val Loss: 205.2846\n",
            "Epoch 8645, Train Loss: 585.7408, Val Loss: 208.1276\n",
            "Epoch 8646, Train Loss: 613.1250, Val Loss: 207.4312\n",
            "Epoch 8647, Train Loss: 621.5835, Val Loss: 186.7183\n",
            "Epoch 8648, Train Loss: 607.6604, Val Loss: 181.4626\n",
            "Epoch 8649, Train Loss: 602.6929, Val Loss: 203.8405\n",
            "Epoch 8650, Train Loss: 591.7383, Val Loss: 212.9424\n",
            "Epoch 8651, Train Loss: 608.5934, Val Loss: 198.4281\n",
            "Epoch 8652, Train Loss: 619.8668, Val Loss: 185.3547\n",
            "Epoch 8653, Train Loss: 626.6580, Val Loss: 179.9899\n",
            "Epoch 8654, Train Loss: 616.2099, Val Loss: 190.6711\n",
            "Epoch 8655, Train Loss: 613.4397, Val Loss: 202.2257\n",
            "Epoch 8656, Train Loss: 588.3712, Val Loss: 214.7536\n",
            "Epoch 8657, Train Loss: 626.7084, Val Loss: 189.7533\n",
            "Epoch 8658, Train Loss: 616.5636, Val Loss: 179.3705\n",
            "Epoch 8659, Train Loss: 626.1284, Val Loss: 183.7364\n",
            "Epoch 8660, Train Loss: 618.4678, Val Loss: 195.6028\n",
            "Epoch 8661, Train Loss: 604.9908, Val Loss: 208.5734\n",
            "Epoch 8662, Train Loss: 572.4214, Val Loss: 232.1559\n",
            "Epoch 8663, Train Loss: 620.4882, Val Loss: 248.0260\n",
            "Epoch 8664, Train Loss: 620.1221, Val Loss: 254.8359\n",
            "Epoch 8665, Train Loss: 611.5359, Val Loss: 230.9636\n",
            "Epoch 8666, Train Loss: 554.3614, Val Loss: 176.9081\n",
            "Epoch 8667, Train Loss: 594.0668, Val Loss: 185.6651\n",
            "Epoch 8668, Train Loss: 616.2411, Val Loss: 205.3528\n",
            "Epoch 8669, Train Loss: 618.0153, Val Loss: 175.7862\n",
            "Epoch 8670, Train Loss: 602.1985, Val Loss: 178.5858\n",
            "Epoch 8671, Train Loss: 622.0003, Val Loss: 192.7482\n",
            "Epoch 8672, Train Loss: 587.4566, Val Loss: 209.3346\n",
            "Epoch 8673, Train Loss: 624.3535, Val Loss: 202.3629\n",
            "Epoch 8674, Train Loss: 621.1808, Val Loss: 181.3240\n",
            "Epoch 8675, Train Loss: 590.8820, Val Loss: 177.4859\n",
            "Epoch 8676, Train Loss: 621.4391, Val Loss: 178.7117\n",
            "Epoch 8677, Train Loss: 624.7505, Val Loss: 201.2381\n",
            "Epoch 8678, Train Loss: 551.9474, Val Loss: 232.4370\n",
            "Epoch 8679, Train Loss: 602.6378, Val Loss: 277.7823\n",
            "Epoch 8680, Train Loss: 626.5666, Val Loss: 299.0146\n",
            "Epoch 8681, Train Loss: 591.8477, Val Loss: 253.6261\n",
            "Epoch 8682, Train Loss: 618.2131, Val Loss: 188.2409\n",
            "Epoch 8683, Train Loss: 625.3586, Val Loss: 178.7044\n",
            "Epoch 8684, Train Loss: 630.6886, Val Loss: 177.1368\n",
            "Epoch 8685, Train Loss: 618.9250, Val Loss: 210.9139\n",
            "Epoch 8686, Train Loss: 623.7219, Val Loss: 192.5608\n",
            "Epoch 8687, Train Loss: 619.8580, Val Loss: 175.7458\n",
            "Epoch 8688, Train Loss: 593.4748, Val Loss: 174.4176\n",
            "Epoch 8689, Train Loss: 622.3796, Val Loss: 183.6309\n",
            "Epoch 8690, Train Loss: 527.8061, Val Loss: 268.3753\n",
            "Epoch 8691, Train Loss: 598.5757, Val Loss: 322.3464\n",
            "Epoch 8692, Train Loss: 615.7684, Val Loss: 301.9079\n",
            "Epoch 8693, Train Loss: 579.1175, Val Loss: 241.0010\n",
            "Epoch 8694, Train Loss: 610.0780, Val Loss: 199.6540\n",
            "Epoch 8695, Train Loss: 622.2024, Val Loss: 199.3009\n",
            "Epoch 8696, Train Loss: 590.4010, Val Loss: 257.1385\n",
            "Epoch 8697, Train Loss: 619.6196, Val Loss: 263.5520\n",
            "Epoch 8698, Train Loss: 584.5464, Val Loss: 262.5114\n",
            "Epoch 8699, Train Loss: 596.7405, Val Loss: 217.2870\n",
            "Epoch 8700, Train Loss: 603.0133, Val Loss: 184.1957\n",
            "Epoch 8701, Train Loss: 609.1689, Val Loss: 179.9352\n",
            "Epoch 8702, Train Loss: 595.1227, Val Loss: 185.8476\n",
            "Epoch 8703, Train Loss: 624.4851, Val Loss: 205.4195\n",
            "Epoch 8704, Train Loss: 601.0672, Val Loss: 221.6042\n",
            "Epoch 8705, Train Loss: 616.2764, Val Loss: 206.7565\n",
            "Epoch 8706, Train Loss: 605.0241, Val Loss: 197.1286\n",
            "Epoch 8707, Train Loss: 607.2730, Val Loss: 190.9486\n",
            "Epoch 8708, Train Loss: 576.3876, Val Loss: 182.2255\n",
            "Epoch 8709, Train Loss: 574.9157, Val Loss: 194.5112\n",
            "Epoch 8710, Train Loss: 620.1471, Val Loss: 203.6836\n",
            "Epoch 8711, Train Loss: 608.8163, Val Loss: 229.0952\n",
            "Epoch 8712, Train Loss: 590.8803, Val Loss: 264.1887\n",
            "Epoch 8713, Train Loss: 578.5940, Val Loss: 270.2777\n",
            "Epoch 8714, Train Loss: 622.7705, Val Loss: 205.0426\n",
            "Epoch 8715, Train Loss: 625.2252, Val Loss: 195.1446\n",
            "Epoch 8716, Train Loss: 612.2970, Val Loss: 200.2879\n",
            "Epoch 8717, Train Loss: 576.9938, Val Loss: 180.6088\n",
            "Epoch 8718, Train Loss: 618.0544, Val Loss: 186.7047\n",
            "Epoch 8719, Train Loss: 619.3253, Val Loss: 202.1202\n",
            "Epoch 8720, Train Loss: 586.7603, Val Loss: 225.9990\n",
            "Epoch 8721, Train Loss: 610.1762, Val Loss: 197.8536\n",
            "Epoch 8722, Train Loss: 607.7216, Val Loss: 197.1297\n",
            "Epoch 8723, Train Loss: 609.0814, Val Loss: 190.7756\n",
            "Epoch 8724, Train Loss: 590.3681, Val Loss: 188.8415\n",
            "Epoch 8725, Train Loss: 612.9601, Val Loss: 194.9255\n",
            "Epoch 8726, Train Loss: 598.2144, Val Loss: 216.6295\n",
            "Epoch 8727, Train Loss: 618.7601, Val Loss: 229.8311\n",
            "Epoch 8728, Train Loss: 621.7876, Val Loss: 211.2998\n",
            "Epoch 8729, Train Loss: 607.1488, Val Loss: 185.5971\n",
            "Epoch 8730, Train Loss: 616.6571, Val Loss: 189.2952\n",
            "Epoch 8731, Train Loss: 612.9522, Val Loss: 192.4787\n",
            "Epoch 8732, Train Loss: 624.9070, Val Loss: 188.3730\n",
            "Epoch 8733, Train Loss: 613.5810, Val Loss: 192.5950\n",
            "Epoch 8734, Train Loss: 622.6735, Val Loss: 189.9669\n",
            "Epoch 8735, Train Loss: 608.2755, Val Loss: 191.7332\n",
            "Epoch 8736, Train Loss: 600.6472, Val Loss: 200.8878\n",
            "Epoch 8737, Train Loss: 607.7035, Val Loss: 196.2998\n",
            "Epoch 8738, Train Loss: 591.1570, Val Loss: 188.8122\n",
            "Epoch 8739, Train Loss: 615.0872, Val Loss: 191.3936\n",
            "Epoch 8740, Train Loss: 615.6015, Val Loss: 206.4132\n",
            "Epoch 8741, Train Loss: 617.5532, Val Loss: 216.6396\n",
            "Epoch 8742, Train Loss: 614.7558, Val Loss: 199.7380\n",
            "Epoch 8743, Train Loss: 615.8231, Val Loss: 188.4927\n",
            "Epoch 8744, Train Loss: 590.4731, Val Loss: 197.2484\n",
            "Epoch 8745, Train Loss: 618.7403, Val Loss: 208.7061\n",
            "Epoch 8746, Train Loss: 583.1561, Val Loss: 223.2819\n",
            "Epoch 8747, Train Loss: 582.8698, Val Loss: 210.7463\n",
            "Epoch 8748, Train Loss: 606.4415, Val Loss: 205.1117\n",
            "Epoch 8749, Train Loss: 610.8613, Val Loss: 225.9540\n",
            "Epoch 8750, Train Loss: 621.8891, Val Loss: 267.2438\n",
            "Epoch 8751, Train Loss: 593.5783, Val Loss: 254.6450\n",
            "Epoch 8752, Train Loss: 573.6954, Val Loss: 199.9519\n",
            "Epoch 8753, Train Loss: 536.6946, Val Loss: 193.1728\n",
            "Epoch 8754, Train Loss: 602.0849, Val Loss: 189.3636\n",
            "Epoch 8755, Train Loss: 623.2627, Val Loss: 180.6625\n",
            "Epoch 8756, Train Loss: 624.1391, Val Loss: 182.1262\n",
            "Epoch 8757, Train Loss: 587.6369, Val Loss: 185.3407\n",
            "Epoch 8758, Train Loss: 567.5798, Val Loss: 194.7056\n",
            "Epoch 8759, Train Loss: 562.4159, Val Loss: 199.4334\n",
            "Epoch 8760, Train Loss: 636.8079, Val Loss: 184.1513\n",
            "Epoch 8761, Train Loss: 609.7163, Val Loss: 184.7429\n",
            "Epoch 8762, Train Loss: 612.1767, Val Loss: 182.4627\n",
            "Epoch 8763, Train Loss: 610.1831, Val Loss: 182.2088\n",
            "Epoch 8764, Train Loss: 623.6940, Val Loss: 267.5940\n",
            "Epoch 8765, Train Loss: 573.0139, Val Loss: 350.4949\n",
            "Epoch 8766, Train Loss: 600.5127, Val Loss: 317.0954\n",
            "Epoch 8767, Train Loss: 620.3494, Val Loss: 240.7430\n",
            "Epoch 8768, Train Loss: 594.4084, Val Loss: 228.8468\n",
            "Epoch 8769, Train Loss: 620.0474, Val Loss: 258.8857\n",
            "Epoch 8770, Train Loss: 586.1817, Val Loss: 235.0939\n",
            "Epoch 8771, Train Loss: 622.4955, Val Loss: 190.5308\n",
            "Epoch 8772, Train Loss: 543.6236, Val Loss: 183.7057\n",
            "Epoch 8773, Train Loss: 611.9443, Val Loss: 211.5586\n",
            "Epoch 8774, Train Loss: 624.6410, Val Loss: 222.3235\n",
            "Epoch 8775, Train Loss: 617.4221, Val Loss: 255.7844\n",
            "Epoch 8776, Train Loss: 612.8330, Val Loss: 230.2616\n",
            "Epoch 8777, Train Loss: 533.1977, Val Loss: 226.5794\n",
            "Epoch 8778, Train Loss: 572.1534, Val Loss: 223.4874\n",
            "Epoch 8779, Train Loss: 609.3312, Val Loss: 184.2865\n",
            "Epoch 8780, Train Loss: 621.6740, Val Loss: 193.1640\n",
            "Epoch 8781, Train Loss: 630.9569, Val Loss: 191.9369\n",
            "Epoch 8782, Train Loss: 609.8055, Val Loss: 172.3022\n",
            "Epoch 8783, Train Loss: 612.1679, Val Loss: 213.9585\n",
            "Epoch 8784, Train Loss: 622.8975, Val Loss: 211.4856\n",
            "Epoch 8785, Train Loss: 610.7099, Val Loss: 182.0538\n",
            "Epoch 8786, Train Loss: 608.9592, Val Loss: 177.5864\n",
            "Epoch 8787, Train Loss: 615.0043, Val Loss: 188.4660\n",
            "Epoch 8788, Train Loss: 613.9033, Val Loss: 222.9809\n",
            "Epoch 8789, Train Loss: 609.9586, Val Loss: 255.0468\n",
            "Epoch 8790, Train Loss: 607.9016, Val Loss: 280.2292\n",
            "Epoch 8791, Train Loss: 623.0082, Val Loss: 190.4445\n",
            "Epoch 8792, Train Loss: 601.9183, Val Loss: 177.3176\n",
            "Epoch 8793, Train Loss: 619.8402, Val Loss: 195.8600\n",
            "Epoch 8794, Train Loss: 588.4556, Val Loss: 260.4070\n",
            "Epoch 8795, Train Loss: 596.5894, Val Loss: 267.4283\n",
            "Epoch 8796, Train Loss: 619.4959, Val Loss: 257.9791\n",
            "Epoch 8797, Train Loss: 579.4213, Val Loss: 210.4050\n",
            "Epoch 8798, Train Loss: 573.7455, Val Loss: 200.5309\n",
            "Epoch 8799, Train Loss: 588.6099, Val Loss: 213.2783\n",
            "Epoch 8800, Train Loss: 626.4220, Val Loss: 208.2856\n",
            "Epoch 8801, Train Loss: 615.5053, Val Loss: 203.6087\n",
            "Epoch 8802, Train Loss: 564.3568, Val Loss: 195.7790\n",
            "Epoch 8803, Train Loss: 607.3218, Val Loss: 203.4745\n",
            "Epoch 8804, Train Loss: 605.1634, Val Loss: 205.2358\n",
            "Epoch 8805, Train Loss: 615.6056, Val Loss: 196.9868\n",
            "Epoch 8806, Train Loss: 610.8633, Val Loss: 219.0900\n",
            "Epoch 8807, Train Loss: 598.3059, Val Loss: 263.1705\n",
            "Epoch 8808, Train Loss: 563.9917, Val Loss: 279.3731\n",
            "Epoch 8809, Train Loss: 605.9791, Val Loss: 242.5894\n",
            "Epoch 8810, Train Loss: 612.9767, Val Loss: 199.3745\n",
            "Epoch 8811, Train Loss: 604.9197, Val Loss: 181.4290\n",
            "Epoch 8812, Train Loss: 627.3328, Val Loss: 173.8498\n",
            "Epoch 8813, Train Loss: 606.2935, Val Loss: 172.3904\n",
            "Epoch 8814, Train Loss: 596.7011, Val Loss: 170.9433\n",
            "Epoch 8815, Train Loss: 622.4804, Val Loss: 179.1805\n",
            "Epoch 8816, Train Loss: 615.6786, Val Loss: 190.3502\n",
            "Epoch 8817, Train Loss: 563.1663, Val Loss: 233.8176\n",
            "Epoch 8818, Train Loss: 627.0404, Val Loss: 266.1119\n",
            "Epoch 8819, Train Loss: 625.0728, Val Loss: 191.0525\n",
            "Epoch 8820, Train Loss: 591.5146, Val Loss: 177.6792\n",
            "Epoch 8821, Train Loss: 611.1448, Val Loss: 178.5682\n",
            "Epoch 8822, Train Loss: 605.6010, Val Loss: 184.1936\n",
            "Epoch 8823, Train Loss: 566.2915, Val Loss: 200.4700\n",
            "Epoch 8824, Train Loss: 623.5864, Val Loss: 207.7095\n",
            "Epoch 8825, Train Loss: 593.2051, Val Loss: 208.3553\n",
            "Epoch 8826, Train Loss: 581.6451, Val Loss: 230.1215\n",
            "Epoch 8827, Train Loss: 621.6745, Val Loss: 228.5734\n",
            "Epoch 8828, Train Loss: 580.5716, Val Loss: 258.0384\n",
            "Epoch 8829, Train Loss: 593.5418, Val Loss: 246.2829\n",
            "Epoch 8830, Train Loss: 580.8943, Val Loss: 216.6652\n",
            "Epoch 8831, Train Loss: 601.8588, Val Loss: 200.6414\n",
            "Epoch 8832, Train Loss: 618.5134, Val Loss: 179.1360\n",
            "Epoch 8833, Train Loss: 616.3746, Val Loss: 185.8678\n",
            "Epoch 8834, Train Loss: 594.1000, Val Loss: 222.5777\n",
            "Epoch 8835, Train Loss: 606.2577, Val Loss: 233.1505\n",
            "Epoch 8836, Train Loss: 611.4846, Val Loss: 192.9967\n",
            "Epoch 8837, Train Loss: 622.2284, Val Loss: 172.8430\n",
            "Epoch 8838, Train Loss: 569.2692, Val Loss: 176.8555\n",
            "Epoch 8839, Train Loss: 603.7354, Val Loss: 177.4944\n",
            "Epoch 8840, Train Loss: 606.3759, Val Loss: 296.5975\n",
            "Epoch 8841, Train Loss: 616.4078, Val Loss: 305.9045\n",
            "Epoch 8842, Train Loss: 599.5224, Val Loss: 235.9291\n",
            "Epoch 8843, Train Loss: 624.7224, Val Loss: 199.6844\n",
            "Epoch 8844, Train Loss: 624.0720, Val Loss: 186.3369\n",
            "Epoch 8845, Train Loss: 596.3914, Val Loss: 190.4015\n",
            "Epoch 8846, Train Loss: 616.6450, Val Loss: 197.6386\n",
            "Epoch 8847, Train Loss: 603.1415, Val Loss: 202.5048\n",
            "Epoch 8848, Train Loss: 616.6435, Val Loss: 192.0568\n",
            "Epoch 8849, Train Loss: 560.1351, Val Loss: 214.7344\n",
            "Epoch 8850, Train Loss: 614.1865, Val Loss: 241.8025\n",
            "Epoch 8851, Train Loss: 600.4293, Val Loss: 237.4258\n",
            "Epoch 8852, Train Loss: 607.5988, Val Loss: 186.3735\n",
            "Epoch 8853, Train Loss: 615.1634, Val Loss: 180.2437\n",
            "Epoch 8854, Train Loss: 623.1824, Val Loss: 180.5833\n",
            "Epoch 8855, Train Loss: 613.7250, Val Loss: 176.3547\n",
            "Epoch 8856, Train Loss: 543.6240, Val Loss: 234.9658\n",
            "Epoch 8857, Train Loss: 607.4098, Val Loss: 251.2298\n",
            "Epoch 8858, Train Loss: 599.1644, Val Loss: 233.8666\n",
            "Epoch 8859, Train Loss: 615.1715, Val Loss: 202.4119\n",
            "Epoch 8860, Train Loss: 624.5386, Val Loss: 198.6275\n",
            "Epoch 8861, Train Loss: 619.8468, Val Loss: 194.5002\n",
            "Epoch 8862, Train Loss: 607.0795, Val Loss: 209.5327\n",
            "Epoch 8863, Train Loss: 596.5300, Val Loss: 217.7434\n",
            "Epoch 8864, Train Loss: 581.3443, Val Loss: 189.6312\n",
            "Epoch 8865, Train Loss: 618.1027, Val Loss: 181.9660\n",
            "Epoch 8866, Train Loss: 595.5252, Val Loss: 181.9748\n",
            "Epoch 8867, Train Loss: 620.5195, Val Loss: 191.4140\n",
            "Epoch 8868, Train Loss: 615.8573, Val Loss: 200.8012\n",
            "Epoch 8869, Train Loss: 620.8402, Val Loss: 191.1163\n",
            "Epoch 8870, Train Loss: 617.1088, Val Loss: 180.5723\n",
            "Epoch 8871, Train Loss: 613.8840, Val Loss: 173.5217\n",
            "Epoch 8872, Train Loss: 550.1417, Val Loss: 274.0763\n",
            "Epoch 8873, Train Loss: 614.0950, Val Loss: 344.4852\n",
            "Epoch 8874, Train Loss: 621.4621, Val Loss: 276.3951\n",
            "Epoch 8875, Train Loss: 569.2143, Val Loss: 203.5131\n",
            "Epoch 8876, Train Loss: 615.3837, Val Loss: 213.3330\n",
            "Epoch 8877, Train Loss: 625.4086, Val Loss: 209.6378\n",
            "Epoch 8878, Train Loss: 620.2396, Val Loss: 184.1752\n",
            "Epoch 8879, Train Loss: 630.8274, Val Loss: 177.8200\n",
            "Epoch 8880, Train Loss: 620.3335, Val Loss: 167.2254\n",
            "Epoch 8881, Train Loss: 620.0107, Val Loss: 179.9923\n",
            "Epoch 8882, Train Loss: 615.6290, Val Loss: 226.3183\n",
            "Epoch 8883, Train Loss: 621.2405, Val Loss: 269.0069\n",
            "Epoch 8884, Train Loss: 620.8125, Val Loss: 273.7510\n",
            "Epoch 8885, Train Loss: 620.7655, Val Loss: 244.7483\n",
            "Epoch 8886, Train Loss: 617.0770, Val Loss: 207.8527\n",
            "Epoch 8887, Train Loss: 614.3687, Val Loss: 197.8323\n",
            "Epoch 8888, Train Loss: 618.5564, Val Loss: 204.7901\n",
            "Epoch 8889, Train Loss: 606.5091, Val Loss: 218.3013\n",
            "Epoch 8890, Train Loss: 611.1101, Val Loss: 210.6384\n",
            "Epoch 8891, Train Loss: 617.7960, Val Loss: 192.9291\n",
            "Epoch 8892, Train Loss: 580.9076, Val Loss: 195.7697\n",
            "Epoch 8893, Train Loss: 621.8337, Val Loss: 183.2884\n",
            "Epoch 8894, Train Loss: 557.3560, Val Loss: 183.7700\n",
            "Epoch 8895, Train Loss: 568.5962, Val Loss: 184.9018\n",
            "Epoch 8896, Train Loss: 591.6316, Val Loss: 200.6143\n",
            "Epoch 8897, Train Loss: 614.9038, Val Loss: 230.0186\n",
            "Epoch 8898, Train Loss: 615.1101, Val Loss: 262.1410\n",
            "Epoch 8899, Train Loss: 589.3393, Val Loss: 225.5209\n",
            "Epoch 8900, Train Loss: 577.7557, Val Loss: 213.3491\n",
            "Epoch 8901, Train Loss: 520.1178, Val Loss: 242.6845\n",
            "Epoch 8902, Train Loss: 621.3712, Val Loss: 239.7674\n",
            "Epoch 8903, Train Loss: 603.0547, Val Loss: 203.9785\n",
            "Epoch 8904, Train Loss: 583.4316, Val Loss: 225.3456\n",
            "Epoch 8905, Train Loss: 623.2509, Val Loss: 223.6287\n",
            "Epoch 8906, Train Loss: 626.0566, Val Loss: 214.0893\n",
            "Epoch 8907, Train Loss: 622.9143, Val Loss: 183.4139\n",
            "Epoch 8908, Train Loss: 587.5565, Val Loss: 178.2968\n",
            "Epoch 8909, Train Loss: 503.7688, Val Loss: 183.6793\n",
            "Epoch 8910, Train Loss: 575.8957, Val Loss: 187.9514\n",
            "Epoch 8911, Train Loss: 615.4284, Val Loss: 193.5885\n",
            "Epoch 8912, Train Loss: 607.0559, Val Loss: 181.9938\n",
            "Epoch 8913, Train Loss: 596.3510, Val Loss: 182.3137\n",
            "Epoch 8914, Train Loss: 605.3539, Val Loss: 182.5491\n",
            "Epoch 8915, Train Loss: 618.2024, Val Loss: 185.9590\n",
            "Epoch 8916, Train Loss: 617.4649, Val Loss: 187.3835\n",
            "Epoch 8917, Train Loss: 618.7393, Val Loss: 187.5069\n",
            "Epoch 8918, Train Loss: 616.2774, Val Loss: 201.0854\n",
            "Epoch 8919, Train Loss: 619.7497, Val Loss: 222.5276\n",
            "Epoch 8920, Train Loss: 566.9759, Val Loss: 263.3631\n",
            "Epoch 8921, Train Loss: 621.3876, Val Loss: 244.8348\n",
            "Epoch 8922, Train Loss: 571.9565, Val Loss: 226.1255\n",
            "Epoch 8923, Train Loss: 557.7740, Val Loss: 200.6080\n",
            "Epoch 8924, Train Loss: 632.3721, Val Loss: 188.5118\n",
            "Epoch 8925, Train Loss: 588.1225, Val Loss: 193.7462\n",
            "Epoch 8926, Train Loss: 606.6790, Val Loss: 197.3875\n",
            "Epoch 8927, Train Loss: 600.2009, Val Loss: 212.9363\n",
            "Epoch 8928, Train Loss: 616.5496, Val Loss: 225.4944\n",
            "Epoch 8929, Train Loss: 559.7254, Val Loss: 241.9973\n",
            "Epoch 8930, Train Loss: 578.8090, Val Loss: 232.6162\n",
            "Epoch 8931, Train Loss: 610.1481, Val Loss: 194.3784\n",
            "Epoch 8932, Train Loss: 588.2556, Val Loss: 186.3197\n",
            "Epoch 8933, Train Loss: 617.1227, Val Loss: 202.1880\n",
            "Epoch 8934, Train Loss: 596.1379, Val Loss: 217.4159\n",
            "Epoch 8935, Train Loss: 592.6634, Val Loss: 215.3903\n",
            "Epoch 8936, Train Loss: 622.8274, Val Loss: 211.3053\n",
            "Epoch 8937, Train Loss: 625.9715, Val Loss: 193.1351\n",
            "Epoch 8938, Train Loss: 624.5915, Val Loss: 181.5787\n",
            "Epoch 8939, Train Loss: 608.6514, Val Loss: 177.0755\n",
            "Epoch 8940, Train Loss: 612.4959, Val Loss: 181.4991\n",
            "Epoch 8941, Train Loss: 545.3107, Val Loss: 304.1286\n",
            "Epoch 8942, Train Loss: 618.3408, Val Loss: 387.2750\n",
            "Epoch 8943, Train Loss: 608.0124, Val Loss: 292.1297\n",
            "Epoch 8944, Train Loss: 624.6803, Val Loss: 188.4841\n",
            "Epoch 8945, Train Loss: 602.5673, Val Loss: 193.1208\n",
            "Epoch 8946, Train Loss: 617.1794, Val Loss: 211.3719\n",
            "Epoch 8947, Train Loss: 614.3620, Val Loss: 215.5795\n",
            "Epoch 8948, Train Loss: 626.6837, Val Loss: 206.6880\n",
            "Epoch 8949, Train Loss: 608.7406, Val Loss: 204.1899\n",
            "Epoch 8950, Train Loss: 607.7054, Val Loss: 196.1058\n",
            "Epoch 8951, Train Loss: 623.7689, Val Loss: 177.1561\n",
            "Epoch 8952, Train Loss: 594.5347, Val Loss: 179.0914\n",
            "Epoch 8953, Train Loss: 623.3963, Val Loss: 187.9382\n",
            "Epoch 8954, Train Loss: 621.5875, Val Loss: 172.7040\n",
            "Epoch 8955, Train Loss: 583.2766, Val Loss: 437.9082\n",
            "Epoch 8956, Train Loss: 604.0114, Val Loss: 390.6106\n",
            "Epoch 8957, Train Loss: 625.0626, Val Loss: 289.4275\n",
            "Epoch 8958, Train Loss: 599.8686, Val Loss: 205.9756\n",
            "Epoch 8959, Train Loss: 595.3609, Val Loss: 178.8560\n",
            "Epoch 8960, Train Loss: 599.0336, Val Loss: 190.1711\n",
            "Epoch 8961, Train Loss: 575.9501, Val Loss: 220.9761\n",
            "Epoch 8962, Train Loss: 613.3333, Val Loss: 212.8803\n",
            "Epoch 8963, Train Loss: 616.5465, Val Loss: 209.2414\n",
            "Epoch 8964, Train Loss: 628.8976, Val Loss: 202.6152\n",
            "Epoch 8965, Train Loss: 615.8561, Val Loss: 200.5267\n",
            "Epoch 8966, Train Loss: 580.0392, Val Loss: 175.3810\n",
            "Epoch 8967, Train Loss: 618.6351, Val Loss: 174.6872\n",
            "Epoch 8968, Train Loss: 627.3665, Val Loss: 176.7578\n",
            "Epoch 8969, Train Loss: 605.2164, Val Loss: 212.7577\n",
            "Epoch 8970, Train Loss: 627.1035, Val Loss: 263.5164\n",
            "Epoch 8971, Train Loss: 595.4143, Val Loss: 261.2017\n",
            "Epoch 8972, Train Loss: 535.8160, Val Loss: 187.7488\n",
            "Epoch 8973, Train Loss: 599.2500, Val Loss: 178.0401\n",
            "Epoch 8974, Train Loss: 615.9692, Val Loss: 171.7106\n",
            "Epoch 8975, Train Loss: 624.5975, Val Loss: 225.7495\n",
            "Epoch 8976, Train Loss: 561.9597, Val Loss: 242.1176\n",
            "Epoch 8977, Train Loss: 600.1393, Val Loss: 211.9139\n",
            "Epoch 8978, Train Loss: 618.0012, Val Loss: 182.2293\n",
            "Epoch 8979, Train Loss: 627.2102, Val Loss: 180.9170\n",
            "Epoch 8980, Train Loss: 607.5414, Val Loss: 205.9402\n",
            "Epoch 8981, Train Loss: 586.0863, Val Loss: 263.0374\n",
            "Epoch 8982, Train Loss: 588.5153, Val Loss: 321.0800\n",
            "Epoch 8983, Train Loss: 629.6247, Val Loss: 253.1270\n",
            "Epoch 8984, Train Loss: 575.6179, Val Loss: 217.5187\n",
            "Epoch 8985, Train Loss: 617.3717, Val Loss: 180.3895\n",
            "Epoch 8986, Train Loss: 621.7877, Val Loss: 177.3505\n",
            "Epoch 8987, Train Loss: 608.8266, Val Loss: 178.7076\n",
            "Epoch 8988, Train Loss: 586.4749, Val Loss: 183.3977\n",
            "Epoch 8989, Train Loss: 584.6813, Val Loss: 188.5415\n",
            "Epoch 8990, Train Loss: 618.4712, Val Loss: 196.1851\n",
            "Epoch 8991, Train Loss: 616.9829, Val Loss: 205.9865\n",
            "Epoch 8992, Train Loss: 597.7065, Val Loss: 237.2117\n",
            "Epoch 8993, Train Loss: 615.4164, Val Loss: 222.8101\n",
            "Epoch 8994, Train Loss: 609.2979, Val Loss: 187.0851\n",
            "Epoch 8995, Train Loss: 586.9219, Val Loss: 193.8119\n",
            "Epoch 8996, Train Loss: 616.1648, Val Loss: 205.8434\n",
            "Epoch 8997, Train Loss: 624.1137, Val Loss: 222.8140\n",
            "Epoch 8998, Train Loss: 601.8621, Val Loss: 225.5265\n",
            "Epoch 8999, Train Loss: 624.9569, Val Loss: 207.6879\n",
            "Epoch 9000, Train Loss: 602.4707, Val Loss: 226.6235\n",
            "Epoch 9001, Train Loss: 595.7082, Val Loss: 216.3502\n",
            "Epoch 9002, Train Loss: 601.4037, Val Loss: 194.9265\n",
            "Epoch 9003, Train Loss: 588.5914, Val Loss: 190.1499\n",
            "Epoch 9004, Train Loss: 619.9614, Val Loss: 177.3109\n",
            "Epoch 9005, Train Loss: 595.9134, Val Loss: 255.2692\n",
            "Epoch 9006, Train Loss: 615.9693, Val Loss: 280.0336\n",
            "Epoch 9007, Train Loss: 610.9210, Val Loss: 222.8922\n",
            "Epoch 9008, Train Loss: 620.8066, Val Loss: 199.5576\n",
            "Epoch 9009, Train Loss: 605.2601, Val Loss: 197.0036\n",
            "Epoch 9010, Train Loss: 556.0478, Val Loss: 252.6618\n",
            "Epoch 9011, Train Loss: 555.9629, Val Loss: 301.4404\n",
            "Epoch 9012, Train Loss: 625.8546, Val Loss: 290.3714\n",
            "Epoch 9013, Train Loss: 530.2968, Val Loss: 281.3710\n",
            "Epoch 9014, Train Loss: 616.3780, Val Loss: 218.3163\n",
            "Epoch 9015, Train Loss: 577.3319, Val Loss: 194.5595\n",
            "Epoch 9016, Train Loss: 574.9420, Val Loss: 193.4086\n",
            "Epoch 9017, Train Loss: 611.1743, Val Loss: 204.5167\n",
            "Epoch 9018, Train Loss: 625.2819, Val Loss: 216.5258\n",
            "Epoch 9019, Train Loss: 622.8293, Val Loss: 213.8323\n",
            "Epoch 9020, Train Loss: 602.3140, Val Loss: 213.5747\n",
            "Epoch 9021, Train Loss: 614.8978, Val Loss: 230.6004\n",
            "Epoch 9022, Train Loss: 630.0466, Val Loss: 201.5855\n",
            "Epoch 9023, Train Loss: 575.0010, Val Loss: 185.7571\n",
            "Epoch 9024, Train Loss: 603.7848, Val Loss: 174.7498\n",
            "Epoch 9025, Train Loss: 617.3553, Val Loss: 173.0436\n",
            "Epoch 9026, Train Loss: 604.8210, Val Loss: 214.5612\n",
            "Epoch 9027, Train Loss: 629.1689, Val Loss: 212.1425\n",
            "Epoch 9028, Train Loss: 608.7440, Val Loss: 234.6462\n",
            "Epoch 9029, Train Loss: 603.2750, Val Loss: 254.6545\n",
            "Epoch 9030, Train Loss: 626.4108, Val Loss: 221.7777\n",
            "Epoch 9031, Train Loss: 622.0491, Val Loss: 194.8395\n",
            "Epoch 9032, Train Loss: 566.0956, Val Loss: 181.9129\n",
            "Epoch 9033, Train Loss: 606.3151, Val Loss: 180.8813\n",
            "Epoch 9034, Train Loss: 595.2278, Val Loss: 185.1592\n",
            "Epoch 9035, Train Loss: 607.0328, Val Loss: 185.9547\n",
            "Epoch 9036, Train Loss: 621.1019, Val Loss: 180.3045\n",
            "Epoch 9037, Train Loss: 622.4863, Val Loss: 186.5014\n",
            "Epoch 9038, Train Loss: 598.7557, Val Loss: 189.3142\n",
            "Epoch 9039, Train Loss: 622.7795, Val Loss: 198.5703\n",
            "Epoch 9040, Train Loss: 623.9749, Val Loss: 185.9528\n",
            "Epoch 9041, Train Loss: 616.3262, Val Loss: 177.9183\n",
            "Epoch 9042, Train Loss: 583.1820, Val Loss: 182.3508\n",
            "Epoch 9043, Train Loss: 617.9551, Val Loss: 206.0355\n",
            "Epoch 9044, Train Loss: 598.0868, Val Loss: 220.8307\n",
            "Epoch 9045, Train Loss: 622.8508, Val Loss: 197.1627\n",
            "Epoch 9046, Train Loss: 622.3527, Val Loss: 191.6354\n",
            "Epoch 9047, Train Loss: 571.6989, Val Loss: 213.0898\n",
            "Epoch 9048, Train Loss: 615.6577, Val Loss: 234.8310\n",
            "Epoch 9049, Train Loss: 605.5737, Val Loss: 217.2309\n",
            "Epoch 9050, Train Loss: 608.4583, Val Loss: 195.5041\n",
            "Epoch 9051, Train Loss: 623.4811, Val Loss: 190.7899\n",
            "Epoch 9052, Train Loss: 614.7688, Val Loss: 199.1938\n",
            "Epoch 9053, Train Loss: 605.3637, Val Loss: 197.6279\n",
            "Epoch 9054, Train Loss: 619.6493, Val Loss: 188.3345\n",
            "Epoch 9055, Train Loss: 616.4471, Val Loss: 181.3924\n",
            "Epoch 9056, Train Loss: 616.3638, Val Loss: 177.9838\n",
            "Epoch 9057, Train Loss: 545.1133, Val Loss: 191.0328\n",
            "Epoch 9058, Train Loss: 590.8580, Val Loss: 209.1934\n",
            "Epoch 9059, Train Loss: 615.2706, Val Loss: 230.1464\n",
            "Epoch 9060, Train Loss: 623.1746, Val Loss: 221.7591\n",
            "Epoch 9061, Train Loss: 607.9287, Val Loss: 202.2730\n",
            "Epoch 9062, Train Loss: 613.2955, Val Loss: 203.5108\n",
            "Epoch 9063, Train Loss: 602.9357, Val Loss: 199.2782\n",
            "Epoch 9064, Train Loss: 585.6388, Val Loss: 200.2064\n",
            "Epoch 9065, Train Loss: 613.4414, Val Loss: 199.1193\n",
            "Epoch 9066, Train Loss: 578.6310, Val Loss: 200.9671\n",
            "Epoch 9067, Train Loss: 619.4714, Val Loss: 210.2734\n",
            "Epoch 9068, Train Loss: 620.3767, Val Loss: 224.4950\n",
            "Epoch 9069, Train Loss: 617.0430, Val Loss: 245.0442\n",
            "Epoch 9070, Train Loss: 611.9571, Val Loss: 209.8984\n",
            "Epoch 9071, Train Loss: 625.8062, Val Loss: 193.8882\n",
            "Epoch 9072, Train Loss: 586.3518, Val Loss: 189.1837\n",
            "Epoch 9073, Train Loss: 592.1762, Val Loss: 206.5676\n",
            "Epoch 9074, Train Loss: 568.1601, Val Loss: 240.8593\n",
            "Epoch 9075, Train Loss: 616.4787, Val Loss: 341.8053\n",
            "Epoch 9076, Train Loss: 595.3792, Val Loss: 309.3737\n",
            "Epoch 9077, Train Loss: 579.7440, Val Loss: 221.6846\n",
            "Epoch 9078, Train Loss: 592.0568, Val Loss: 192.7966\n",
            "Epoch 9079, Train Loss: 600.5764, Val Loss: 193.0286\n",
            "Epoch 9080, Train Loss: 601.5195, Val Loss: 202.6467\n",
            "Epoch 9081, Train Loss: 608.4769, Val Loss: 222.3429\n",
            "Epoch 9082, Train Loss: 566.7296, Val Loss: 191.4201\n",
            "Epoch 9083, Train Loss: 615.1862, Val Loss: 177.3408\n",
            "Epoch 9084, Train Loss: 622.0541, Val Loss: 178.1089\n",
            "Epoch 9085, Train Loss: 606.9109, Val Loss: 180.3083\n",
            "Epoch 9086, Train Loss: 627.3594, Val Loss: 179.5240\n",
            "Epoch 9087, Train Loss: 593.9642, Val Loss: 191.6312\n",
            "Epoch 9088, Train Loss: 557.2509, Val Loss: 198.2175\n",
            "Epoch 9089, Train Loss: 590.9572, Val Loss: 196.9383\n",
            "Epoch 9090, Train Loss: 624.4341, Val Loss: 205.9338\n",
            "Epoch 9091, Train Loss: 585.7550, Val Loss: 246.5380\n",
            "Epoch 9092, Train Loss: 615.8387, Val Loss: 212.2847\n",
            "Epoch 9093, Train Loss: 605.5217, Val Loss: 198.7529\n",
            "Epoch 9094, Train Loss: 602.3877, Val Loss: 199.8344\n",
            "Epoch 9095, Train Loss: 607.4293, Val Loss: 213.7131\n",
            "Epoch 9096, Train Loss: 595.6502, Val Loss: 213.4947\n",
            "Epoch 9097, Train Loss: 583.0373, Val Loss: 216.6250\n",
            "Epoch 9098, Train Loss: 621.5996, Val Loss: 202.8919\n",
            "Epoch 9099, Train Loss: 612.5023, Val Loss: 191.4147\n",
            "Epoch 9100, Train Loss: 591.0333, Val Loss: 182.9725\n",
            "Epoch 9101, Train Loss: 609.6550, Val Loss: 174.0242\n",
            "Epoch 9102, Train Loss: 607.6070, Val Loss: 177.6462\n",
            "Epoch 9103, Train Loss: 583.0371, Val Loss: 200.6600\n",
            "Epoch 9104, Train Loss: 627.2571, Val Loss: 206.5087\n",
            "Epoch 9105, Train Loss: 600.8174, Val Loss: 205.7835\n",
            "Epoch 9106, Train Loss: 574.0557, Val Loss: 190.7786\n",
            "Epoch 9107, Train Loss: 611.0649, Val Loss: 175.3192\n",
            "Epoch 9108, Train Loss: 608.4429, Val Loss: 173.7751\n",
            "Epoch 9109, Train Loss: 615.5548, Val Loss: 176.2759\n",
            "Epoch 9110, Train Loss: 593.1148, Val Loss: 177.7082\n",
            "Epoch 9111, Train Loss: 588.3992, Val Loss: 245.5052\n",
            "Epoch 9112, Train Loss: 621.3464, Val Loss: 278.9358\n",
            "Epoch 9113, Train Loss: 597.3809, Val Loss: 253.4078\n",
            "Epoch 9114, Train Loss: 604.0687, Val Loss: 213.1554\n",
            "Epoch 9115, Train Loss: 616.2065, Val Loss: 207.7986\n",
            "Epoch 9116, Train Loss: 599.1727, Val Loss: 224.0170\n",
            "Epoch 9117, Train Loss: 612.0087, Val Loss: 239.0095\n",
            "Epoch 9118, Train Loss: 570.5848, Val Loss: 186.1436\n",
            "Epoch 9119, Train Loss: 578.5311, Val Loss: 173.0559\n",
            "Epoch 9120, Train Loss: 611.2494, Val Loss: 173.5977\n",
            "Epoch 9121, Train Loss: 586.1845, Val Loss: 223.8108\n",
            "Epoch 9122, Train Loss: 624.5507, Val Loss: 282.0625\n",
            "Epoch 9123, Train Loss: 591.7101, Val Loss: 304.8254\n",
            "Epoch 9124, Train Loss: 616.8493, Val Loss: 341.1585\n",
            "Epoch 9125, Train Loss: 616.6461, Val Loss: 276.5372\n",
            "Epoch 9126, Train Loss: 592.6475, Val Loss: 205.0703\n",
            "Epoch 9127, Train Loss: 616.1302, Val Loss: 178.7636\n",
            "Epoch 9128, Train Loss: 624.5535, Val Loss: 184.9301\n",
            "Epoch 9129, Train Loss: 619.4995, Val Loss: 177.0633\n",
            "Epoch 9130, Train Loss: 611.7948, Val Loss: 192.8244\n",
            "Epoch 9131, Train Loss: 574.6763, Val Loss: 200.5822\n",
            "Epoch 9132, Train Loss: 620.8290, Val Loss: 198.6870\n",
            "Epoch 9133, Train Loss: 602.2619, Val Loss: 191.5379\n",
            "Epoch 9134, Train Loss: 584.3690, Val Loss: 213.8209\n",
            "Epoch 9135, Train Loss: 615.0653, Val Loss: 239.6944\n",
            "Epoch 9136, Train Loss: 596.8976, Val Loss: 223.6944\n",
            "Epoch 9137, Train Loss: 615.4267, Val Loss: 208.5278\n",
            "Epoch 9138, Train Loss: 612.2080, Val Loss: 186.0257\n",
            "Epoch 9139, Train Loss: 623.1768, Val Loss: 186.4994\n",
            "Epoch 9140, Train Loss: 561.2549, Val Loss: 195.3854\n",
            "Epoch 9141, Train Loss: 625.0274, Val Loss: 205.4653\n",
            "Epoch 9142, Train Loss: 623.7039, Val Loss: 194.0496\n",
            "Epoch 9143, Train Loss: 576.2477, Val Loss: 181.8820\n",
            "Epoch 9144, Train Loss: 590.7199, Val Loss: 181.7972\n",
            "Epoch 9145, Train Loss: 623.1917, Val Loss: 187.1901\n",
            "Epoch 9146, Train Loss: 623.8986, Val Loss: 220.1732\n",
            "Epoch 9147, Train Loss: 600.5073, Val Loss: 222.2148\n",
            "Epoch 9148, Train Loss: 623.8023, Val Loss: 189.6497\n",
            "Epoch 9149, Train Loss: 583.5618, Val Loss: 184.7288\n",
            "Epoch 9150, Train Loss: 616.7286, Val Loss: 197.4609\n",
            "Epoch 9151, Train Loss: 545.5130, Val Loss: 213.1292\n",
            "Epoch 9152, Train Loss: 607.2898, Val Loss: 207.3122\n",
            "Epoch 9153, Train Loss: 620.5147, Val Loss: 174.4904\n",
            "Epoch 9154, Train Loss: 588.8278, Val Loss: 177.7749\n",
            "Epoch 9155, Train Loss: 623.4841, Val Loss: 177.7275\n",
            "Epoch 9156, Train Loss: 622.5869, Val Loss: 226.1392\n",
            "Epoch 9157, Train Loss: 613.7068, Val Loss: 218.0784\n",
            "Epoch 9158, Train Loss: 582.4679, Val Loss: 210.4184\n",
            "Epoch 9159, Train Loss: 615.6823, Val Loss: 199.1235\n",
            "Epoch 9160, Train Loss: 612.2944, Val Loss: 202.2108\n",
            "Epoch 9161, Train Loss: 592.7461, Val Loss: 195.9141\n",
            "Epoch 9162, Train Loss: 598.2224, Val Loss: 205.4435\n",
            "Epoch 9163, Train Loss: 599.1599, Val Loss: 209.1082\n",
            "Epoch 9164, Train Loss: 623.1007, Val Loss: 200.7336\n",
            "Epoch 9165, Train Loss: 616.5141, Val Loss: 212.1560\n",
            "Epoch 9166, Train Loss: 599.3782, Val Loss: 222.5000\n",
            "Epoch 9167, Train Loss: 599.8220, Val Loss: 202.7114\n",
            "Epoch 9168, Train Loss: 587.7648, Val Loss: 181.8840\n",
            "Epoch 9169, Train Loss: 612.7484, Val Loss: 175.3247\n",
            "Epoch 9170, Train Loss: 609.9342, Val Loss: 179.3275\n",
            "Epoch 9171, Train Loss: 601.6285, Val Loss: 190.4352\n",
            "Epoch 9172, Train Loss: 618.4841, Val Loss: 201.5173\n",
            "Epoch 9173, Train Loss: 588.3249, Val Loss: 233.9588\n",
            "Epoch 9174, Train Loss: 604.4909, Val Loss: 267.6013\n",
            "Epoch 9175, Train Loss: 618.8286, Val Loss: 282.4423\n",
            "Epoch 9176, Train Loss: 614.2909, Val Loss: 311.9504\n",
            "Epoch 9177, Train Loss: 613.9834, Val Loss: 270.7804\n",
            "Epoch 9178, Train Loss: 611.5789, Val Loss: 209.2704\n",
            "Epoch 9179, Train Loss: 593.5364, Val Loss: 183.5221\n",
            "Epoch 9180, Train Loss: 595.0158, Val Loss: 179.9078\n",
            "Epoch 9181, Train Loss: 572.0733, Val Loss: 174.4809\n",
            "Epoch 9182, Train Loss: 619.0492, Val Loss: 186.2406\n",
            "Epoch 9183, Train Loss: 607.8822, Val Loss: 177.6323\n",
            "Epoch 9184, Train Loss: 584.2797, Val Loss: 173.7215\n",
            "Epoch 9185, Train Loss: 621.6531, Val Loss: 176.9707\n",
            "Epoch 9186, Train Loss: 521.1544, Val Loss: 195.4631\n",
            "Epoch 9187, Train Loss: 532.4550, Val Loss: 253.6071\n",
            "Epoch 9188, Train Loss: 614.0910, Val Loss: 350.9110\n",
            "Epoch 9189, Train Loss: 623.9234, Val Loss: 264.5906\n",
            "Epoch 9190, Train Loss: 614.7327, Val Loss: 175.4967\n",
            "Epoch 9191, Train Loss: 595.8219, Val Loss: 175.7523\n",
            "Epoch 9192, Train Loss: 619.1569, Val Loss: 276.4013\n",
            "Epoch 9193, Train Loss: 606.2163, Val Loss: 384.7273\n",
            "Epoch 9194, Train Loss: 619.7700, Val Loss: 327.6056\n",
            "Epoch 9195, Train Loss: 606.2602, Val Loss: 258.7739\n",
            "Epoch 9196, Train Loss: 621.1237, Val Loss: 245.3025\n",
            "Epoch 9197, Train Loss: 617.7690, Val Loss: 236.0148\n",
            "Epoch 9198, Train Loss: 621.4662, Val Loss: 220.2626\n",
            "Epoch 9199, Train Loss: 592.3963, Val Loss: 209.9211\n",
            "Epoch 9200, Train Loss: 614.9861, Val Loss: 198.0215\n",
            "Epoch 9201, Train Loss: 603.3088, Val Loss: 192.6510\n",
            "Epoch 9202, Train Loss: 590.1668, Val Loss: 201.5039\n",
            "Epoch 9203, Train Loss: 615.4769, Val Loss: 179.9811\n",
            "Epoch 9204, Train Loss: 623.4718, Val Loss: 173.7005\n",
            "Epoch 9205, Train Loss: 595.8176, Val Loss: 174.5799\n",
            "Epoch 9206, Train Loss: 604.3826, Val Loss: 191.5972\n",
            "Epoch 9207, Train Loss: 603.7303, Val Loss: 186.3253\n",
            "Epoch 9208, Train Loss: 590.0984, Val Loss: 180.9992\n",
            "Epoch 9209, Train Loss: 620.0895, Val Loss: 180.9795\n",
            "Epoch 9210, Train Loss: 613.7500, Val Loss: 183.7291\n",
            "Epoch 9211, Train Loss: 566.1451, Val Loss: 185.3801\n",
            "Epoch 9212, Train Loss: 614.5187, Val Loss: 199.5296\n",
            "Epoch 9213, Train Loss: 621.5202, Val Loss: 220.2647\n",
            "Epoch 9214, Train Loss: 627.1395, Val Loss: 195.6389\n",
            "Epoch 9215, Train Loss: 597.0842, Val Loss: 177.6291\n",
            "Epoch 9216, Train Loss: 625.5245, Val Loss: 183.1984\n",
            "Epoch 9217, Train Loss: 569.3160, Val Loss: 217.7848\n",
            "Epoch 9218, Train Loss: 612.0658, Val Loss: 227.8802\n",
            "Epoch 9219, Train Loss: 624.4477, Val Loss: 228.1632\n",
            "Epoch 9220, Train Loss: 623.5165, Val Loss: 202.5227\n",
            "Epoch 9221, Train Loss: 604.3096, Val Loss: 188.8204\n",
            "Epoch 9222, Train Loss: 568.8472, Val Loss: 197.3986\n",
            "Epoch 9223, Train Loss: 615.6577, Val Loss: 200.7760\n",
            "Epoch 9224, Train Loss: 605.6900, Val Loss: 226.4007\n",
            "Epoch 9225, Train Loss: 598.7177, Val Loss: 259.5027\n",
            "Epoch 9226, Train Loss: 599.7750, Val Loss: 258.7797\n",
            "Epoch 9227, Train Loss: 569.5364, Val Loss: 206.8125\n",
            "Epoch 9228, Train Loss: 624.5950, Val Loss: 179.0202\n",
            "Epoch 9229, Train Loss: 561.4957, Val Loss: 178.4941\n",
            "Epoch 9230, Train Loss: 616.1495, Val Loss: 217.8660\n",
            "Epoch 9231, Train Loss: 620.3799, Val Loss: 214.6253\n",
            "Epoch 9232, Train Loss: 606.3321, Val Loss: 183.3000\n",
            "Epoch 9233, Train Loss: 580.5479, Val Loss: 179.5164\n",
            "Epoch 9234, Train Loss: 579.6125, Val Loss: 193.4313\n",
            "Epoch 9235, Train Loss: 600.2467, Val Loss: 211.8438\n",
            "Epoch 9236, Train Loss: 591.5610, Val Loss: 195.2946\n",
            "Epoch 9237, Train Loss: 524.7513, Val Loss: 184.7569\n",
            "Epoch 9238, Train Loss: 625.4325, Val Loss: 179.7295\n",
            "Epoch 9239, Train Loss: 614.5793, Val Loss: 178.6999\n",
            "Epoch 9240, Train Loss: 589.5885, Val Loss: 192.6411\n",
            "Epoch 9241, Train Loss: 623.8443, Val Loss: 221.2664\n",
            "Epoch 9242, Train Loss: 614.7242, Val Loss: 217.6351\n",
            "Epoch 9243, Train Loss: 623.9576, Val Loss: 204.4133\n",
            "Epoch 9244, Train Loss: 601.1837, Val Loss: 184.2845\n",
            "Epoch 9245, Train Loss: 624.6079, Val Loss: 188.9159\n",
            "Epoch 9246, Train Loss: 594.2114, Val Loss: 223.6716\n",
            "Epoch 9247, Train Loss: 626.7886, Val Loss: 216.0889\n",
            "Epoch 9248, Train Loss: 611.8610, Val Loss: 196.2430\n",
            "Epoch 9249, Train Loss: 624.9802, Val Loss: 229.4250\n",
            "Epoch 9250, Train Loss: 625.5971, Val Loss: 293.0683\n",
            "Epoch 9251, Train Loss: 611.9800, Val Loss: 281.2553\n",
            "Epoch 9252, Train Loss: 609.5912, Val Loss: 202.8957\n",
            "Epoch 9253, Train Loss: 599.6980, Val Loss: 186.9043\n",
            "Epoch 9254, Train Loss: 601.5219, Val Loss: 201.4987\n",
            "Epoch 9255, Train Loss: 600.8614, Val Loss: 189.2597\n",
            "Epoch 9256, Train Loss: 594.7844, Val Loss: 184.6468\n",
            "Epoch 9257, Train Loss: 571.7331, Val Loss: 177.2869\n",
            "Epoch 9258, Train Loss: 610.6944, Val Loss: 184.8540\n",
            "Epoch 9259, Train Loss: 600.9940, Val Loss: 182.7494\n",
            "Epoch 9260, Train Loss: 599.1021, Val Loss: 189.0901\n",
            "Epoch 9261, Train Loss: 620.6418, Val Loss: 179.4859\n",
            "Epoch 9262, Train Loss: 587.7538, Val Loss: 198.0613\n",
            "Epoch 9263, Train Loss: 558.5288, Val Loss: 224.3571\n",
            "Epoch 9264, Train Loss: 602.6237, Val Loss: 238.2927\n",
            "Epoch 9265, Train Loss: 623.9203, Val Loss: 229.1810\n",
            "Epoch 9266, Train Loss: 594.2626, Val Loss: 229.8133\n",
            "Epoch 9267, Train Loss: 558.9157, Val Loss: 243.6689\n",
            "Epoch 9268, Train Loss: 618.9905, Val Loss: 212.6077\n",
            "Epoch 9269, Train Loss: 548.7503, Val Loss: 191.0421\n",
            "Epoch 9270, Train Loss: 605.7408, Val Loss: 195.9275\n",
            "Epoch 9271, Train Loss: 623.6357, Val Loss: 229.7286\n",
            "Epoch 9272, Train Loss: 619.0563, Val Loss: 313.0536\n",
            "Epoch 9273, Train Loss: 608.4164, Val Loss: 322.1174\n",
            "Epoch 9274, Train Loss: 620.5478, Val Loss: 228.1415\n",
            "Epoch 9275, Train Loss: 577.1712, Val Loss: 175.2207\n",
            "Epoch 9276, Train Loss: 590.0246, Val Loss: 175.7241\n",
            "Epoch 9277, Train Loss: 568.7707, Val Loss: 189.8855\n",
            "Epoch 9278, Train Loss: 608.6746, Val Loss: 370.1967\n",
            "Epoch 9279, Train Loss: 631.9714, Val Loss: 251.0213\n",
            "Epoch 9280, Train Loss: 616.1078, Val Loss: 222.9776\n",
            "Epoch 9281, Train Loss: 621.9263, Val Loss: 226.4868\n",
            "Epoch 9282, Train Loss: 598.3262, Val Loss: 208.9188\n",
            "Epoch 9283, Train Loss: 625.6937, Val Loss: 187.5063\n",
            "Epoch 9284, Train Loss: 605.4895, Val Loss: 202.9581\n",
            "Epoch 9285, Train Loss: 607.7988, Val Loss: 170.7175\n",
            "Epoch 9286, Train Loss: 607.7817, Val Loss: 188.5962\n",
            "Epoch 9287, Train Loss: 617.4587, Val Loss: 205.0311\n",
            "Epoch 9288, Train Loss: 611.5593, Val Loss: 226.3711\n",
            "Epoch 9289, Train Loss: 619.0070, Val Loss: 235.8022\n",
            "Epoch 9290, Train Loss: 620.6036, Val Loss: 224.5219\n",
            "Epoch 9291, Train Loss: 620.7750, Val Loss: 204.5978\n",
            "Epoch 9292, Train Loss: 584.5894, Val Loss: 228.4717\n",
            "Epoch 9293, Train Loss: 593.3734, Val Loss: 251.8309\n",
            "Epoch 9294, Train Loss: 614.7102, Val Loss: 254.5928\n",
            "Epoch 9295, Train Loss: 618.5838, Val Loss: 208.9947\n",
            "Epoch 9296, Train Loss: 616.9889, Val Loss: 176.2985\n",
            "Epoch 9297, Train Loss: 615.8470, Val Loss: 174.9070\n",
            "Epoch 9298, Train Loss: 622.5682, Val Loss: 175.4419\n",
            "Epoch 9299, Train Loss: 616.1002, Val Loss: 188.5090\n",
            "Epoch 9300, Train Loss: 624.1564, Val Loss: 205.0895\n",
            "Epoch 9301, Train Loss: 609.7906, Val Loss: 209.7392\n",
            "Epoch 9302, Train Loss: 591.9360, Val Loss: 215.2818\n",
            "Epoch 9303, Train Loss: 624.0663, Val Loss: 199.2838\n",
            "Epoch 9304, Train Loss: 591.1003, Val Loss: 187.3708\n",
            "Epoch 9305, Train Loss: 587.5911, Val Loss: 197.6671\n",
            "Epoch 9306, Train Loss: 572.5053, Val Loss: 279.8253\n",
            "Epoch 9307, Train Loss: 628.5296, Val Loss: 402.3603\n",
            "Epoch 9308, Train Loss: 550.1494, Val Loss: 319.6051\n",
            "Epoch 9309, Train Loss: 588.5514, Val Loss: 184.7979\n",
            "Epoch 9310, Train Loss: 625.8991, Val Loss: 176.9137\n",
            "Epoch 9311, Train Loss: 613.8269, Val Loss: 187.3950\n",
            "Epoch 9312, Train Loss: 572.8065, Val Loss: 171.9115\n",
            "Epoch 9313, Train Loss: 623.1437, Val Loss: 181.9820\n",
            "Epoch 9314, Train Loss: 598.0144, Val Loss: 193.4572\n",
            "Epoch 9315, Train Loss: 609.5229, Val Loss: 189.1208\n",
            "Epoch 9316, Train Loss: 613.6134, Val Loss: 193.7232\n",
            "Epoch 9317, Train Loss: 553.8899, Val Loss: 318.3481\n",
            "Epoch 9318, Train Loss: 623.7786, Val Loss: 359.9197\n",
            "Epoch 9319, Train Loss: 569.5737, Val Loss: 306.4118\n",
            "Epoch 9320, Train Loss: 611.3363, Val Loss: 295.7323\n",
            "Epoch 9321, Train Loss: 621.6981, Val Loss: 221.7860\n",
            "Epoch 9322, Train Loss: 606.4775, Val Loss: 235.9893\n",
            "Epoch 9323, Train Loss: 592.7436, Val Loss: 286.7391\n",
            "Epoch 9324, Train Loss: 626.9220, Val Loss: 246.9108\n",
            "Epoch 9325, Train Loss: 608.8335, Val Loss: 219.6475\n",
            "Epoch 9326, Train Loss: 575.4745, Val Loss: 194.3328\n",
            "Epoch 9327, Train Loss: 594.9627, Val Loss: 190.6154\n",
            "Epoch 9328, Train Loss: 616.9887, Val Loss: 179.9088\n",
            "Epoch 9329, Train Loss: 572.5568, Val Loss: 183.4381\n",
            "Epoch 9330, Train Loss: 611.3697, Val Loss: 196.9935\n",
            "Epoch 9331, Train Loss: 602.1873, Val Loss: 217.5156\n",
            "Epoch 9332, Train Loss: 550.2859, Val Loss: 190.0500\n",
            "Epoch 9333, Train Loss: 607.8382, Val Loss: 182.5204\n",
            "Epoch 9334, Train Loss: 615.7961, Val Loss: 185.9472\n",
            "Epoch 9335, Train Loss: 627.0798, Val Loss: 177.6812\n",
            "Epoch 9336, Train Loss: 619.9439, Val Loss: 177.9147\n",
            "Epoch 9337, Train Loss: 562.4477, Val Loss: 210.1251\n",
            "Epoch 9338, Train Loss: 620.3128, Val Loss: 221.7498\n",
            "Epoch 9339, Train Loss: 619.0139, Val Loss: 192.3507\n",
            "Epoch 9340, Train Loss: 622.9429, Val Loss: 193.1406\n",
            "Epoch 9341, Train Loss: 610.0176, Val Loss: 214.7159\n",
            "Epoch 9342, Train Loss: 577.5891, Val Loss: 255.8887\n",
            "Epoch 9343, Train Loss: 610.2034, Val Loss: 228.6560\n",
            "Epoch 9344, Train Loss: 578.2497, Val Loss: 204.0209\n",
            "Epoch 9345, Train Loss: 598.4166, Val Loss: 199.9818\n",
            "Epoch 9346, Train Loss: 620.8385, Val Loss: 213.2506\n",
            "Epoch 9347, Train Loss: 616.4166, Val Loss: 219.2643\n",
            "Epoch 9348, Train Loss: 620.4902, Val Loss: 214.1145\n",
            "Epoch 9349, Train Loss: 619.2949, Val Loss: 192.6613\n",
            "Epoch 9350, Train Loss: 618.3423, Val Loss: 181.5379\n",
            "Epoch 9351, Train Loss: 624.4234, Val Loss: 178.2867\n",
            "Epoch 9352, Train Loss: 559.6057, Val Loss: 171.8186\n",
            "Epoch 9353, Train Loss: 621.7522, Val Loss: 195.8545\n",
            "Epoch 9354, Train Loss: 599.8330, Val Loss: 244.4637\n",
            "Epoch 9355, Train Loss: 619.8037, Val Loss: 264.0643\n",
            "Epoch 9356, Train Loss: 620.5263, Val Loss: 263.4041\n",
            "Epoch 9357, Train Loss: 573.2251, Val Loss: 224.3634\n",
            "Epoch 9358, Train Loss: 597.6631, Val Loss: 180.5360\n",
            "Epoch 9359, Train Loss: 606.0711, Val Loss: 176.2255\n",
            "Epoch 9360, Train Loss: 572.2657, Val Loss: 196.6923\n",
            "Epoch 9361, Train Loss: 598.8630, Val Loss: 339.3810\n",
            "Epoch 9362, Train Loss: 618.9996, Val Loss: 329.4326\n",
            "Epoch 9363, Train Loss: 619.9083, Val Loss: 254.2467\n",
            "Epoch 9364, Train Loss: 621.4088, Val Loss: 199.5702\n",
            "Epoch 9365, Train Loss: 618.3112, Val Loss: 184.6731\n",
            "Epoch 9366, Train Loss: 590.6691, Val Loss: 196.5306\n",
            "Epoch 9367, Train Loss: 611.6532, Val Loss: 189.0290\n",
            "Epoch 9368, Train Loss: 621.2130, Val Loss: 177.1037\n",
            "Epoch 9369, Train Loss: 620.5957, Val Loss: 175.3743\n",
            "Epoch 9370, Train Loss: 538.5479, Val Loss: 228.3385\n",
            "Epoch 9371, Train Loss: 608.6935, Val Loss: 298.4845\n",
            "Epoch 9372, Train Loss: 628.6732, Val Loss: 195.4680\n",
            "Epoch 9373, Train Loss: 622.7724, Val Loss: 191.6105\n",
            "Epoch 9374, Train Loss: 606.1346, Val Loss: 207.9225\n",
            "Epoch 9375, Train Loss: 605.9212, Val Loss: 176.8183\n",
            "Epoch 9376, Train Loss: 616.2268, Val Loss: 315.5993\n",
            "Epoch 9377, Train Loss: 626.1209, Val Loss: 429.1177\n",
            "Epoch 9378, Train Loss: 595.9909, Val Loss: 369.0405\n",
            "Epoch 9379, Train Loss: 610.5040, Val Loss: 294.5806\n",
            "Epoch 9380, Train Loss: 622.6530, Val Loss: 203.2390\n",
            "Epoch 9381, Train Loss: 623.1879, Val Loss: 207.2574\n",
            "Epoch 9382, Train Loss: 568.8261, Val Loss: 231.5975\n",
            "Epoch 9383, Train Loss: 617.0300, Val Loss: 214.8886\n",
            "Epoch 9384, Train Loss: 599.6144, Val Loss: 187.5983\n",
            "Epoch 9385, Train Loss: 625.8992, Val Loss: 196.6743\n",
            "Epoch 9386, Train Loss: 619.0650, Val Loss: 194.7676\n",
            "Epoch 9387, Train Loss: 598.1983, Val Loss: 211.2951\n",
            "Epoch 9388, Train Loss: 605.6218, Val Loss: 213.0244\n",
            "Epoch 9389, Train Loss: 625.6577, Val Loss: 177.5524\n",
            "Epoch 9390, Train Loss: 617.8366, Val Loss: 178.4318\n",
            "Epoch 9391, Train Loss: 624.7746, Val Loss: 171.1279\n",
            "Epoch 9392, Train Loss: 606.8555, Val Loss: 180.0347\n",
            "Epoch 9393, Train Loss: 602.2355, Val Loss: 195.4927\n",
            "Epoch 9394, Train Loss: 623.8146, Val Loss: 191.1087\n",
            "Epoch 9395, Train Loss: 620.9618, Val Loss: 178.5159\n",
            "Epoch 9396, Train Loss: 621.5269, Val Loss: 175.9681\n",
            "Epoch 9397, Train Loss: 625.7114, Val Loss: 178.9495\n",
            "Epoch 9398, Train Loss: 605.2193, Val Loss: 193.5570\n",
            "Epoch 9399, Train Loss: 618.1526, Val Loss: 212.8418\n",
            "Epoch 9400, Train Loss: 598.8694, Val Loss: 207.9791\n",
            "Epoch 9401, Train Loss: 608.9895, Val Loss: 204.7644\n",
            "Epoch 9402, Train Loss: 602.8599, Val Loss: 203.2921\n",
            "Epoch 9403, Train Loss: 602.3236, Val Loss: 232.3019\n",
            "Epoch 9404, Train Loss: 609.6698, Val Loss: 321.5615\n",
            "Epoch 9405, Train Loss: 620.2155, Val Loss: 288.3500\n",
            "Epoch 9406, Train Loss: 551.5752, Val Loss: 247.5147\n",
            "Epoch 9407, Train Loss: 585.0765, Val Loss: 186.3353\n",
            "Epoch 9408, Train Loss: 618.7221, Val Loss: 181.9218\n",
            "Epoch 9409, Train Loss: 598.1884, Val Loss: 185.5693\n",
            "Epoch 9410, Train Loss: 599.9628, Val Loss: 180.8711\n",
            "Epoch 9411, Train Loss: 604.0426, Val Loss: 184.2000\n",
            "Epoch 9412, Train Loss: 595.8977, Val Loss: 184.3170\n",
            "Epoch 9413, Train Loss: 618.2053, Val Loss: 190.2120\n",
            "Epoch 9414, Train Loss: 598.0834, Val Loss: 191.7839\n",
            "Epoch 9415, Train Loss: 624.7643, Val Loss: 205.3717\n",
            "Epoch 9416, Train Loss: 623.2389, Val Loss: 199.9076\n",
            "Epoch 9417, Train Loss: 595.8194, Val Loss: 201.8312\n",
            "Epoch 9418, Train Loss: 614.8067, Val Loss: 240.4496\n",
            "Epoch 9419, Train Loss: 616.1739, Val Loss: 235.3855\n",
            "Epoch 9420, Train Loss: 575.0418, Val Loss: 187.5680\n",
            "Epoch 9421, Train Loss: 609.1624, Val Loss: 191.3825\n",
            "Epoch 9422, Train Loss: 600.2855, Val Loss: 206.4934\n",
            "Epoch 9423, Train Loss: 618.4935, Val Loss: 237.5499\n",
            "Epoch 9424, Train Loss: 607.9310, Val Loss: 213.9364\n",
            "Epoch 9425, Train Loss: 572.3141, Val Loss: 213.5896\n",
            "Epoch 9426, Train Loss: 615.4725, Val Loss: 213.5230\n",
            "Epoch 9427, Train Loss: 618.7065, Val Loss: 225.2082\n",
            "Epoch 9428, Train Loss: 603.6604, Val Loss: 185.4378\n",
            "Epoch 9429, Train Loss: 617.1893, Val Loss: 182.1821\n",
            "Epoch 9430, Train Loss: 605.6759, Val Loss: 180.7812\n",
            "Epoch 9431, Train Loss: 610.7680, Val Loss: 179.7459\n",
            "Epoch 9432, Train Loss: 606.9874, Val Loss: 176.8022\n",
            "Epoch 9433, Train Loss: 619.4014, Val Loss: 175.9114\n",
            "Epoch 9434, Train Loss: 610.8128, Val Loss: 173.2593\n",
            "Epoch 9435, Train Loss: 617.8922, Val Loss: 178.8224\n",
            "Epoch 9436, Train Loss: 620.6190, Val Loss: 184.5455\n",
            "Epoch 9437, Train Loss: 607.6952, Val Loss: 189.9742\n",
            "Epoch 9438, Train Loss: 623.6351, Val Loss: 188.5853\n",
            "Epoch 9439, Train Loss: 593.8449, Val Loss: 201.0625\n",
            "Epoch 9440, Train Loss: 552.8371, Val Loss: 234.9446\n",
            "Epoch 9441, Train Loss: 590.5711, Val Loss: 257.7140\n",
            "Epoch 9442, Train Loss: 597.7488, Val Loss: 252.0408\n",
            "Epoch 9443, Train Loss: 590.4102, Val Loss: 265.6938\n",
            "Epoch 9444, Train Loss: 615.6748, Val Loss: 226.0241\n",
            "Epoch 9445, Train Loss: 624.7873, Val Loss: 207.5457\n",
            "Epoch 9446, Train Loss: 616.7432, Val Loss: 194.9968\n",
            "Epoch 9447, Train Loss: 609.0144, Val Loss: 190.7066\n",
            "Epoch 9448, Train Loss: 607.0024, Val Loss: 194.5600\n",
            "Epoch 9449, Train Loss: 625.7276, Val Loss: 181.6703\n",
            "Epoch 9450, Train Loss: 626.1468, Val Loss: 176.4210\n",
            "Epoch 9451, Train Loss: 620.1324, Val Loss: 179.3431\n",
            "Epoch 9452, Train Loss: 623.8667, Val Loss: 181.2393\n",
            "Epoch 9453, Train Loss: 571.0725, Val Loss: 218.4853\n",
            "Epoch 9454, Train Loss: 618.7391, Val Loss: 236.9099\n",
            "Epoch 9455, Train Loss: 599.8666, Val Loss: 207.0737\n",
            "Epoch 9456, Train Loss: 621.6693, Val Loss: 200.5803\n",
            "Epoch 9457, Train Loss: 596.2276, Val Loss: 201.8091\n",
            "Epoch 9458, Train Loss: 603.6059, Val Loss: 216.9910\n",
            "Epoch 9459, Train Loss: 540.7404, Val Loss: 214.6584\n",
            "Epoch 9460, Train Loss: 618.0864, Val Loss: 211.2571\n",
            "Epoch 9461, Train Loss: 619.7324, Val Loss: 197.5193\n",
            "Epoch 9462, Train Loss: 606.0774, Val Loss: 197.7936\n",
            "Epoch 9463, Train Loss: 613.9319, Val Loss: 196.2452\n",
            "Epoch 9464, Train Loss: 585.8108, Val Loss: 195.0121\n",
            "Epoch 9465, Train Loss: 622.7617, Val Loss: 191.0435\n",
            "Epoch 9466, Train Loss: 615.3469, Val Loss: 185.0247\n",
            "Epoch 9467, Train Loss: 603.3101, Val Loss: 181.9518\n",
            "Epoch 9468, Train Loss: 618.5470, Val Loss: 181.6103\n",
            "Epoch 9469, Train Loss: 600.5935, Val Loss: 188.1354\n",
            "Epoch 9470, Train Loss: 610.5159, Val Loss: 203.9966\n",
            "Epoch 9471, Train Loss: 603.8467, Val Loss: 209.2925\n",
            "Epoch 9472, Train Loss: 605.4308, Val Loss: 217.1058\n",
            "Epoch 9473, Train Loss: 590.6197, Val Loss: 220.5873\n",
            "Epoch 9474, Train Loss: 580.4396, Val Loss: 226.4344\n",
            "Epoch 9475, Train Loss: 606.1960, Val Loss: 232.8250\n",
            "Epoch 9476, Train Loss: 622.3881, Val Loss: 214.9729\n",
            "Epoch 9477, Train Loss: 561.8490, Val Loss: 232.5919\n",
            "Epoch 9478, Train Loss: 609.5404, Val Loss: 217.4095\n",
            "Epoch 9479, Train Loss: 606.2923, Val Loss: 201.9382\n",
            "Epoch 9480, Train Loss: 616.3921, Val Loss: 203.5876\n",
            "Epoch 9481, Train Loss: 618.4465, Val Loss: 186.5069\n",
            "Epoch 9482, Train Loss: 601.3403, Val Loss: 180.1345\n",
            "Epoch 9483, Train Loss: 611.3186, Val Loss: 177.7998\n",
            "Epoch 9484, Train Loss: 586.4769, Val Loss: 177.4697\n",
            "Epoch 9485, Train Loss: 605.7427, Val Loss: 176.2060\n",
            "Epoch 9486, Train Loss: 560.1712, Val Loss: 204.8336\n",
            "Epoch 9487, Train Loss: 620.2154, Val Loss: 205.1528\n",
            "Epoch 9488, Train Loss: 618.3166, Val Loss: 189.6125\n",
            "Epoch 9489, Train Loss: 596.3155, Val Loss: 190.1864\n",
            "Epoch 9490, Train Loss: 618.0968, Val Loss: 228.5290\n",
            "Epoch 9491, Train Loss: 596.5618, Val Loss: 249.3465\n",
            "Epoch 9492, Train Loss: 555.5185, Val Loss: 224.9629\n",
            "Epoch 9493, Train Loss: 606.3838, Val Loss: 197.3211\n",
            "Epoch 9494, Train Loss: 591.4389, Val Loss: 197.2147\n",
            "Epoch 9495, Train Loss: 619.8050, Val Loss: 222.5907\n",
            "Epoch 9496, Train Loss: 620.4574, Val Loss: 226.8401\n",
            "Epoch 9497, Train Loss: 611.9412, Val Loss: 222.4536\n",
            "Epoch 9498, Train Loss: 622.5826, Val Loss: 207.8565\n",
            "Epoch 9499, Train Loss: 573.1453, Val Loss: 200.1643\n",
            "Epoch 9500, Train Loss: 600.7097, Val Loss: 203.4819\n",
            "Epoch 9501, Train Loss: 609.4209, Val Loss: 185.4184\n",
            "Epoch 9502, Train Loss: 597.9725, Val Loss: 185.9388\n",
            "Epoch 9503, Train Loss: 611.1714, Val Loss: 189.3994\n",
            "Epoch 9504, Train Loss: 612.8170, Val Loss: 196.1140\n",
            "Epoch 9505, Train Loss: 616.9972, Val Loss: 195.3406\n",
            "Epoch 9506, Train Loss: 619.4953, Val Loss: 197.7423\n",
            "Epoch 9507, Train Loss: 610.5638, Val Loss: 205.5329\n",
            "Epoch 9508, Train Loss: 630.4991, Val Loss: 198.2898\n",
            "Epoch 9509, Train Loss: 594.7822, Val Loss: 180.9514\n",
            "Epoch 9510, Train Loss: 623.1668, Val Loss: 178.7705\n",
            "Epoch 9511, Train Loss: 610.1191, Val Loss: 176.7822\n",
            "Epoch 9512, Train Loss: 571.6487, Val Loss: 179.7202\n",
            "Epoch 9513, Train Loss: 623.3508, Val Loss: 207.2083\n",
            "Epoch 9514, Train Loss: 604.1707, Val Loss: 202.1697\n",
            "Epoch 9515, Train Loss: 621.3090, Val Loss: 193.0650\n",
            "Epoch 9516, Train Loss: 545.6713, Val Loss: 197.8686\n",
            "Epoch 9517, Train Loss: 619.5941, Val Loss: 213.2963\n",
            "Epoch 9518, Train Loss: 621.5485, Val Loss: 240.2932\n",
            "Epoch 9519, Train Loss: 623.4601, Val Loss: 253.6849\n",
            "Epoch 9520, Train Loss: 539.1259, Val Loss: 219.7544\n",
            "Epoch 9521, Train Loss: 620.8865, Val Loss: 182.2326\n",
            "Epoch 9522, Train Loss: 624.2085, Val Loss: 178.0891\n",
            "Epoch 9523, Train Loss: 592.2754, Val Loss: 190.9094\n",
            "Epoch 9524, Train Loss: 587.1107, Val Loss: 275.9307\n",
            "Epoch 9525, Train Loss: 578.5751, Val Loss: 351.7309\n",
            "Epoch 9526, Train Loss: 607.6772, Val Loss: 346.5206\n",
            "Epoch 9527, Train Loss: 593.0082, Val Loss: 297.6176\n",
            "Epoch 9528, Train Loss: 602.8525, Val Loss: 234.6227\n",
            "Epoch 9529, Train Loss: 609.7058, Val Loss: 225.3607\n",
            "Epoch 9530, Train Loss: 622.9099, Val Loss: 206.9342\n",
            "Epoch 9531, Train Loss: 564.6053, Val Loss: 216.8100\n",
            "Epoch 9532, Train Loss: 612.9173, Val Loss: 216.4282\n",
            "Epoch 9533, Train Loss: 619.5698, Val Loss: 185.6563\n",
            "Epoch 9534, Train Loss: 615.7172, Val Loss: 172.4764\n",
            "Epoch 9535, Train Loss: 593.8821, Val Loss: 170.5908\n",
            "Epoch 9536, Train Loss: 616.4019, Val Loss: 176.5669\n",
            "Epoch 9537, Train Loss: 588.9576, Val Loss: 179.1604\n",
            "Epoch 9538, Train Loss: 626.3506, Val Loss: 186.0101\n",
            "Epoch 9539, Train Loss: 625.0608, Val Loss: 187.5889\n",
            "Epoch 9540, Train Loss: 569.3959, Val Loss: 195.7305\n",
            "Epoch 9541, Train Loss: 587.2787, Val Loss: 219.5508\n",
            "Epoch 9542, Train Loss: 625.8176, Val Loss: 202.9048\n",
            "Epoch 9543, Train Loss: 580.7552, Val Loss: 188.3102\n",
            "Epoch 9544, Train Loss: 598.6465, Val Loss: 184.4564\n",
            "Epoch 9545, Train Loss: 561.0158, Val Loss: 199.0145\n",
            "Epoch 9546, Train Loss: 605.2162, Val Loss: 224.3232\n",
            "Epoch 9547, Train Loss: 625.3982, Val Loss: 217.9649\n",
            "Epoch 9548, Train Loss: 605.9607, Val Loss: 192.8293\n",
            "Epoch 9549, Train Loss: 603.8476, Val Loss: 191.9703\n",
            "Epoch 9550, Train Loss: 600.2835, Val Loss: 218.4592\n",
            "Epoch 9551, Train Loss: 603.7879, Val Loss: 237.4733\n",
            "Epoch 9552, Train Loss: 615.2334, Val Loss: 263.0014\n",
            "Epoch 9553, Train Loss: 619.5962, Val Loss: 274.3348\n",
            "Epoch 9554, Train Loss: 600.5490, Val Loss: 248.6997\n",
            "Epoch 9555, Train Loss: 578.7332, Val Loss: 228.8787\n",
            "Epoch 9556, Train Loss: 622.7843, Val Loss: 207.7028\n",
            "Epoch 9557, Train Loss: 609.3933, Val Loss: 199.3763\n",
            "Epoch 9558, Train Loss: 622.8526, Val Loss: 188.1102\n",
            "Epoch 9559, Train Loss: 616.9399, Val Loss: 179.6024\n",
            "Epoch 9560, Train Loss: 599.7327, Val Loss: 182.3812\n",
            "Epoch 9561, Train Loss: 601.7753, Val Loss: 197.2394\n",
            "Epoch 9562, Train Loss: 617.5906, Val Loss: 190.1027\n",
            "Epoch 9563, Train Loss: 626.3805, Val Loss: 187.6652\n",
            "Epoch 9564, Train Loss: 610.7044, Val Loss: 196.4874\n",
            "Epoch 9565, Train Loss: 619.7668, Val Loss: 207.3384\n",
            "Epoch 9566, Train Loss: 616.5559, Val Loss: 181.3325\n",
            "Epoch 9567, Train Loss: 610.4426, Val Loss: 173.0297\n",
            "Epoch 9568, Train Loss: 605.5465, Val Loss: 178.9194\n",
            "Epoch 9569, Train Loss: 587.7619, Val Loss: 175.2830\n",
            "Epoch 9570, Train Loss: 623.5899, Val Loss: 172.4913\n",
            "Epoch 9571, Train Loss: 619.4910, Val Loss: 182.0999\n",
            "Epoch 9572, Train Loss: 619.9177, Val Loss: 194.0922\n",
            "Epoch 9573, Train Loss: 609.3188, Val Loss: 203.2331\n",
            "Epoch 9574, Train Loss: 559.1679, Val Loss: 233.0695\n",
            "Epoch 9575, Train Loss: 612.9037, Val Loss: 227.1448\n",
            "Epoch 9576, Train Loss: 583.7847, Val Loss: 248.3350\n",
            "Epoch 9577, Train Loss: 616.7345, Val Loss: 274.3520\n",
            "Epoch 9578, Train Loss: 572.8718, Val Loss: 248.7494\n",
            "Epoch 9579, Train Loss: 608.4256, Val Loss: 184.4430\n",
            "Epoch 9580, Train Loss: 569.9529, Val Loss: 190.5456\n",
            "Epoch 9581, Train Loss: 584.1098, Val Loss: 360.4515\n",
            "Epoch 9582, Train Loss: 591.5011, Val Loss: 396.7569\n",
            "Epoch 9583, Train Loss: 625.7534, Val Loss: 280.3018\n",
            "Epoch 9584, Train Loss: 601.8092, Val Loss: 194.3711\n",
            "Epoch 9585, Train Loss: 602.3751, Val Loss: 187.4744\n",
            "Epoch 9586, Train Loss: 616.8843, Val Loss: 189.3730\n",
            "Epoch 9587, Train Loss: 617.9722, Val Loss: 197.3831\n",
            "Epoch 9588, Train Loss: 618.1149, Val Loss: 205.8030\n",
            "Epoch 9589, Train Loss: 616.9105, Val Loss: 245.1456\n",
            "Epoch 9590, Train Loss: 612.2097, Val Loss: 205.8682\n",
            "Epoch 9591, Train Loss: 601.9629, Val Loss: 188.1890\n",
            "Epoch 9592, Train Loss: 615.5089, Val Loss: 184.6142\n",
            "Epoch 9593, Train Loss: 592.7833, Val Loss: 183.0551\n",
            "Epoch 9594, Train Loss: 624.1777, Val Loss: 184.8871\n",
            "Epoch 9595, Train Loss: 598.4777, Val Loss: 203.7697\n",
            "Epoch 9596, Train Loss: 611.9064, Val Loss: 193.2273\n",
            "Epoch 9597, Train Loss: 624.1807, Val Loss: 178.3769\n",
            "Epoch 9598, Train Loss: 615.6495, Val Loss: 174.8428\n",
            "Epoch 9599, Train Loss: 581.5023, Val Loss: 186.5468\n",
            "Epoch 9600, Train Loss: 619.4976, Val Loss: 201.4771\n",
            "Epoch 9601, Train Loss: 605.5459, Val Loss: 183.1363\n",
            "Epoch 9602, Train Loss: 594.1879, Val Loss: 179.2885\n",
            "Epoch 9603, Train Loss: 561.2324, Val Loss: 181.6457\n",
            "Epoch 9604, Train Loss: 618.2706, Val Loss: 214.1832\n",
            "Epoch 9605, Train Loss: 612.0846, Val Loss: 242.2676\n",
            "Epoch 9606, Train Loss: 624.8024, Val Loss: 225.3238\n",
            "Epoch 9607, Train Loss: 555.5434, Val Loss: 218.0832\n",
            "Epoch 9608, Train Loss: 609.8851, Val Loss: 214.9691\n",
            "Epoch 9609, Train Loss: 569.4488, Val Loss: 236.4684\n",
            "Epoch 9610, Train Loss: 623.6371, Val Loss: 222.0865\n",
            "Epoch 9611, Train Loss: 617.7354, Val Loss: 203.0530\n",
            "Epoch 9612, Train Loss: 621.3863, Val Loss: 190.6188\n",
            "Epoch 9613, Train Loss: 586.8416, Val Loss: 185.8579\n",
            "Epoch 9614, Train Loss: 617.9270, Val Loss: 182.4867\n",
            "Epoch 9615, Train Loss: 626.5906, Val Loss: 180.9188\n",
            "Epoch 9616, Train Loss: 592.9019, Val Loss: 182.1647\n",
            "Epoch 9617, Train Loss: 615.2781, Val Loss: 195.0289\n",
            "Epoch 9618, Train Loss: 624.0836, Val Loss: 208.0336\n",
            "Epoch 9619, Train Loss: 621.0586, Val Loss: 214.5684\n",
            "Epoch 9620, Train Loss: 623.1601, Val Loss: 219.3360\n",
            "Epoch 9621, Train Loss: 590.2789, Val Loss: 207.3919\n",
            "Epoch 9622, Train Loss: 593.5358, Val Loss: 217.8972\n",
            "Epoch 9623, Train Loss: 620.6819, Val Loss: 228.0513\n",
            "Epoch 9624, Train Loss: 618.2376, Val Loss: 237.4405\n",
            "Epoch 9625, Train Loss: 620.3398, Val Loss: 220.4929\n",
            "Epoch 9626, Train Loss: 607.5429, Val Loss: 206.5624\n",
            "Epoch 9627, Train Loss: 575.8416, Val Loss: 242.5097\n",
            "Epoch 9628, Train Loss: 541.4526, Val Loss: 274.4553\n",
            "Epoch 9629, Train Loss: 620.7949, Val Loss: 235.1344\n",
            "Epoch 9630, Train Loss: 600.3265, Val Loss: 203.3951\n",
            "Epoch 9631, Train Loss: 621.9761, Val Loss: 185.5773\n",
            "Epoch 9632, Train Loss: 604.4783, Val Loss: 194.1397\n",
            "Epoch 9633, Train Loss: 590.9710, Val Loss: 202.4280\n",
            "Epoch 9634, Train Loss: 623.8489, Val Loss: 176.4833\n",
            "Epoch 9635, Train Loss: 611.1296, Val Loss: 172.8088\n",
            "Epoch 9636, Train Loss: 590.0280, Val Loss: 173.0797\n",
            "Epoch 9637, Train Loss: 610.7927, Val Loss: 176.1401\n",
            "Epoch 9638, Train Loss: 620.3759, Val Loss: 180.6595\n",
            "Epoch 9639, Train Loss: 623.2246, Val Loss: 197.4774\n",
            "Epoch 9640, Train Loss: 621.3021, Val Loss: 209.4140\n",
            "Epoch 9641, Train Loss: 622.8851, Val Loss: 204.3914\n",
            "Epoch 9642, Train Loss: 603.1769, Val Loss: 191.8582\n",
            "Epoch 9643, Train Loss: 603.7695, Val Loss: 187.0665\n",
            "Epoch 9644, Train Loss: 607.0585, Val Loss: 193.5933\n",
            "Epoch 9645, Train Loss: 621.1313, Val Loss: 201.7737\n",
            "Epoch 9646, Train Loss: 608.2292, Val Loss: 202.9893\n",
            "Epoch 9647, Train Loss: 595.3765, Val Loss: 205.4845\n",
            "Epoch 9648, Train Loss: 594.1601, Val Loss: 209.7858\n",
            "Epoch 9649, Train Loss: 550.4248, Val Loss: 209.8251\n",
            "Epoch 9650, Train Loss: 563.9135, Val Loss: 219.6753\n",
            "Epoch 9651, Train Loss: 581.9898, Val Loss: 247.7590\n",
            "Epoch 9652, Train Loss: 616.0528, Val Loss: 273.6441\n",
            "Epoch 9653, Train Loss: 601.8782, Val Loss: 291.3535\n",
            "Epoch 9654, Train Loss: 624.6544, Val Loss: 206.7082\n",
            "Epoch 9655, Train Loss: 612.5251, Val Loss: 177.5950\n",
            "Epoch 9656, Train Loss: 579.6782, Val Loss: 187.8361\n",
            "Epoch 9657, Train Loss: 590.9123, Val Loss: 176.5345\n",
            "Epoch 9658, Train Loss: 593.7963, Val Loss: 247.7003\n",
            "Epoch 9659, Train Loss: 621.8973, Val Loss: 234.4255\n",
            "Epoch 9660, Train Loss: 614.6546, Val Loss: 192.0777\n",
            "Epoch 9661, Train Loss: 625.9148, Val Loss: 195.0735\n",
            "Epoch 9662, Train Loss: 620.7010, Val Loss: 201.1241\n",
            "Epoch 9663, Train Loss: 625.9784, Val Loss: 194.9701\n",
            "Epoch 9664, Train Loss: 621.0820, Val Loss: 180.8466\n",
            "Epoch 9665, Train Loss: 608.1132, Val Loss: 188.2945\n",
            "Epoch 9666, Train Loss: 607.0287, Val Loss: 204.8899\n",
            "Epoch 9667, Train Loss: 616.3665, Val Loss: 200.5873\n",
            "Epoch 9668, Train Loss: 626.6480, Val Loss: 209.6907\n",
            "Epoch 9669, Train Loss: 619.1939, Val Loss: 195.7093\n",
            "Epoch 9670, Train Loss: 626.7177, Val Loss: 194.6560\n",
            "Epoch 9671, Train Loss: 545.1727, Val Loss: 223.3605\n",
            "Epoch 9672, Train Loss: 613.4412, Val Loss: 234.6208\n",
            "Epoch 9673, Train Loss: 581.6444, Val Loss: 222.1526\n",
            "Epoch 9674, Train Loss: 604.8293, Val Loss: 189.6300\n",
            "Epoch 9675, Train Loss: 617.7087, Val Loss: 190.1675\n",
            "Epoch 9676, Train Loss: 616.1205, Val Loss: 194.1365\n",
            "Epoch 9677, Train Loss: 622.8753, Val Loss: 202.1853\n",
            "Epoch 9678, Train Loss: 605.3348, Val Loss: 199.3507\n",
            "Epoch 9679, Train Loss: 618.5521, Val Loss: 215.2897\n",
            "Epoch 9680, Train Loss: 613.9308, Val Loss: 228.5313\n",
            "Epoch 9681, Train Loss: 597.3217, Val Loss: 219.1801\n",
            "Epoch 9682, Train Loss: 590.3685, Val Loss: 203.8310\n",
            "Epoch 9683, Train Loss: 588.7157, Val Loss: 238.4727\n",
            "Epoch 9684, Train Loss: 617.0812, Val Loss: 250.5371\n",
            "Epoch 9685, Train Loss: 620.4826, Val Loss: 237.5727\n",
            "Epoch 9686, Train Loss: 618.9019, Val Loss: 247.3834\n",
            "Epoch 9687, Train Loss: 620.7988, Val Loss: 214.7147\n",
            "Epoch 9688, Train Loss: 606.3028, Val Loss: 203.7435\n",
            "Epoch 9689, Train Loss: 572.4248, Val Loss: 214.1145\n",
            "Epoch 9690, Train Loss: 608.1035, Val Loss: 226.1553\n",
            "Epoch 9691, Train Loss: 541.8806, Val Loss: 176.7776\n",
            "Epoch 9692, Train Loss: 623.1300, Val Loss: 171.5944\n",
            "Epoch 9693, Train Loss: 622.2235, Val Loss: 170.8621\n",
            "Epoch 9694, Train Loss: 582.8258, Val Loss: 213.2730\n",
            "Epoch 9695, Train Loss: 568.8886, Val Loss: 299.5388\n",
            "Epoch 9696, Train Loss: 614.3430, Val Loss: 223.0044\n",
            "Epoch 9697, Train Loss: 623.6356, Val Loss: 226.0718\n",
            "Epoch 9698, Train Loss: 622.6994, Val Loss: 239.4477\n",
            "Epoch 9699, Train Loss: 600.2312, Val Loss: 244.0155\n",
            "Epoch 9700, Train Loss: 561.1884, Val Loss: 246.6896\n",
            "Epoch 9701, Train Loss: 594.3400, Val Loss: 238.8302\n",
            "Epoch 9702, Train Loss: 623.2633, Val Loss: 178.4675\n",
            "Epoch 9703, Train Loss: 608.4633, Val Loss: 181.9019\n",
            "Epoch 9704, Train Loss: 618.1531, Val Loss: 172.6609\n",
            "Epoch 9705, Train Loss: 589.6230, Val Loss: 200.3777\n",
            "Epoch 9706, Train Loss: 632.1733, Val Loss: 229.8326\n",
            "Epoch 9707, Train Loss: 613.5159, Val Loss: 199.5037\n",
            "Epoch 9708, Train Loss: 570.3673, Val Loss: 191.2331\n",
            "Epoch 9709, Train Loss: 620.9066, Val Loss: 197.3017\n",
            "Epoch 9710, Train Loss: 611.9178, Val Loss: 207.3835\n",
            "Epoch 9711, Train Loss: 629.2762, Val Loss: 224.7736\n",
            "Epoch 9712, Train Loss: 600.8654, Val Loss: 226.6084\n",
            "Epoch 9713, Train Loss: 597.5816, Val Loss: 227.2451\n",
            "Epoch 9714, Train Loss: 616.3576, Val Loss: 197.0082\n",
            "Epoch 9715, Train Loss: 592.6396, Val Loss: 189.6931\n",
            "Epoch 9716, Train Loss: 610.3080, Val Loss: 211.3545\n",
            "Epoch 9717, Train Loss: 599.3018, Val Loss: 236.3071\n",
            "Epoch 9718, Train Loss: 597.5671, Val Loss: 208.3754\n",
            "Epoch 9719, Train Loss: 623.1459, Val Loss: 204.4076\n",
            "Epoch 9720, Train Loss: 592.6063, Val Loss: 183.3351\n",
            "Epoch 9721, Train Loss: 621.2495, Val Loss: 174.5194\n",
            "Epoch 9722, Train Loss: 602.2324, Val Loss: 173.2432\n",
            "Epoch 9723, Train Loss: 626.2591, Val Loss: 172.5376\n",
            "Epoch 9724, Train Loss: 618.8649, Val Loss: 177.0095\n",
            "Epoch 9725, Train Loss: 609.3257, Val Loss: 174.0378\n",
            "Epoch 9726, Train Loss: 613.7985, Val Loss: 176.1643\n",
            "Epoch 9727, Train Loss: 622.6719, Val Loss: 196.5974\n",
            "Epoch 9728, Train Loss: 621.6922, Val Loss: 197.4726\n",
            "Epoch 9729, Train Loss: 591.9783, Val Loss: 243.0357\n",
            "Epoch 9730, Train Loss: 623.0785, Val Loss: 274.5500\n",
            "Epoch 9731, Train Loss: 600.5332, Val Loss: 241.5999\n",
            "Epoch 9732, Train Loss: 598.6067, Val Loss: 245.4956\n",
            "Epoch 9733, Train Loss: 602.2675, Val Loss: 195.8662\n",
            "Epoch 9734, Train Loss: 621.4778, Val Loss: 180.8964\n",
            "Epoch 9735, Train Loss: 616.4169, Val Loss: 177.7654\n",
            "Epoch 9736, Train Loss: 561.6343, Val Loss: 186.8472\n",
            "Epoch 9737, Train Loss: 621.6156, Val Loss: 302.9317\n",
            "Epoch 9738, Train Loss: 588.3578, Val Loss: 317.9611\n",
            "Epoch 9739, Train Loss: 617.5444, Val Loss: 243.0569\n",
            "Epoch 9740, Train Loss: 595.8693, Val Loss: 203.7027\n",
            "Epoch 9741, Train Loss: 611.7611, Val Loss: 208.2714\n",
            "Epoch 9742, Train Loss: 620.8636, Val Loss: 200.0427\n",
            "Epoch 9743, Train Loss: 573.0717, Val Loss: 217.1999\n",
            "Epoch 9744, Train Loss: 609.5942, Val Loss: 212.3597\n",
            "Epoch 9745, Train Loss: 600.2937, Val Loss: 231.8685\n",
            "Epoch 9746, Train Loss: 610.3129, Val Loss: 230.8911\n",
            "Epoch 9747, Train Loss: 620.9432, Val Loss: 218.7291\n",
            "Epoch 9748, Train Loss: 568.2191, Val Loss: 188.5932\n",
            "Epoch 9749, Train Loss: 608.0955, Val Loss: 185.4131\n",
            "Epoch 9750, Train Loss: 577.7398, Val Loss: 205.3426\n",
            "Epoch 9751, Train Loss: 622.4051, Val Loss: 223.2752\n",
            "Epoch 9752, Train Loss: 622.8840, Val Loss: 214.1030\n",
            "Epoch 9753, Train Loss: 587.8459, Val Loss: 201.8186\n",
            "Epoch 9754, Train Loss: 550.8238, Val Loss: 208.5872\n",
            "Epoch 9755, Train Loss: 620.4633, Val Loss: 214.3663\n",
            "Epoch 9756, Train Loss: 622.0854, Val Loss: 189.1896\n",
            "Epoch 9757, Train Loss: 590.6440, Val Loss: 214.3514\n",
            "Epoch 9758, Train Loss: 579.3893, Val Loss: 334.8890\n",
            "Epoch 9759, Train Loss: 609.4512, Val Loss: 315.8234\n",
            "Epoch 9760, Train Loss: 552.2681, Val Loss: 195.6340\n",
            "Epoch 9761, Train Loss: 583.0269, Val Loss: 241.8496\n",
            "Epoch 9762, Train Loss: 627.6302, Val Loss: 243.2414\n",
            "Epoch 9763, Train Loss: 615.9665, Val Loss: 215.5938\n",
            "Epoch 9764, Train Loss: 623.9346, Val Loss: 217.7992\n",
            "Epoch 9765, Train Loss: 573.9831, Val Loss: 260.7795\n",
            "Epoch 9766, Train Loss: 602.6134, Val Loss: 240.2271\n",
            "Epoch 9767, Train Loss: 625.2521, Val Loss: 173.3487\n",
            "Epoch 9768, Train Loss: 609.6089, Val Loss: 200.8647\n",
            "Epoch 9769, Train Loss: 544.0990, Val Loss: 180.8547\n",
            "Epoch 9770, Train Loss: 618.8284, Val Loss: 207.2738\n",
            "Epoch 9771, Train Loss: 585.7646, Val Loss: 246.9145\n",
            "Epoch 9772, Train Loss: 605.6825, Val Loss: 222.6952\n",
            "Epoch 9773, Train Loss: 595.7590, Val Loss: 195.9886\n",
            "Epoch 9774, Train Loss: 609.3580, Val Loss: 183.1325\n",
            "Epoch 9775, Train Loss: 612.1317, Val Loss: 180.7113\n",
            "Epoch 9776, Train Loss: 597.4312, Val Loss: 194.0778\n",
            "Epoch 9777, Train Loss: 614.0017, Val Loss: 207.7223\n",
            "Epoch 9778, Train Loss: 586.3096, Val Loss: 239.3933\n",
            "Epoch 9779, Train Loss: 583.9790, Val Loss: 279.9555\n",
            "Epoch 9780, Train Loss: 624.0570, Val Loss: 264.5557\n",
            "Epoch 9781, Train Loss: 626.3658, Val Loss: 200.4678\n",
            "Epoch 9782, Train Loss: 555.8738, Val Loss: 195.0386\n",
            "Epoch 9783, Train Loss: 598.6057, Val Loss: 188.5788\n",
            "Epoch 9784, Train Loss: 600.5411, Val Loss: 191.5964\n",
            "Epoch 9785, Train Loss: 571.3082, Val Loss: 212.4805\n",
            "Epoch 9786, Train Loss: 587.5873, Val Loss: 266.0633\n",
            "Epoch 9787, Train Loss: 620.0760, Val Loss: 287.6838\n",
            "Epoch 9788, Train Loss: 623.2519, Val Loss: 193.1302\n",
            "Epoch 9789, Train Loss: 595.2299, Val Loss: 174.7276\n",
            "Epoch 9790, Train Loss: 610.1719, Val Loss: 175.4838\n",
            "Epoch 9791, Train Loss: 559.8964, Val Loss: 199.3365\n",
            "Epoch 9792, Train Loss: 621.9763, Val Loss: 226.5667\n",
            "Epoch 9793, Train Loss: 589.0724, Val Loss: 215.2229\n",
            "Epoch 9794, Train Loss: 615.3668, Val Loss: 200.0594\n",
            "Epoch 9795, Train Loss: 594.9777, Val Loss: 191.2836\n",
            "Epoch 9796, Train Loss: 613.0202, Val Loss: 212.8222\n",
            "Epoch 9797, Train Loss: 610.1795, Val Loss: 258.0546\n",
            "Epoch 9798, Train Loss: 620.2354, Val Loss: 268.8711\n",
            "Epoch 9799, Train Loss: 622.1727, Val Loss: 258.1161\n",
            "Epoch 9800, Train Loss: 582.0203, Val Loss: 268.4015\n",
            "Epoch 9801, Train Loss: 608.5378, Val Loss: 254.5945\n",
            "Epoch 9802, Train Loss: 577.2377, Val Loss: 196.4589\n",
            "Epoch 9803, Train Loss: 598.0642, Val Loss: 180.5849\n",
            "Epoch 9804, Train Loss: 613.2663, Val Loss: 173.7855\n",
            "Epoch 9805, Train Loss: 617.9370, Val Loss: 170.0700\n",
            "Epoch 9806, Train Loss: 608.5136, Val Loss: 183.1788\n",
            "Epoch 9807, Train Loss: 582.2600, Val Loss: 180.4363\n",
            "Epoch 9808, Train Loss: 624.9042, Val Loss: 179.6631\n",
            "Epoch 9809, Train Loss: 573.3755, Val Loss: 187.0586\n",
            "Epoch 9810, Train Loss: 623.9918, Val Loss: 192.4496\n",
            "Epoch 9811, Train Loss: 610.6980, Val Loss: 189.3680\n",
            "Epoch 9812, Train Loss: 609.0961, Val Loss: 208.7870\n",
            "Epoch 9813, Train Loss: 627.5647, Val Loss: 224.9168\n",
            "Epoch 9814, Train Loss: 605.8689, Val Loss: 221.4838\n",
            "Epoch 9815, Train Loss: 592.4666, Val Loss: 225.3791\n",
            "Epoch 9816, Train Loss: 604.0777, Val Loss: 219.4344\n",
            "Epoch 9817, Train Loss: 613.4472, Val Loss: 204.7159\n",
            "Epoch 9818, Train Loss: 615.5022, Val Loss: 196.2102\n",
            "Epoch 9819, Train Loss: 618.5386, Val Loss: 186.7563\n",
            "Epoch 9820, Train Loss: 592.3594, Val Loss: 185.0898\n",
            "Epoch 9821, Train Loss: 599.3999, Val Loss: 198.9387\n",
            "Epoch 9822, Train Loss: 605.1783, Val Loss: 218.6918\n",
            "Epoch 9823, Train Loss: 609.5565, Val Loss: 211.6208\n",
            "Epoch 9824, Train Loss: 614.3468, Val Loss: 227.6132\n",
            "Epoch 9825, Train Loss: 560.7787, Val Loss: 222.4937\n",
            "Epoch 9826, Train Loss: 617.6834, Val Loss: 192.6473\n",
            "Epoch 9827, Train Loss: 624.2144, Val Loss: 185.9635\n",
            "Epoch 9828, Train Loss: 590.7761, Val Loss: 215.8017\n",
            "Epoch 9829, Train Loss: 627.7761, Val Loss: 200.1368\n",
            "Epoch 9830, Train Loss: 550.0578, Val Loss: 195.9390\n",
            "Epoch 9831, Train Loss: 614.2537, Val Loss: 179.4990\n",
            "Epoch 9832, Train Loss: 624.2311, Val Loss: 176.7810\n",
            "Epoch 9833, Train Loss: 616.8275, Val Loss: 182.5777\n",
            "Epoch 9834, Train Loss: 591.6060, Val Loss: 222.9833\n",
            "Epoch 9835, Train Loss: 611.8059, Val Loss: 245.0278\n",
            "Epoch 9836, Train Loss: 603.1093, Val Loss: 208.8550\n",
            "Epoch 9837, Train Loss: 611.3302, Val Loss: 206.6145\n",
            "Epoch 9838, Train Loss: 574.9125, Val Loss: 257.8109\n",
            "Epoch 9839, Train Loss: 597.7724, Val Loss: 269.5443\n",
            "Epoch 9840, Train Loss: 624.0648, Val Loss: 232.1034\n",
            "Epoch 9841, Train Loss: 572.2131, Val Loss: 207.7931\n",
            "Epoch 9842, Train Loss: 561.6381, Val Loss: 229.7352\n",
            "Epoch 9843, Train Loss: 609.3577, Val Loss: 218.4240\n",
            "Epoch 9844, Train Loss: 621.4538, Val Loss: 184.3239\n",
            "Epoch 9845, Train Loss: 600.4238, Val Loss: 178.2590\n",
            "Epoch 9846, Train Loss: 602.7886, Val Loss: 175.3774\n",
            "Epoch 9847, Train Loss: 577.5044, Val Loss: 172.5594\n",
            "Epoch 9848, Train Loss: 613.6189, Val Loss: 178.6089\n",
            "Epoch 9849, Train Loss: 604.8860, Val Loss: 191.1931\n",
            "Epoch 9850, Train Loss: 624.3375, Val Loss: 191.5569\n",
            "Epoch 9851, Train Loss: 616.9604, Val Loss: 181.9478\n",
            "Epoch 9852, Train Loss: 620.3945, Val Loss: 179.5179\n",
            "Epoch 9853, Train Loss: 587.0134, Val Loss: 200.3908\n",
            "Epoch 9854, Train Loss: 607.8212, Val Loss: 240.2640\n",
            "Epoch 9855, Train Loss: 607.2209, Val Loss: 234.3367\n",
            "Epoch 9856, Train Loss: 616.9608, Val Loss: 211.2119\n",
            "Epoch 9857, Train Loss: 617.2728, Val Loss: 201.8996\n",
            "Epoch 9858, Train Loss: 619.1470, Val Loss: 203.5072\n",
            "Epoch 9859, Train Loss: 614.0897, Val Loss: 198.6994\n",
            "Epoch 9860, Train Loss: 599.3042, Val Loss: 199.7695\n",
            "Epoch 9861, Train Loss: 619.8555, Val Loss: 183.0313\n",
            "Epoch 9862, Train Loss: 615.1240, Val Loss: 179.5314\n",
            "Epoch 9863, Train Loss: 619.1261, Val Loss: 178.0491\n",
            "Epoch 9864, Train Loss: 608.7777, Val Loss: 185.3592\n",
            "Epoch 9865, Train Loss: 608.0858, Val Loss: 223.3207\n",
            "Epoch 9866, Train Loss: 594.0218, Val Loss: 228.4990\n",
            "Epoch 9867, Train Loss: 626.0610, Val Loss: 229.0210\n",
            "Epoch 9868, Train Loss: 615.3369, Val Loss: 217.3573\n",
            "Epoch 9869, Train Loss: 616.3764, Val Loss: 194.2305\n",
            "Epoch 9870, Train Loss: 612.3445, Val Loss: 184.3770\n",
            "Epoch 9871, Train Loss: 593.8885, Val Loss: 191.4829\n",
            "Epoch 9872, Train Loss: 602.2207, Val Loss: 201.4688\n",
            "Epoch 9873, Train Loss: 624.1721, Val Loss: 207.3735\n",
            "Epoch 9874, Train Loss: 537.2275, Val Loss: 218.7454\n",
            "Epoch 9875, Train Loss: 604.9416, Val Loss: 221.5119\n",
            "Epoch 9876, Train Loss: 617.3645, Val Loss: 185.0379\n",
            "Epoch 9877, Train Loss: 578.4583, Val Loss: 184.9591\n",
            "Epoch 9878, Train Loss: 594.5097, Val Loss: 196.3517\n",
            "Epoch 9879, Train Loss: 620.6946, Val Loss: 272.5442\n",
            "Epoch 9880, Train Loss: 603.3757, Val Loss: 265.0272\n",
            "Epoch 9881, Train Loss: 534.0265, Val Loss: 264.4532\n",
            "Epoch 9882, Train Loss: 622.8982, Val Loss: 252.2636\n",
            "Epoch 9883, Train Loss: 589.4619, Val Loss: 253.8646\n",
            "Epoch 9884, Train Loss: 571.4877, Val Loss: 208.7095\n",
            "Epoch 9885, Train Loss: 618.7521, Val Loss: 192.5591\n",
            "Epoch 9886, Train Loss: 617.4076, Val Loss: 185.7132\n",
            "Epoch 9887, Train Loss: 616.8910, Val Loss: 170.4336\n",
            "Epoch 9888, Train Loss: 606.3810, Val Loss: 170.4139\n",
            "Epoch 9889, Train Loss: 598.1496, Val Loss: 188.5339\n",
            "Epoch 9890, Train Loss: 603.6643, Val Loss: 194.2330\n",
            "Epoch 9891, Train Loss: 621.5340, Val Loss: 197.0244\n",
            "Epoch 9892, Train Loss: 626.3547, Val Loss: 191.4816\n",
            "Epoch 9893, Train Loss: 622.4011, Val Loss: 195.4781\n",
            "Epoch 9894, Train Loss: 582.3833, Val Loss: 226.4419\n",
            "Epoch 9895, Train Loss: 627.2668, Val Loss: 235.2785\n",
            "Epoch 9896, Train Loss: 623.6170, Val Loss: 206.2495\n",
            "Epoch 9897, Train Loss: 616.5980, Val Loss: 194.6099\n",
            "Epoch 9898, Train Loss: 623.4540, Val Loss: 205.0996\n",
            "Epoch 9899, Train Loss: 606.0488, Val Loss: 217.4725\n",
            "Epoch 9900, Train Loss: 598.5439, Val Loss: 206.2770\n",
            "Epoch 9901, Train Loss: 590.0783, Val Loss: 198.7943\n",
            "Epoch 9902, Train Loss: 616.1358, Val Loss: 206.9907\n",
            "Epoch 9903, Train Loss: 609.3385, Val Loss: 225.2031\n",
            "Epoch 9904, Train Loss: 612.9902, Val Loss: 218.5387\n",
            "Epoch 9905, Train Loss: 604.6067, Val Loss: 211.7893\n",
            "Epoch 9906, Train Loss: 618.3292, Val Loss: 204.0156\n",
            "Epoch 9907, Train Loss: 619.5053, Val Loss: 212.9345\n",
            "Epoch 9908, Train Loss: 626.2114, Val Loss: 215.7253\n",
            "Epoch 9909, Train Loss: 629.6218, Val Loss: 207.2968\n",
            "Epoch 9910, Train Loss: 622.4455, Val Loss: 214.6824\n",
            "Epoch 9911, Train Loss: 588.9198, Val Loss: 227.4530\n",
            "Epoch 9912, Train Loss: 613.7994, Val Loss: 251.6115\n",
            "Epoch 9913, Train Loss: 604.0417, Val Loss: 215.2959\n",
            "Epoch 9914, Train Loss: 593.3291, Val Loss: 192.2001\n",
            "Epoch 9915, Train Loss: 626.3454, Val Loss: 189.1841\n",
            "Epoch 9916, Train Loss: 602.4369, Val Loss: 212.7900\n",
            "Epoch 9917, Train Loss: 609.9090, Val Loss: 252.4269\n",
            "Epoch 9918, Train Loss: 606.2458, Val Loss: 289.0884\n",
            "Epoch 9919, Train Loss: 607.6682, Val Loss: 197.8507\n",
            "Epoch 9920, Train Loss: 620.6705, Val Loss: 180.0166\n",
            "Epoch 9921, Train Loss: 616.5087, Val Loss: 175.1047\n",
            "Epoch 9922, Train Loss: 617.6873, Val Loss: 175.0368\n",
            "Epoch 9923, Train Loss: 619.9830, Val Loss: 190.3509\n",
            "Epoch 9924, Train Loss: 551.4710, Val Loss: 317.0437\n",
            "Epoch 9925, Train Loss: 562.1210, Val Loss: 310.7051\n",
            "Epoch 9926, Train Loss: 610.3578, Val Loss: 272.0684\n",
            "Epoch 9927, Train Loss: 622.0978, Val Loss: 205.6259\n",
            "Epoch 9928, Train Loss: 616.6278, Val Loss: 184.6686\n",
            "Epoch 9929, Train Loss: 606.5808, Val Loss: 188.3046\n",
            "Epoch 9930, Train Loss: 614.6260, Val Loss: 218.1719\n",
            "Epoch 9931, Train Loss: 603.4723, Val Loss: 222.6892\n",
            "Epoch 9932, Train Loss: 609.0910, Val Loss: 170.1990\n",
            "Epoch 9933, Train Loss: 596.8320, Val Loss: 190.5141\n",
            "Epoch 9934, Train Loss: 598.0188, Val Loss: 237.8912\n",
            "Epoch 9935, Train Loss: 575.6963, Val Loss: 204.8852\n",
            "Epoch 9936, Train Loss: 555.4023, Val Loss: 399.5606\n",
            "Epoch 9937, Train Loss: 627.7192, Val Loss: 397.9166\n",
            "Epoch 9938, Train Loss: 624.9677, Val Loss: 268.5846\n",
            "Epoch 9939, Train Loss: 606.3191, Val Loss: 253.1139\n",
            "Epoch 9940, Train Loss: 562.0474, Val Loss: 269.0940\n",
            "Epoch 9941, Train Loss: 613.5908, Val Loss: 286.0858\n",
            "Epoch 9942, Train Loss: 583.6772, Val Loss: 239.3639\n",
            "Epoch 9943, Train Loss: 610.8325, Val Loss: 224.5754\n",
            "Epoch 9944, Train Loss: 624.0667, Val Loss: 215.4548\n",
            "Epoch 9945, Train Loss: 610.9967, Val Loss: 179.8087\n",
            "Epoch 9946, Train Loss: 587.5231, Val Loss: 175.3524\n",
            "Epoch 9947, Train Loss: 624.3761, Val Loss: 176.7374\n",
            "Epoch 9948, Train Loss: 572.0851, Val Loss: 175.9301\n",
            "Epoch 9949, Train Loss: 626.1375, Val Loss: 176.5239\n",
            "Epoch 9950, Train Loss: 606.2908, Val Loss: 180.1757\n",
            "Epoch 9951, Train Loss: 595.8347, Val Loss: 183.7142\n",
            "Epoch 9952, Train Loss: 609.9098, Val Loss: 176.6758\n",
            "Epoch 9953, Train Loss: 601.0158, Val Loss: 180.3285\n",
            "Epoch 9954, Train Loss: 555.9651, Val Loss: 220.5814\n",
            "Epoch 9955, Train Loss: 612.1227, Val Loss: 268.9246\n",
            "Epoch 9956, Train Loss: 601.4355, Val Loss: 266.0908\n",
            "Epoch 9957, Train Loss: 622.6869, Val Loss: 263.2824\n",
            "Epoch 9958, Train Loss: 599.6138, Val Loss: 277.8512\n",
            "Epoch 9959, Train Loss: 626.1680, Val Loss: 251.4977\n",
            "Epoch 9960, Train Loss: 600.9100, Val Loss: 220.7188\n",
            "Epoch 9961, Train Loss: 576.5709, Val Loss: 206.0570\n",
            "Epoch 9962, Train Loss: 564.3203, Val Loss: 202.9980\n",
            "Epoch 9963, Train Loss: 609.2609, Val Loss: 219.3578\n",
            "Epoch 9964, Train Loss: 598.4023, Val Loss: 190.8592\n",
            "Epoch 9965, Train Loss: 592.6281, Val Loss: 189.2070\n",
            "Epoch 9966, Train Loss: 613.1148, Val Loss: 196.4268\n",
            "Epoch 9967, Train Loss: 625.8664, Val Loss: 197.4396\n",
            "Epoch 9968, Train Loss: 612.7961, Val Loss: 203.6480\n",
            "Epoch 9969, Train Loss: 611.1483, Val Loss: 201.0194\n",
            "Epoch 9970, Train Loss: 602.6034, Val Loss: 197.6561\n",
            "Epoch 9971, Train Loss: 514.9994, Val Loss: 212.8308\n",
            "Epoch 9972, Train Loss: 617.7892, Val Loss: 203.7606\n",
            "Epoch 9973, Train Loss: 623.2990, Val Loss: 190.7247\n",
            "Epoch 9974, Train Loss: 610.3863, Val Loss: 195.9402\n",
            "Epoch 9975, Train Loss: 583.2156, Val Loss: 210.1444\n",
            "Epoch 9976, Train Loss: 609.1202, Val Loss: 223.8506\n",
            "Epoch 9977, Train Loss: 627.5082, Val Loss: 203.5764\n",
            "Epoch 9978, Train Loss: 623.5201, Val Loss: 190.0798\n",
            "Epoch 9979, Train Loss: 609.5622, Val Loss: 184.1398\n",
            "Epoch 9980, Train Loss: 582.4987, Val Loss: 194.3666\n",
            "Epoch 9981, Train Loss: 609.4330, Val Loss: 231.1133\n",
            "Epoch 9982, Train Loss: 609.8403, Val Loss: 240.4512\n",
            "Epoch 9983, Train Loss: 604.7943, Val Loss: 206.6273\n",
            "Epoch 9984, Train Loss: 615.5036, Val Loss: 183.7788\n",
            "Epoch 9985, Train Loss: 606.0873, Val Loss: 181.9886\n",
            "Epoch 9986, Train Loss: 625.2483, Val Loss: 193.2572\n",
            "Epoch 9987, Train Loss: 587.1162, Val Loss: 226.4705\n",
            "Epoch 9988, Train Loss: 610.1869, Val Loss: 243.8929\n",
            "Epoch 9989, Train Loss: 605.2892, Val Loss: 187.1101\n",
            "Epoch 9990, Train Loss: 592.8285, Val Loss: 195.7288\n",
            "Epoch 9991, Train Loss: 585.1197, Val Loss: 178.9914\n",
            "Epoch 9992, Train Loss: 616.8852, Val Loss: 207.6105\n",
            "Epoch 9993, Train Loss: 627.0244, Val Loss: 230.0335\n",
            "Epoch 9994, Train Loss: 610.4105, Val Loss: 218.8910\n",
            "Epoch 9995, Train Loss: 622.8568, Val Loss: 198.7139\n",
            "Epoch 9996, Train Loss: 603.1032, Val Loss: 194.1136\n",
            "Epoch 9997, Train Loss: 600.9739, Val Loss: 198.2721\n",
            "Epoch 9998, Train Loss: 616.3304, Val Loss: 204.8675\n",
            "Epoch 9999, Train Loss: 624.8740, Val Loss: 208.7055\n",
            "Epoch 10000, Train Loss: 576.9114, Val Loss: 233.3280\n"
          ]
        }
      ],
      "source": [
        "# 위에서 정의한 MobileNet3D 모델 인스턴스화\n",
        "model = MobileNetV3(input_shape=(1368, 64, 64, 3), num_classes=min_frames)\n",
        "model.cuda()  # GPU 사용 설정\n",
        "\n",
        "# 손실 함수 및 최적화 알고리즘 설정\n",
        "criterion = nn.MSELoss()  # 회귀 문제이므로 Mean Squared Error 손실 함수 사용\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # 학습률은 실험적으로 조정\n",
        "\n",
        "# 훈련 루프\n",
        "num_epochs = 10000  # 에폭 수는 실험적으로 조정\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # 모델을 훈련 모드로 설정\n",
        "    running_loss = 0.0\n",
        "    for video, ppg in train_loader:\n",
        "        video = video.cuda()  # 입력 데이터를 GPU로 이동\n",
        "        ppg = ppg.cuda()  # 타겟 데이터를 GPU로 이동\n",
        "\n",
        "        optimizer.zero_grad()  # 그래디언트 초기화\n",
        "\n",
        "        outputs = model(video)  # 모델의 순전파\n",
        "        loss = criterion(outputs, ppg.unsqueeze(1))  # 손실 계산\n",
        "        loss.backward()  # 역전파\n",
        "        optimizer.step()  # 최적화\n",
        "\n",
        "        running_loss += loss.item() * video.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # 검증 루프\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for video, ppg in val_loader:\n",
        "            video = video.cuda()\n",
        "            ppg = ppg.cuda()\n",
        "\n",
        "            outputs = model(video)\n",
        "            loss = criterion(outputs, ppg.unsqueeze(1))\n",
        "\n",
        "            val_loss += loss.item() * video.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9peWEoJRisRl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
